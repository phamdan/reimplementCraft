{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"trainSyndata.ipynb","provenance":[],"authorship_tag":"ABX9TyMUjsphL3nzty4RewRHSeeM"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3wSqXN888Fs","executionInfo":{"status":"ok","timestamp":1609496078258,"user_tz":-420,"elapsed":1535,"user":{"displayName":"mạnh dân phạm","photoUrl":"","userId":"04253576041367165005"}},"outputId":"69990465-b061-4020-bdbc-7304c72242e3"},"source":["from google.colab import drive\r\n","drive.mount('/content/gdrive')\r\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c2qZUvSp9AID","executionInfo":{"status":"ok","timestamp":1609496080382,"user_tz":-420,"elapsed":1335,"user":{"displayName":"mạnh dân phạm","photoUrl":"","userId":"04253576041367165005"}},"outputId":"75edf6f0-8045-461c-b817-092e2766c306"},"source":["import os\r\n","path='/content/gdrive/MyDrive/CRAFT-Reimplementation-master'\r\n","os.chdir(path)\r\n","!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["basenet\t\tfile_utils.py  __pycache__\t train-MLT_data.py\n","coordinates.py\tgaussian.py    README.md\t trainSyndata.ipynb\n","craft.py\timage\t       requirements.txt  trainSyndata.py\n","craft_utils.py\timgproc.py     testgt.ipynb\t watershed.py\n","data_loader.py\tmep.py\t       test.py\n","eval\t\tmseloss.py     torchutil.py\n","evaluation.py\tpretrain       trainic15data.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"S--5Ffmknik5","executionInfo":{"status":"ok","timestamp":1609496051569,"user_tz":-420,"elapsed":19690,"user":{"displayName":"mạnh dân phạm","photoUrl":"","userId":"04253576041367165005"}},"outputId":"bef34974-a82b-4498-9b5d-15aaa7ae32bf"},"source":["# !pip install torch\r\n","# !pip install torchvision\r\n","# !pip install opencv-python\r\n","# !pip install scikit-image\r\n","# !pip install scipy\r\n","# !pip install Polygon3\r\n","# !pip install pillow==6.1"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.19.4)\n","Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.7.0+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.8)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.19.4)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (0.16.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.5)\n","Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.4.1)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.4.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.1.1)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (3.2.2)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (7.0.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image) (4.4.2)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.19.0->scikit-image) (1.19.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.19.4)\n","Collecting Polygon3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/a0/d07a4f3e80ed7020a33f3111db217f54ac44a485ff45da3c21ce49f65041/Polygon3-3.0.8.tar.gz (71kB)\n","\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n","\u001b[?25hBuilding wheels for collected packages: Polygon3\n","  Building wheel for Polygon3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Polygon3: filename=Polygon3-3.0.8-cp36-cp36m-linux_x86_64.whl size=101481 sha256=a5d9f63b5ee3c996feba7206146064c9e72b2bcad1cc7b9f1d4c267463960a78\n","  Stored in directory: /root/.cache/pip/wheels/95/32/f1/5525b233996d9d99cbce2f0a8da60d137ddddc555d3e8b0e2a\n","Successfully built Polygon3\n","Installing collected packages: Polygon3\n","Successfully installed Polygon3-3.0.8\n","Collecting pillow==6.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/41/db6dec65ddbc176a59b89485e8cc136a433ed9c6397b6bfe2cd38412051e/Pillow-6.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 15.1MB/s \n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: pillow\n","  Found existing installation: Pillow 7.0.0\n","    Uninstalling Pillow-7.0.0:\n","      Successfully uninstalled Pillow-7.0.0\n","Successfully installed pillow-6.1.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6cAT1X9f4GoA","executionInfo":{"elapsed":135835,"status":"ok","timestamp":1609312713998,"user":{"displayName":"mạnh dân phạm","photoUrl":"","userId":"04253576041367165005"},"user_tz":-420},"outputId":"30be5ed4-2b02-4cd8-dca5-0fa733996f6a"},"source":["# pip install -r requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting torch==0.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n","\u001b[K     |████████████████████████████████| 519.5MB 31kB/s \n","\u001b[?25hCollecting torchvision==0.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n","\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n","\u001b[?25hCollecting opencv-python==3.4.2.17\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/e0/21c8964fa8ef50842ebefaa7346a3cf0e37b56c8ecd97ed6bd2dbe577705/opencv_python-3.4.2.17-cp36-cp36m-manylinux1_x86_64.whl (25.0MB)\n","\u001b[K     |████████████████████████████████| 25.0MB 1.3MB/s \n","\u001b[?25hCollecting scikit-image==0.14.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/06/d560630eb9e36d90d69fe57d9ff762d8f501664ce478b8a0ae132b3c3008/scikit_image-0.14.2-cp36-cp36m-manylinux1_x86_64.whl (25.3MB)\n","\u001b[K     |████████████████████████████████| 25.3MB 116kB/s \n","\u001b[?25hCollecting scipy==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n","\u001b[K     |████████████████████████████████| 31.2MB 111kB/s \n","\u001b[?25hCollecting Polygon3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/a0/d07a4f3e80ed7020a33f3111db217f54ac44a485ff45da3c21ce49f65041/Polygon3-3.0.8.tar.gz (71kB)\n","\u001b[K     |████████████████████████████████| 71kB 12.0MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1->-r requirements.txt (line 2)) (1.15.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1->-r requirements.txt (line 2)) (6.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1->-r requirements.txt (line 2)) (1.19.4)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2->-r requirements.txt (line 4)) (1.1.1)\n","Requirement already satisfied: dask[array]>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2->-r requirements.txt (line 4)) (2.12.0)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2->-r requirements.txt (line 4)) (3.2.2)\n","Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2->-r requirements.txt (line 4)) (1.3.0)\n","Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.2->-r requirements.txt (line 4)) (2.5)\n","Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array]>=1.0.0->scikit-image==0.14.2->-r requirements.txt (line 4)) (0.11.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-image==0.14.2->-r requirements.txt (line 4)) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-image==0.14.2->-r requirements.txt (line 4)) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-image==0.14.2->-r requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-image==0.14.2->-r requirements.txt (line 4)) (0.10.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image==0.14.2->-r requirements.txt (line 4)) (4.4.2)\n","Building wheels for collected packages: Polygon3\n","  Building wheel for Polygon3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Polygon3: filename=Polygon3-3.0.8-cp36-cp36m-linux_x86_64.whl size=101482 sha256=3bc0d3b3f632206b7dd0158863216e309dddd6248b21020a412cfa75e85a8104\n","  Stored in directory: /root/.cache/pip/wheels/95/32/f1/5525b233996d9d99cbce2f0a8da60d137ddddc555d3e8b0e2a\n","Successfully built Polygon3\n","\u001b[31mERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fastai 1.0.61 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: torch, torchvision, opencv-python, scipy, scikit-image, Polygon3\n","  Found existing installation: torch 1.7.0+cu101\n","    Uninstalling torch-1.7.0+cu101:\n","      Successfully uninstalled torch-1.7.0+cu101\n","  Found existing installation: torchvision 0.8.1+cu101\n","    Uninstalling torchvision-0.8.1+cu101:\n","      Successfully uninstalled torchvision-0.8.1+cu101\n","  Found existing installation: opencv-python 4.1.2.30\n","    Uninstalling opencv-python-4.1.2.30:\n","      Successfully uninstalled opencv-python-4.1.2.30\n","  Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","  Found existing installation: scikit-image 0.16.2\n","    Uninstalling scikit-image-0.16.2:\n","      Successfully uninstalled scikit-image-0.16.2\n","Successfully installed Polygon3-3.0.8 opencv-python-3.4.2.17 scikit-image-0.14.2 scipy-1.1.0 torch-0.4.1 torchvision-0.2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"szeFl2qT5LVa","executionInfo":{"status":"ok","timestamp":1609511182647,"user_tz":-420,"elapsed":15096184,"user":{"displayName":"mạnh dân phạm","photoUrl":"","userId":"04253576041367165005"}},"outputId":"ece684f5-1151-4411-f9c4-58cad7676fc3"},"source":["!python trainSyndata.py"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","epoch 0:(2/293) batch || training time for 2 batch 15.345521450042725 || training loss 7.686274290084839 ||\n","epoch 0:(4/293) batch || training time for 2 batch 4.958162307739258 || training loss 2.919329881668091 ||\n","epoch 0:(6/293) batch || training time for 2 batch 5.057635068893433 || training loss 1.9005945920944214 ||\n","epoch 0:(8/293) batch || training time for 2 batch 4.895914793014526 || training loss 1.5930628776550293 ||\n","epoch 0:(10/293) batch || training time for 2 batch 5.313720941543579 || training loss 1.1523017883300781 ||\n","epoch 0:(12/293) batch || training time for 2 batch 5.411604404449463 || training loss 0.8675909638404846 ||\n","epoch 0:(14/293) batch || training time for 2 batch 5.287835597991943 || training loss 0.9097025990486145 ||\n","epoch 0:(16/293) batch || training time for 2 batch 5.664208173751831 || training loss 0.7046621143817902 ||\n","epoch 0:(18/293) batch || training time for 2 batch 5.129122018814087 || training loss 0.7017031908035278 ||\n","epoch 0:(20/293) batch || training time for 2 batch 5.49088454246521 || training loss 0.6432040631771088 ||\n","epoch 0:(22/293) batch || training time for 2 batch 5.179529190063477 || training loss 0.5755165219306946 ||\n","epoch 0:(24/293) batch || training time for 2 batch 5.366134405136108 || training loss 0.6006874144077301 ||\n","epoch 0:(26/293) batch || training time for 2 batch 4.963765382766724 || training loss 0.5435491502285004 ||\n","epoch 0:(28/293) batch || training time for 2 batch 5.446843862533569 || training loss 0.5161740183830261 ||\n","epoch 0:(30/293) batch || training time for 2 batch 5.127569198608398 || training loss 0.5289991796016693 ||\n","epoch 0:(32/293) batch || training time for 2 batch 5.563101768493652 || training loss 0.47874145209789276 ||\n","epoch 0:(34/293) batch || training time for 2 batch 5.2324793338775635 || training loss 0.49847225844860077 ||\n","epoch 0:(36/293) batch || training time for 2 batch 5.343679189682007 || training loss 0.46365371346473694 ||\n","epoch 0:(38/293) batch || training time for 2 batch 5.404698848724365 || training loss 0.5286822319030762 ||\n","epoch 0:(40/293) batch || training time for 2 batch 5.678274869918823 || training loss 0.4793512672185898 ||\n","epoch 0:(42/293) batch || training time for 2 batch 5.388728857040405 || training loss 0.443616658449173 ||\n","epoch 0:(44/293) batch || training time for 2 batch 5.682249307632446 || training loss 0.45483462512493134 ||\n","epoch 0:(46/293) batch || training time for 2 batch 4.910670518875122 || training loss 0.52932970225811 ||\n","epoch 0:(48/293) batch || training time for 2 batch 5.082280397415161 || training loss 0.44235269725322723 ||\n","epoch 0:(50/293) batch || training time for 2 batch 5.604180574417114 || training loss 0.4222741425037384 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 19.206976890563965s\n","epoch 0:(52/293) batch || training time for 2 batch 29.57488989830017 || training loss 0.4330144226551056 ||\n","epoch 0:(54/293) batch || training time for 2 batch 5.078838348388672 || training loss 0.4184642434120178 ||\n","epoch 0:(56/293) batch || training time for 2 batch 5.514669418334961 || training loss 0.4076527953147888 ||\n","epoch 0:(58/293) batch || training time for 2 batch 5.599186182022095 || training loss 0.36268287897109985 ||\n","epoch 0:(60/293) batch || training time for 2 batch 5.1307454109191895 || training loss 1.5854510515928268 ||\n","epoch 0:(62/293) batch || training time for 2 batch 5.526793956756592 || training loss 0.3916517496109009 ||\n","epoch 0:(64/293) batch || training time for 2 batch 5.3458616733551025 || training loss 2.248028874397278 ||\n","epoch 0:(66/293) batch || training time for 2 batch 5.489230632781982 || training loss 0.4300408810377121 ||\n","epoch 0:(68/293) batch || training time for 2 batch 5.537036657333374 || training loss 0.41870248317718506 ||\n","epoch 0:(70/293) batch || training time for 2 batch 5.3532867431640625 || training loss 0.471173495054245 ||\n","epoch 0:(72/293) batch || training time for 2 batch 5.333833694458008 || training loss 0.3551248013973236 ||\n","epoch 0:(74/293) batch || training time for 2 batch 5.524736404418945 || training loss 0.39184460043907166 ||\n","epoch 0:(76/293) batch || training time for 2 batch 5.191036224365234 || training loss 0.3615962713956833 ||\n","epoch 0:(78/293) batch || training time for 2 batch 5.732970952987671 || training loss 0.34037233889102936 ||\n","epoch 0:(80/293) batch || training time for 2 batch 5.333409547805786 || training loss 0.41456061601638794 ||\n","epoch 0:(82/293) batch || training time for 2 batch 5.2052223682403564 || training loss 0.9665544331073761 ||\n","epoch 0:(84/293) batch || training time for 2 batch 5.432038068771362 || training loss 0.36936600506305695 ||\n","epoch 0:(86/293) batch || training time for 2 batch 5.186102867126465 || training loss 0.3069888651371002 ||\n","epoch 0:(88/293) batch || training time for 2 batch 5.325132608413696 || training loss 0.33946414291858673 ||\n","epoch 0:(90/293) batch || training time for 2 batch 5.592591047286987 || training loss 0.2884903848171234 ||\n","epoch 0:(92/293) batch || training time for 2 batch 5.018372535705566 || training loss 0.3616962879896164 ||\n","epoch 0:(94/293) batch || training time for 2 batch 5.2197370529174805 || training loss 0.3390195518732071 ||\n","epoch 0:(96/293) batch || training time for 2 batch 5.482768297195435 || training loss 0.3132162541151047 ||\n","epoch 0:(98/293) batch || training time for 2 batch 5.717011451721191 || training loss 0.4218885004520416 ||\n","epoch 0:(100/293) batch || training time for 2 batch 5.774907827377319 || training loss 0.3277291804552078 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 17.257439374923706s\n","epoch 0:(102/293) batch || training time for 2 batch 27.21442413330078 || training loss 0.2944125831127167 ||\n","epoch 0:(104/293) batch || training time for 2 batch 5.434204816818237 || training loss 0.3028837889432907 ||\n","epoch 0:(106/293) batch || training time for 2 batch 5.196938514709473 || training loss 0.2917245179414749 ||\n","epoch 0:(108/293) batch || training time for 2 batch 5.491323709487915 || training loss 0.2668573558330536 ||\n","epoch 0:(110/293) batch || training time for 2 batch 5.2472803592681885 || training loss 0.2648780196905136 ||\n","epoch 0:(112/293) batch || training time for 2 batch 6.085243463516235 || training loss 0.2914857268333435 ||\n","epoch 0:(114/293) batch || training time for 2 batch 5.504354000091553 || training loss 0.27784930169582367 ||\n","epoch 0:(116/293) batch || training time for 2 batch 5.466429948806763 || training loss 0.2895732522010803 ||\n","epoch 0:(118/293) batch || training time for 2 batch 5.70949912071228 || training loss 0.28176626563072205 ||\n","epoch 0:(120/293) batch || training time for 2 batch 5.044438600540161 || training loss 0.27632491290569305 ||\n","epoch 0:(122/293) batch || training time for 2 batch 5.276562213897705 || training loss 0.5774144977331161 ||\n","epoch 0:(124/293) batch || training time for 2 batch 5.8330559730529785 || training loss 0.311456561088562 ||\n","epoch 0:(126/293) batch || training time for 2 batch 5.5649495124816895 || training loss 0.28044334053993225 ||\n","epoch 0:(128/293) batch || training time for 2 batch 5.309027910232544 || training loss 0.27564552426338196 ||\n","epoch 0:(130/293) batch || training time for 2 batch 5.596450328826904 || training loss 0.3226761519908905 ||\n","epoch 0:(132/293) batch || training time for 2 batch 5.289400815963745 || training loss 0.2889300733804703 ||\n","epoch 0:(134/293) batch || training time for 2 batch 5.140540361404419 || training loss 0.2830021232366562 ||\n","epoch 0:(136/293) batch || training time for 2 batch 5.569591522216797 || training loss 0.2649106904864311 ||\n","epoch 0:(138/293) batch || training time for 2 batch 4.952304363250732 || training loss 0.3049253225326538 ||\n","epoch 0:(140/293) batch || training time for 2 batch 5.609751462936401 || training loss 0.23226124048233032 ||\n","epoch 0:(142/293) batch || training time for 2 batch 5.336793661117554 || training loss 0.23212479054927826 ||\n","epoch 0:(144/293) batch || training time for 2 batch 5.090994834899902 || training loss 0.2833016812801361 ||\n","epoch 0:(146/293) batch || training time for 2 batch 5.762339115142822 || training loss 0.2894554138183594 ||\n","epoch 0:(148/293) batch || training time for 2 batch 5.450308561325073 || training loss 0.24013811349868774 ||\n","epoch 0:(150/293) batch || training time for 2 batch 5.082154035568237 || training loss 0.20619157701730728 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 17.250866413116455s\n","epoch 0:(152/293) batch || training time for 2 batch 27.26838994026184 || training loss 0.2488991767168045 ||\n","epoch 0:(154/293) batch || training time for 2 batch 5.121114015579224 || training loss 0.29106901586055756 ||\n","epoch 0:(156/293) batch || training time for 2 batch 5.021606922149658 || training loss 0.23417027294635773 ||\n","epoch 0:(158/293) batch || training time for 2 batch 5.468803644180298 || training loss 0.4347689002752304 ||\n","epoch 0:(160/293) batch || training time for 2 batch 5.713718414306641 || training loss 0.2509768679738045 ||\n","epoch 0:(162/293) batch || training time for 2 batch 5.290615081787109 || training loss 0.2294187769293785 ||\n","epoch 0:(164/293) batch || training time for 2 batch 5.279358148574829 || training loss 0.3726843148469925 ||\n","epoch 0:(166/293) batch || training time for 2 batch 5.973205804824829 || training loss 0.30468903481960297 ||\n","epoch 0:(168/293) batch || training time for 2 batch 5.33592963218689 || training loss 0.21519751101732254 ||\n","epoch 0:(170/293) batch || training time for 2 batch 5.2405500411987305 || training loss 0.24241596460342407 ||\n","epoch 0:(172/293) batch || training time for 2 batch 5.646070957183838 || training loss 0.24016428738832474 ||\n","epoch 0:(174/293) batch || training time for 2 batch 5.267135381698608 || training loss 0.20593909919261932 ||\n","epoch 0:(176/293) batch || training time for 2 batch 5.368049621582031 || training loss 0.25518694519996643 ||\n","epoch 0:(178/293) batch || training time for 2 batch 5.243913412094116 || training loss 0.2528303414583206 ||\n","epoch 0:(180/293) batch || training time for 2 batch 5.2510621547698975 || training loss 0.206216000020504 ||\n","epoch 0:(182/293) batch || training time for 2 batch 5.221760988235474 || training loss 0.24764082580804825 ||\n","epoch 0:(184/293) batch || training time for 2 batch 5.177072763442993 || training loss 0.22979091107845306 ||\n","epoch 0:(186/293) batch || training time for 2 batch 5.056671380996704 || training loss 0.23922044038772583 ||\n","epoch 0:(188/293) batch || training time for 2 batch 5.654025554656982 || training loss 0.2656264901161194 ||\n","epoch 0:(190/293) batch || training time for 2 batch 5.68960165977478 || training loss 0.26712700724601746 ||\n","epoch 0:(192/293) batch || training time for 2 batch 5.256977796554565 || training loss 0.21020100265741348 ||\n","epoch 0:(194/293) batch || training time for 2 batch 5.367568731307983 || training loss 0.19619199633598328 ||\n","epoch 0:(196/293) batch || training time for 2 batch 5.510128974914551 || training loss 0.26384836435317993 ||\n","epoch 0:(198/293) batch || training time for 2 batch 5.192792177200317 || training loss 0.39812126755714417 ||\n","2.6214400000000004e-05\n","epoch 0:(200/293) batch || training time for 2 batch 5.368812084197998 || training loss 0.23587845265865326 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 17.53445553779602s\n","epoch 0:(202/293) batch || training time for 2 batch 27.621952772140503 || training loss 0.3198577016592026 ||\n","epoch 0:(204/293) batch || training time for 2 batch 5.586970806121826 || training loss 0.22291456162929535 ||\n","epoch 0:(206/293) batch || training time for 2 batch 5.552849769592285 || training loss 0.23588646948337555 ||\n","epoch 0:(208/293) batch || training time for 2 batch 5.893329381942749 || training loss 0.23911812901496887 ||\n","epoch 0:(210/293) batch || training time for 2 batch 5.706803560256958 || training loss 0.1960107758641243 ||\n","epoch 0:(212/293) batch || training time for 2 batch 5.437647581100464 || training loss 0.23072052001953125 ||\n","epoch 0:(214/293) batch || training time for 2 batch 5.535765647888184 || training loss 0.21300915628671646 ||\n","epoch 0:(216/293) batch || training time for 2 batch 5.626437664031982 || training loss 0.21129575371742249 ||\n","epoch 0:(218/293) batch || training time for 2 batch 5.433854579925537 || training loss 0.2632400840520859 ||\n","epoch 0:(220/293) batch || training time for 2 batch 5.415685415267944 || training loss 0.22272536158561707 ||\n","epoch 0:(222/293) batch || training time for 2 batch 5.461723327636719 || training loss 0.3648843541741371 ||\n","epoch 0:(224/293) batch || training time for 2 batch 5.132466077804565 || training loss 0.20124411582946777 ||\n","epoch 0:(226/293) batch || training time for 2 batch 5.642023086547852 || training loss 0.2393089383840561 ||\n","epoch 0:(228/293) batch || training time for 2 batch 5.457951545715332 || training loss 0.23020948469638824 ||\n","epoch 0:(230/293) batch || training time for 2 batch 5.6260082721710205 || training loss 0.26173608750104904 ||\n","epoch 0:(232/293) batch || training time for 2 batch 5.318755865097046 || training loss 0.3017391338944435 ||\n","epoch 0:(234/293) batch || training time for 2 batch 5.003215551376343 || training loss 0.18349456787109375 ||\n","epoch 0:(236/293) batch || training time for 2 batch 5.179907560348511 || training loss 0.20501065999269485 ||\n","epoch 0:(238/293) batch || training time for 2 batch 5.2724289894104 || training loss 0.2539421319961548 ||\n","epoch 0:(240/293) batch || training time for 2 batch 5.903974294662476 || training loss 0.21559403091669083 ||\n","epoch 0:(242/293) batch || training time for 2 batch 6.087012052536011 || training loss 0.1875510811805725 ||\n","epoch 0:(244/293) batch || training time for 2 batch 5.277989625930786 || training loss 0.2190297693014145 ||\n","epoch 0:(246/293) batch || training time for 2 batch 5.361417531967163 || training loss 0.18862832337617874 ||\n","epoch 0:(248/293) batch || training time for 2 batch 5.660463094711304 || training loss 0.21932931244373322 ||\n","epoch 0:(250/293) batch || training time for 2 batch 5.700791120529175 || training loss 0.2321755215525627 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 17.134655952453613s\n","epoch 0:(252/293) batch || training time for 2 batch 27.259478092193604 || training loss 0.20180518925189972 ||\n","epoch 0:(254/293) batch || training time for 2 batch 5.64007043838501 || training loss 0.23535418510437012 ||\n","epoch 0:(256/293) batch || training time for 2 batch 5.716020822525024 || training loss 0.2315034568309784 ||\n","epoch 0:(258/293) batch || training time for 2 batch 5.280860900878906 || training loss 0.21218466758728027 ||\n","epoch 0:(260/293) batch || training time for 2 batch 5.2341179847717285 || training loss 0.19742247462272644 ||\n","epoch 0:(262/293) batch || training time for 2 batch 5.018705368041992 || training loss 0.20301281660795212 ||\n","epoch 0:(264/293) batch || training time for 2 batch 5.373462915420532 || training loss 0.27463071793317795 ||\n","epoch 0:(266/293) batch || training time for 2 batch 5.389160633087158 || training loss 0.20251888036727905 ||\n","epoch 0:(268/293) batch || training time for 2 batch 5.433186054229736 || training loss 0.20872408151626587 ||\n","epoch 0:(270/293) batch || training time for 2 batch 5.647886514663696 || training loss 0.20088494569063187 ||\n","epoch 0:(272/293) batch || training time for 2 batch 5.606536626815796 || training loss 0.22645576298236847 ||\n","epoch 0:(274/293) batch || training time for 2 batch 5.7675769329071045 || training loss 0.1994245946407318 ||\n","epoch 0:(276/293) batch || training time for 2 batch 5.848606586456299 || training loss 0.20612650364637375 ||\n","epoch 0:(278/293) batch || training time for 2 batch 5.0456132888793945 || training loss 0.1935558244585991 ||\n","epoch 0:(280/293) batch || training time for 2 batch 5.382298946380615 || training loss 0.21671098470687866 ||\n","epoch 0:(282/293) batch || training time for 2 batch 5.4267897605896 || training loss 0.19772187620401382 ||\n","epoch 0:(284/293) batch || training time for 2 batch 5.300655364990234 || training loss 0.1836034283041954 ||\n","epoch 0:(286/293) batch || training time for 2 batch 5.507680416107178 || training loss 0.16096198558807373 ||\n","epoch 0:(288/293) batch || training time for 2 batch 5.694568634033203 || training loss 0.17751754820346832 ||\n","epoch 0:(290/293) batch || training time for 2 batch 5.303579330444336 || training loss 0.20479761809110641 ||\n","epoch 0:(292/293) batch || training time for 2 batch 5.383458375930786 || training loss 0.2041337713599205 ||\n","epoch 1:(2/293) batch || training time for 2 batch 4.663046360015869 || training loss 0.7516781091690063 ||\n","epoch 1:(4/293) batch || training time for 2 batch 3.0379467010498047 || training loss 0.32300326228141785 ||\n","epoch 1:(6/293) batch || training time for 2 batch 3.26729679107666 || training loss 0.39474348723888397 ||\n","epoch 1:(8/293) batch || training time for 2 batch 3.0766124725341797 || training loss 0.16901517659425735 ||\n","epoch 1:(10/293) batch || training time for 2 batch 3.0242440700531006 || training loss 0.2449638545513153 ||\n","epoch 1:(12/293) batch || training time for 2 batch 3.0087907314300537 || training loss 0.2399972528219223 ||\n","epoch 1:(14/293) batch || training time for 2 batch 3.018320322036743 || training loss 0.19222483038902283 ||\n","epoch 1:(16/293) batch || training time for 2 batch 3.1521503925323486 || training loss 0.280074767768383 ||\n","epoch 1:(18/293) batch || training time for 2 batch 3.1251060962677 || training loss 0.32595670223236084 ||\n","epoch 1:(20/293) batch || training time for 2 batch 3.2270569801330566 || training loss 0.3740376830101013 ||\n","epoch 1:(22/293) batch || training time for 2 batch 3.1287403106689453 || training loss 0.3250165581703186 ||\n","epoch 1:(24/293) batch || training time for 2 batch 3.0097010135650635 || training loss 0.22850091755390167 ||\n","epoch 1:(26/293) batch || training time for 2 batch 3.053565502166748 || training loss 0.21480391174554825 ||\n","epoch 1:(28/293) batch || training time for 2 batch 3.1552510261535645 || training loss 0.36607709527015686 ||\n","epoch 1:(30/293) batch || training time for 2 batch 3.0220954418182373 || training loss 0.21317097544670105 ||\n","epoch 1:(32/293) batch || training time for 2 batch 3.047593832015991 || training loss 0.2705497741699219 ||\n","epoch 1:(34/293) batch || training time for 2 batch 3.1177268028259277 || training loss 0.368762269616127 ||\n","epoch 1:(36/293) batch || training time for 2 batch 3.306464910507202 || training loss 0.2971937656402588 ||\n","epoch 1:(38/293) batch || training time for 2 batch 3.031355619430542 || training loss 0.3588031381368637 ||\n","epoch 1:(40/293) batch || training time for 2 batch 3.157550811767578 || training loss 0.25666187703609467 ||\n","epoch 1:(42/293) batch || training time for 2 batch 3.047640800476074 || training loss 0.2523128166794777 ||\n","epoch 1:(44/293) batch || training time for 2 batch 3.10359787940979 || training loss 0.31046730279922485 ||\n","epoch 1:(46/293) batch || training time for 2 batch 3.04902982711792 || training loss 0.2894204929471016 ||\n","epoch 1:(48/293) batch || training time for 2 batch 3.0497868061065674 || training loss 0.2738751024007797 ||\n","epoch 1:(50/293) batch || training time for 2 batch 2.9750256538391113 || training loss 0.21574848145246506 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 16.987610578536987s\n","epoch 1:(52/293) batch || training time for 2 batch 23.513787746429443 || training loss 0.3175153136253357 ||\n","epoch 1:(54/293) batch || training time for 2 batch 3.029587745666504 || training loss 0.30969128012657166 ||\n","epoch 1:(56/293) batch || training time for 2 batch 3.3774890899658203 || training loss 0.22285676002502441 ||\n","epoch 1:(58/293) batch || training time for 2 batch 3.035534620285034 || training loss 0.2191581204533577 ||\n","epoch 1:(60/293) batch || training time for 2 batch 3.0363657474517822 || training loss 0.21554312109947205 ||\n","epoch 1:(62/293) batch || training time for 2 batch 3.0878655910491943 || training loss 0.2628358453512192 ||\n","epoch 1:(64/293) batch || training time for 2 batch 3.126688241958618 || training loss 0.2628745138645172 ||\n","epoch 1:(66/293) batch || training time for 2 batch 3.1750106811523438 || training loss 0.2660578340291977 ||\n","epoch 1:(68/293) batch || training time for 2 batch 3.2925527095794678 || training loss 0.3042341321706772 ||\n","epoch 1:(70/293) batch || training time for 2 batch 3.309624671936035 || training loss 0.36544308066368103 ||\n","epoch 1:(72/293) batch || training time for 2 batch 3.0920705795288086 || training loss 0.22787633538246155 ||\n","epoch 1:(74/293) batch || training time for 2 batch 3.2031242847442627 || training loss 0.2720194160938263 ||\n","epoch 1:(76/293) batch || training time for 2 batch 3.077444314956665 || training loss 0.22162196040153503 ||\n","epoch 1:(78/293) batch || training time for 2 batch 3.047980546951294 || training loss 0.25929535180330276 ||\n","epoch 1:(80/293) batch || training time for 2 batch 3.280705213546753 || training loss 0.38524913787841797 ||\n","epoch 1:(82/293) batch || training time for 2 batch 3.1552207469940186 || training loss 0.2599682956933975 ||\n","epoch 1:(84/293) batch || training time for 2 batch 3.1314165592193604 || training loss 0.2596796900033951 ||\n","epoch 1:(86/293) batch || training time for 2 batch 3.0842432975769043 || training loss 0.3100194185972214 ||\n","epoch 1:(88/293) batch || training time for 2 batch 3.0911171436309814 || training loss 0.29136715829372406 ||\n","epoch 1:(90/293) batch || training time for 2 batch 3.0557663440704346 || training loss 0.25358036905527115 ||\n","epoch 1:(92/293) batch || training time for 2 batch 3.1641488075256348 || training loss 0.29293251037597656 ||\n","epoch 1:(94/293) batch || training time for 2 batch 3.0000345706939697 || training loss 0.12806250154972076 ||\n","epoch 1:(96/293) batch || training time for 2 batch 3.0845868587493896 || training loss 0.35283221304416656 ||\n","epoch 1:(98/293) batch || training time for 2 batch 2.9729912281036377 || training loss 0.29986725747585297 ||\n","epoch 1:(100/293) batch || training time for 2 batch 3.1157894134521484 || training loss 0.2560788094997406 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 16.793508052825928s\n","epoch 1:(102/293) batch || training time for 2 batch 23.221318244934082 || training loss 0.21093812584877014 ||\n","epoch 1:(104/293) batch || training time for 2 batch 3.0664286613464355 || training loss 0.2641666829586029 ||\n","epoch 1:(106/293) batch || training time for 2 batch 3.007307529449463 || training loss 0.21233076602220535 ||\n","epoch 1:(108/293) batch || training time for 2 batch 3.120060920715332 || training loss 0.21632461994886398 ||\n","epoch 1:(110/293) batch || training time for 2 batch 3.1624104976654053 || training loss 0.24847343564033508 ||\n","epoch 1:(112/293) batch || training time for 2 batch 3.0810434818267822 || training loss 0.2597302570939064 ||\n","epoch 1:(114/293) batch || training time for 2 batch 3.0607433319091797 || training loss 0.24515699595212936 ||\n","epoch 1:(116/293) batch || training time for 2 batch 3.055210590362549 || training loss 0.2100744992494583 ||\n","epoch 1:(118/293) batch || training time for 2 batch 3.1293342113494873 || training loss 0.2500991076231003 ||\n","epoch 1:(120/293) batch || training time for 2 batch 3.1457912921905518 || training loss 0.27666863799095154 ||\n","epoch 1:(122/293) batch || training time for 2 batch 3.2142112255096436 || training loss 0.250418484210968 ||\n","epoch 1:(124/293) batch || training time for 2 batch 3.1383512020111084 || training loss 0.28412266075611115 ||\n","epoch 1:(126/293) batch || training time for 2 batch 3.231318712234497 || training loss 0.3540981411933899 ||\n","epoch 1:(128/293) batch || training time for 2 batch 3.1960949897766113 || training loss 0.24584947526454926 ||\n","epoch 1:(130/293) batch || training time for 2 batch 3.1335408687591553 || training loss 0.2966390922665596 ||\n","epoch 1:(132/293) batch || training time for 2 batch 2.9830660820007324 || training loss 0.12787249311804771 ||\n","epoch 1:(134/293) batch || training time for 2 batch 3.1874570846557617 || training loss 0.2485910728573799 ||\n","epoch 1:(136/293) batch || training time for 2 batch 3.002138376235962 || training loss 0.19911683350801468 ||\n","epoch 1:(138/293) batch || training time for 2 batch 3.1161022186279297 || training loss 0.2887231111526489 ||\n","epoch 1:(140/293) batch || training time for 2 batch 3.1592535972595215 || training loss 0.23596176505088806 ||\n","epoch 1:(142/293) batch || training time for 2 batch 3.1937036514282227 || training loss 0.27513018250465393 ||\n","epoch 1:(144/293) batch || training time for 2 batch 3.05977201461792 || training loss 0.2924899458885193 ||\n","epoch 1:(146/293) batch || training time for 2 batch 3.102940559387207 || training loss 0.20860663056373596 ||\n","epoch 1:(148/293) batch || training time for 2 batch 2.957728385925293 || training loss 0.17340217530727386 ||\n","epoch 1:(150/293) batch || training time for 2 batch 3.1323394775390625 || training loss 0.3287181556224823 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 17.12043857574463s\n","epoch 1:(152/293) batch || training time for 2 batch 23.553541898727417 || training loss 0.17257314920425415 ||\n","epoch 1:(154/293) batch || training time for 2 batch 3.101548910140991 || training loss 0.3519248217344284 ||\n","epoch 1:(156/293) batch || training time for 2 batch 3.0937843322753906 || training loss 0.2314726561307907 ||\n","epoch 1:(158/293) batch || training time for 2 batch 3.0445289611816406 || training loss 0.29287442564964294 ||\n","epoch 1:(160/293) batch || training time for 2 batch 3.047741413116455 || training loss 0.2546530067920685 ||\n","epoch 1:(162/293) batch || training time for 2 batch 3.0863499641418457 || training loss 0.23951398581266403 ||\n","epoch 1:(164/293) batch || training time for 2 batch 3.2114648818969727 || training loss 0.2671024948358536 ||\n","epoch 1:(166/293) batch || training time for 2 batch 3.140815019607544 || training loss 0.20697453618049622 ||\n","epoch 1:(168/293) batch || training time for 2 batch 3.079707384109497 || training loss 0.20686546713113785 ||\n","epoch 1:(170/293) batch || training time for 2 batch 3.193115711212158 || training loss 0.28136326372623444 ||\n","epoch 1:(172/293) batch || training time for 2 batch 3.18044114112854 || training loss 0.23985575884580612 ||\n","epoch 1:(174/293) batch || training time for 2 batch 3.190441370010376 || training loss 0.27711015939712524 ||\n","epoch 1:(176/293) batch || training time for 2 batch 3.202575922012329 || training loss 0.2364058941602707 ||\n","epoch 1:(178/293) batch || training time for 2 batch 3.1287527084350586 || training loss 0.25979354977607727 ||\n","epoch 1:(180/293) batch || training time for 2 batch 3.0871074199676514 || training loss 0.25150614976882935 ||\n","epoch 1:(182/293) batch || training time for 2 batch 3.0676167011260986 || training loss 0.23755715787410736 ||\n","epoch 1:(184/293) batch || training time for 2 batch 3.0612826347351074 || training loss 0.2226041704416275 ||\n","epoch 1:(186/293) batch || training time for 2 batch 3.0843563079833984 || training loss 0.25831155478954315 ||\n","epoch 1:(188/293) batch || training time for 2 batch 3.0361506938934326 || training loss 0.2312452346086502 ||\n","epoch 1:(190/293) batch || training time for 2 batch 3.005563735961914 || training loss 0.27779124677181244 ||\n","epoch 1:(192/293) batch || training time for 2 batch 2.9640748500823975 || training loss 0.16362704522907734 ||\n","epoch 1:(194/293) batch || training time for 2 batch 2.944636821746826 || training loss 0.1645815521478653 ||\n","epoch 1:(196/293) batch || training time for 2 batch 3.057427167892456 || training loss 0.2633729577064514 ||\n","epoch 1:(198/293) batch || training time for 2 batch 3.1740236282348633 || training loss 0.2313801571726799 ||\n","2.0971520000000006e-05\n","epoch 1:(200/293) batch || training time for 2 batch 3.023963212966919 || training loss 0.236683189868927 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 16.972295999526978s\n","epoch 1:(202/293) batch || training time for 2 batch 23.37904930114746 || training loss 0.2727114111185074 ||\n","epoch 1:(204/293) batch || training time for 2 batch 2.952225923538208 || training loss 0.17465884238481522 ||\n","epoch 1:(206/293) batch || training time for 2 batch 3.1382863521575928 || training loss 0.2661293298006058 ||\n","epoch 1:(208/293) batch || training time for 2 batch 3.101863145828247 || training loss 0.2862331122159958 ||\n","epoch 1:(210/293) batch || training time for 2 batch 3.061014175415039 || training loss 0.27556103467941284 ||\n","epoch 1:(212/293) batch || training time for 2 batch 3.0925121307373047 || training loss 0.2355722337961197 ||\n","epoch 1:(214/293) batch || training time for 2 batch 3.0450634956359863 || training loss 0.1596803367137909 ||\n","epoch 1:(216/293) batch || training time for 2 batch 3.048095226287842 || training loss 0.23767812550067902 ||\n","epoch 1:(218/293) batch || training time for 2 batch 3.42413592338562 || training loss 0.23754321038722992 ||\n","epoch 1:(220/293) batch || training time for 2 batch 3.163012742996216 || training loss 0.23851772397756577 ||\n","epoch 1:(222/293) batch || training time for 2 batch 3.1550512313842773 || training loss 0.35199500620365143 ||\n","epoch 1:(224/293) batch || training time for 2 batch 3.189329147338867 || training loss 0.3456561416387558 ||\n","epoch 1:(226/293) batch || training time for 2 batch 3.052036762237549 || training loss 0.16463878005743027 ||\n","epoch 1:(228/293) batch || training time for 2 batch 3.0688905715942383 || training loss 0.18910156935453415 ||\n","epoch 1:(230/293) batch || training time for 2 batch 3.042945146560669 || training loss 0.20076964050531387 ||\n","epoch 1:(232/293) batch || training time for 2 batch 3.0169517993927 || training loss 0.22669441998004913 ||\n","epoch 1:(234/293) batch || training time for 2 batch 3.0316243171691895 || training loss 0.20094776898622513 ||\n","epoch 1:(236/293) batch || training time for 2 batch 3.0725646018981934 || training loss 0.26718682795763016 ||\n","epoch 1:(238/293) batch || training time for 2 batch 3.0198628902435303 || training loss 0.21272820979356766 ||\n","epoch 1:(240/293) batch || training time for 2 batch 3.3770899772644043 || training loss 0.26996177434921265 ||\n","epoch 1:(242/293) batch || training time for 2 batch 3.0541257858276367 || training loss 0.22826170921325684 ||\n","epoch 1:(244/293) batch || training time for 2 batch 3.0197184085845947 || training loss 0.15681423246860504 ||\n","epoch 1:(246/293) batch || training time for 2 batch 3.0245440006256104 || training loss 0.23029626160860062 ||\n","epoch 1:(248/293) batch || training time for 2 batch 3.100520610809326 || training loss 0.12531238794326782 ||\n","epoch 1:(250/293) batch || training time for 2 batch 3.0849859714508057 || training loss 0.31605492532253265 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 16.832850456237793s\n","epoch 1:(252/293) batch || training time for 2 batch 23.365718603134155 || training loss 0.21360668540000916 ||\n","epoch 1:(254/293) batch || training time for 2 batch 3.161959648132324 || training loss 0.23107165098190308 ||\n","epoch 1:(256/293) batch || training time for 2 batch 2.9967024326324463 || training loss 0.16826441138982773 ||\n","epoch 1:(258/293) batch || training time for 2 batch 2.9714951515197754 || training loss 0.2427162230014801 ||\n","epoch 1:(260/293) batch || training time for 2 batch 3.050170660018921 || training loss 0.1959328055381775 ||\n","epoch 1:(262/293) batch || training time for 2 batch 3.001310348510742 || training loss 0.19360467791557312 ||\n","epoch 1:(264/293) batch || training time for 2 batch 3.1549715995788574 || training loss 0.2702041417360306 ||\n","epoch 1:(266/293) batch || training time for 2 batch 3.1307740211486816 || training loss 0.29941631853580475 ||\n","epoch 1:(268/293) batch || training time for 2 batch 3.1521451473236084 || training loss 0.20670324563980103 ||\n","epoch 1:(270/293) batch || training time for 2 batch 3.1539061069488525 || training loss 0.287944033741951 ||\n","epoch 1:(272/293) batch || training time for 2 batch 3.1407151222229004 || training loss 0.2743356376886368 ||\n","epoch 1:(274/293) batch || training time for 2 batch 3.180460214614868 || training loss 0.26827293634414673 ||\n","epoch 1:(276/293) batch || training time for 2 batch 3.2122294902801514 || training loss 0.23727966845035553 ||\n","epoch 1:(278/293) batch || training time for 2 batch 3.1742732524871826 || training loss 0.2863493263721466 ||\n","epoch 1:(280/293) batch || training time for 2 batch 3.0262014865875244 || training loss 0.16294827312231064 ||\n","epoch 1:(282/293) batch || training time for 2 batch 3.0729012489318848 || training loss 0.27040739357471466 ||\n","epoch 1:(284/293) batch || training time for 2 batch 3.061728000640869 || training loss 0.15510493516921997 ||\n","epoch 1:(286/293) batch || training time for 2 batch 3.067894220352173 || training loss 0.2618994116783142 ||\n","epoch 1:(288/293) batch || training time for 2 batch 3.0804049968719482 || training loss 0.33602017164230347 ||\n","epoch 1:(290/293) batch || training time for 2 batch 2.955488681793213 || training loss 0.12635462544858456 ||\n","epoch 1:(292/293) batch || training time for 2 batch 3.1145997047424316 || training loss 0.23108941316604614 ||\n","epoch 2:(2/293) batch || training time for 2 batch 4.368424415588379 || training loss 0.14872587844729424 ||\n","epoch 2:(4/293) batch || training time for 2 batch 2.9446213245391846 || training loss 0.12471955083310604 ||\n","epoch 2:(6/293) batch || training time for 2 batch 2.959505319595337 || training loss 0.17981333285570145 ||\n","epoch 2:(8/293) batch || training time for 2 batch 2.9384422302246094 || training loss 0.1195925809442997 ||\n","epoch 2:(10/293) batch || training time for 2 batch 2.90639066696167 || training loss 0.05832049064338207 ||\n","epoch 2:(12/293) batch || training time for 2 batch 2.908843517303467 || training loss 0.05032365210354328 ||\n","epoch 2:(14/293) batch || training time for 2 batch 2.9335713386535645 || training loss 0.08211219310760498 ||\n","epoch 2:(16/293) batch || training time for 2 batch 2.9467334747314453 || training loss 0.06561322510242462 ||\n","epoch 2:(18/293) batch || training time for 2 batch 2.931189775466919 || training loss 0.13509626686573029 ||\n","epoch 2:(20/293) batch || training time for 2 batch 3.032968282699585 || training loss 0.12038367614150047 ||\n","epoch 2:(22/293) batch || training time for 2 batch 2.9980030059814453 || training loss 0.12048762664198875 ||\n","epoch 2:(24/293) batch || training time for 2 batch 2.977485418319702 || training loss 0.08011001721024513 ||\n","epoch 2:(26/293) batch || training time for 2 batch 3.06182861328125 || training loss 0.20627156645059586 ||\n","epoch 2:(28/293) batch || training time for 2 batch 2.9786274433135986 || training loss 0.1377546340227127 ||\n","epoch 2:(30/293) batch || training time for 2 batch 3.0128355026245117 || training loss 0.15633536130189896 ||\n","epoch 2:(32/293) batch || training time for 2 batch 3.0325372219085693 || training loss 0.10786500200629234 ||\n","epoch 2:(34/293) batch || training time for 2 batch 3.036768674850464 || training loss 0.15400110185146332 ||\n","epoch 2:(36/293) batch || training time for 2 batch 3.0335049629211426 || training loss 0.15122342109680176 ||\n","epoch 2:(38/293) batch || training time for 2 batch 2.9721500873565674 || training loss 0.055299608036875725 ||\n","epoch 2:(40/293) batch || training time for 2 batch 3.0221688747406006 || training loss 0.15028708055615425 ||\n","epoch 2:(42/293) batch || training time for 2 batch 2.9818601608276367 || training loss 0.08988221734762192 ||\n","epoch 2:(44/293) batch || training time for 2 batch 3.003274440765381 || training loss 0.0754486620426178 ||\n","epoch 2:(46/293) batch || training time for 2 batch 3.0342278480529785 || training loss 0.19579415023326874 ||\n","epoch 2:(48/293) batch || training time for 2 batch 3.040935516357422 || training loss 0.15828825533390045 ||\n","epoch 2:(50/293) batch || training time for 2 batch 2.942195177078247 || training loss 0.05921940132975578 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 17.043299198150635s\n","epoch 2:(52/293) batch || training time for 2 batch 23.427640438079834 || training loss 0.23477652668952942 ||\n","epoch 2:(54/293) batch || training time for 2 batch 2.89025616645813 || training loss 0.07646138966083527 ||\n","epoch 2:(56/293) batch || training time for 2 batch 2.9289369583129883 || training loss 0.11127057299017906 ||\n","epoch 2:(58/293) batch || training time for 2 batch 2.9245669841766357 || training loss 0.09242510609328747 ||\n","epoch 2:(60/293) batch || training time for 2 batch 2.9872913360595703 || training loss 0.11112231761217117 ||\n","epoch 2:(62/293) batch || training time for 2 batch 3.05436110496521 || training loss 0.2249433696269989 ||\n","epoch 2:(64/293) batch || training time for 2 batch 2.990354537963867 || training loss 0.11938454210758209 ||\n","epoch 2:(66/293) batch || training time for 2 batch 2.961228847503662 || training loss 0.07442601770162582 ||\n","epoch 2:(68/293) batch || training time for 2 batch 3.0134623050689697 || training loss 0.07425636053085327 ||\n","epoch 2:(70/293) batch || training time for 2 batch 3.043264865875244 || training loss 0.04528283514082432 ||\n","epoch 2:(72/293) batch || training time for 2 batch 3.0860671997070312 || training loss 0.15871428698301315 ||\n","epoch 2:(74/293) batch || training time for 2 batch 3.0310351848602295 || training loss 0.030840342864394188 ||\n","epoch 2:(76/293) batch || training time for 2 batch 3.065054178237915 || training loss 0.10979646444320679 ||\n","epoch 2:(78/293) batch || training time for 2 batch 3.038632869720459 || training loss 0.07671137899160385 ||\n","epoch 2:(80/293) batch || training time for 2 batch 3.0665955543518066 || training loss 0.14525867998600006 ||\n","epoch 2:(82/293) batch || training time for 2 batch 3.058332681655884 || training loss 0.19195497035980225 ||\n","epoch 2:(84/293) batch || training time for 2 batch 3.0191760063171387 || training loss 0.10854934807866812 ||\n","epoch 2:(86/293) batch || training time for 2 batch 3.0582082271575928 || training loss 0.10496528819203377 ||\n","epoch 2:(88/293) batch || training time for 2 batch 2.9981000423431396 || training loss 0.14692963287234306 ||\n","epoch 2:(90/293) batch || training time for 2 batch 2.9826462268829346 || training loss 0.11149729788303375 ||\n","epoch 2:(92/293) batch || training time for 2 batch 3.0353918075561523 || training loss 0.1913291960954666 ||\n","epoch 2:(94/293) batch || training time for 2 batch 2.9427506923675537 || training loss 0.06662083137780428 ||\n","epoch 2:(96/293) batch || training time for 2 batch 2.90261173248291 || training loss 0.030475061386823654 ||\n","epoch 2:(98/293) batch || training time for 2 batch 2.9473001956939697 || training loss 0.14721358567476273 ||\n","epoch 2:(100/293) batch || training time for 2 batch 2.9553654193878174 || training loss 0.11307736486196518 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 16.78284478187561s\n","epoch 2:(102/293) batch || training time for 2 batch 23.201390743255615 || training loss 0.1130467876791954 ||\n","epoch 2:(104/293) batch || training time for 2 batch 2.9228932857513428 || training loss 0.14559319987893105 ||\n","epoch 2:(106/293) batch || training time for 2 batch 2.887009620666504 || training loss 0.06684141606092453 ||\n","epoch 2:(108/293) batch || training time for 2 batch 3.0420334339141846 || training loss 0.1488681472837925 ||\n","epoch 2:(110/293) batch || training time for 2 batch 2.9516422748565674 || training loss 0.17289535515010357 ||\n","epoch 2:(112/293) batch || training time for 2 batch 3.110579252243042 || training loss 0.24263710528612137 ||\n","epoch 2:(114/293) batch || training time for 2 batch 3.018754720687866 || training loss 0.10846506524831057 ||\n","epoch 2:(116/293) batch || training time for 2 batch 3.0428783893585205 || training loss 0.18735942244529724 ||\n","epoch 2:(118/293) batch || training time for 2 batch 3.052098274230957 || training loss 0.11054256930947304 ||\n","epoch 2:(120/293) batch || training time for 2 batch 3.034921407699585 || training loss 0.068232461810112 ||\n","epoch 2:(122/293) batch || training time for 2 batch 3.051957130432129 || training loss 0.02779177762567997 ||\n","epoch 2:(124/293) batch || training time for 2 batch 3.048431396484375 || training loss 0.026723587885499 ||\n","epoch 2:(126/293) batch || training time for 2 batch 3.078233003616333 || training loss 0.08685489743947983 ||\n","epoch 2:(128/293) batch || training time for 2 batch 3.020494222640991 || training loss 0.06719955056905746 ||\n","epoch 2:(130/293) batch || training time for 2 batch 3.030963182449341 || training loss 0.10904521867632866 ||\n","epoch 2:(132/293) batch || training time for 2 batch 3.050686836242676 || training loss 0.08672324940562248 ||\n","epoch 2:(134/293) batch || training time for 2 batch 3.0110697746276855 || training loss 0.10730070993304253 ||\n","epoch 2:(136/293) batch || training time for 2 batch 2.951183795928955 || training loss 0.12862836197018623 ||\n","epoch 2:(138/293) batch || training time for 2 batch 2.955350160598755 || training loss 0.05848769471049309 ||\n","epoch 2:(140/293) batch || training time for 2 batch 2.9923133850097656 || training loss 0.2024896964430809 ||\n","epoch 2:(142/293) batch || training time for 2 batch 2.96728777885437 || training loss 0.14923828840255737 ||\n","epoch 2:(144/293) batch || training time for 2 batch 2.9800479412078857 || training loss 0.15073717758059502 ||\n","epoch 2:(146/293) batch || training time for 2 batch 2.928269147872925 || training loss 0.06631451845169067 ||\n","epoch 2:(148/293) batch || training time for 2 batch 2.901768207550049 || training loss 0.02125554997473955 ||\n","epoch 2:(150/293) batch || training time for 2 batch 2.9280245304107666 || training loss 0.0648764194920659 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 17.242009162902832s\n","epoch 2:(152/293) batch || training time for 2 batch 23.66321563720703 || training loss 0.14966968446969986 ||\n","epoch 2:(154/293) batch || training time for 2 batch 2.8848562240600586 || training loss 0.10707141552120447 ||\n","epoch 2:(156/293) batch || training time for 2 batch 2.9310038089752197 || training loss 0.10896188765764236 ||\n","epoch 2:(158/293) batch || training time for 2 batch 2.9707553386688232 || training loss 0.19509240984916687 ||\n","epoch 2:(160/293) batch || training time for 2 batch 2.9587807655334473 || training loss 0.06530804559588432 ||\n","epoch 2:(162/293) batch || training time for 2 batch 2.9907922744750977 || training loss 0.1462779864668846 ||\n","epoch 2:(164/293) batch || training time for 2 batch 3.023484230041504 || training loss 0.19076962769031525 ||\n","epoch 2:(166/293) batch || training time for 2 batch 2.9791369438171387 || training loss 0.04419782478362322 ||\n","epoch 2:(168/293) batch || training time for 2 batch 3.0269155502319336 || training loss 0.06463058199733496 ||\n","epoch 2:(170/293) batch || training time for 2 batch 3.026210069656372 || training loss 0.10870150476694107 ||\n","epoch 2:(172/293) batch || training time for 2 batch 3.0461344718933105 || training loss 0.14943603798747063 ||\n","epoch 2:(174/293) batch || training time for 2 batch 3.0832128524780273 || training loss 0.14429232478141785 ||\n","epoch 2:(176/293) batch || training time for 2 batch 3.0881762504577637 || training loss 0.10494250059127808 ||\n","epoch 2:(178/293) batch || training time for 2 batch 3.075134038925171 || training loss 0.10702453646808863 ||\n","epoch 2:(180/293) batch || training time for 2 batch 3.059217929840088 || training loss 0.15722569823265076 ||\n","epoch 2:(182/293) batch || training time for 2 batch 2.9994380474090576 || training loss 0.10538188740611076 ||\n","epoch 2:(184/293) batch || training time for 2 batch 3.02372407913208 || training loss 0.10168663784861565 ||\n","epoch 2:(186/293) batch || training time for 2 batch 2.9143881797790527 || training loss 0.025760553777217865 ||\n","epoch 2:(188/293) batch || training time for 2 batch 2.956132411956787 || training loss 0.0637265108525753 ||\n","epoch 2:(190/293) batch || training time for 2 batch 2.9401817321777344 || training loss 0.0611159848049283 ||\n","epoch 2:(192/293) batch || training time for 2 batch 2.9332520961761475 || training loss 0.06174628250300884 ||\n","epoch 2:(194/293) batch || training time for 2 batch 2.913459300994873 || training loss 0.107909195125103 ||\n","epoch 2:(196/293) batch || training time for 2 batch 2.9487757682800293 || training loss 0.10400984436273575 ||\n","epoch 2:(198/293) batch || training time for 2 batch 2.9519784450531006 || training loss 0.1545088030397892 ||\n","1.6777216000000006e-05\n","epoch 2:(200/293) batch || training time for 2 batch 2.9678773880004883 || training loss 0.1496143788099289 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 16.853699445724487s\n","epoch 2:(202/293) batch || training time for 2 batch 23.179683685302734 || training loss 0.10889235138893127 ||\n","epoch 2:(204/293) batch || training time for 2 batch 2.9207379817962646 || training loss 0.10781321674585342 ||\n","epoch 2:(206/293) batch || training time for 2 batch 2.923056125640869 || training loss 0.060796450823545456 ||\n","epoch 2:(208/293) batch || training time for 2 batch 2.916839122772217 || training loss 0.06374807469546795 ||\n","epoch 2:(210/293) batch || training time for 2 batch 2.990297317504883 || training loss 0.10800028592348099 ||\n","epoch 2:(212/293) batch || training time for 2 batch 2.9718129634857178 || training loss 0.10820966586470604 ||\n","epoch 2:(214/293) batch || training time for 2 batch 2.9786174297332764 || training loss 0.1064282339066267 ||\n","epoch 2:(216/293) batch || training time for 2 batch 3.055370330810547 || training loss 0.26292889565229416 ||\n","epoch 2:(218/293) batch || training time for 2 batch 3.0624947547912598 || training loss 0.23392577469348907 ||\n","epoch 2:(220/293) batch || training time for 2 batch 3.065291404724121 || training loss 0.1983458399772644 ||\n","epoch 2:(222/293) batch || training time for 2 batch 3.0722739696502686 || training loss 0.14246095344424248 ||\n","epoch 2:(224/293) batch || training time for 2 batch 3.088817596435547 || training loss 0.1868799701333046 ||\n","epoch 2:(226/293) batch || training time for 2 batch 3.034449815750122 || training loss 0.07194293476641178 ||\n","epoch 2:(228/293) batch || training time for 2 batch 3.040595769882202 || training loss 0.10791787505149841 ||\n","epoch 2:(230/293) batch || training time for 2 batch 3.0324180126190186 || training loss 0.1475449651479721 ||\n","epoch 2:(232/293) batch || training time for 2 batch 2.986237049102783 || training loss 0.020012824796140194 ||\n","epoch 2:(234/293) batch || training time for 2 batch 3.0230064392089844 || training loss 0.10733712092041969 ||\n","epoch 2:(236/293) batch || training time for 2 batch 3.022508144378662 || training loss 0.15548770129680634 ||\n","epoch 2:(238/293) batch || training time for 2 batch 3.005980968475342 || training loss 0.1454077512025833 ||\n","epoch 2:(240/293) batch || training time for 2 batch 3.0104615688323975 || training loss 0.11183570045977831 ||\n","epoch 2:(242/293) batch || training time for 2 batch 2.9324822425842285 || training loss 0.2430453822016716 ||\n","epoch 2:(244/293) batch || training time for 2 batch 2.9336841106414795 || training loss 0.14902392402291298 ||\n","epoch 2:(246/293) batch || training time for 2 batch 2.9694089889526367 || training loss 0.10528119280934334 ||\n","epoch 2:(248/293) batch || training time for 2 batch 2.9387667179107666 || training loss 0.10163556970655918 ||\n","epoch 2:(250/293) batch || training time for 2 batch 2.8971505165100098 || training loss 0.08412192948162556 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 17.187508821487427s\n","epoch 2:(252/293) batch || training time for 2 batch 23.60098361968994 || training loss 0.10615865886211395 ||\n","epoch 2:(254/293) batch || training time for 2 batch 2.962120771408081 || training loss 0.2396020144224167 ||\n","epoch 2:(256/293) batch || training time for 2 batch 2.9129738807678223 || training loss 0.06514486856758595 ||\n","epoch 2:(258/293) batch || training time for 2 batch 2.9549834728240967 || training loss 0.10527530685067177 ||\n","epoch 2:(260/293) batch || training time for 2 batch 2.9171268939971924 || training loss 0.06587798148393631 ||\n","epoch 2:(262/293) batch || training time for 2 batch 2.9829702377319336 || training loss 0.1282355710864067 ||\n","epoch 2:(264/293) batch || training time for 2 batch 3.0091099739074707 || training loss 0.10836237296462059 ||\n","epoch 2:(266/293) batch || training time for 2 batch 2.986666202545166 || training loss 0.06354893743991852 ||\n","epoch 2:(268/293) batch || training time for 2 batch 3.000741720199585 || training loss 0.10601117461919785 ||\n","epoch 2:(270/293) batch || training time for 2 batch 3.0545363426208496 || training loss 0.10740476846694946 ||\n","epoch 2:(272/293) batch || training time for 2 batch 3.065873146057129 || training loss 0.14724759757518768 ||\n","epoch 2:(274/293) batch || training time for 2 batch 3.0695531368255615 || training loss 0.10822887718677521 ||\n","epoch 2:(276/293) batch || training time for 2 batch 3.0690200328826904 || training loss 0.14311357587575912 ||\n","epoch 2:(278/293) batch || training time for 2 batch 3.0176308155059814 || training loss 0.06601682864129543 ||\n","epoch 2:(280/293) batch || training time for 2 batch 3.038447141647339 || training loss 0.060541607439517975 ||\n","epoch 2:(282/293) batch || training time for 2 batch 3.027033805847168 || training loss 0.1467132493853569 ||\n","epoch 2:(284/293) batch || training time for 2 batch 3.0557010173797607 || training loss 0.1950528472661972 ||\n","epoch 2:(286/293) batch || training time for 2 batch 3.017087459564209 || training loss 0.21948207169771194 ||\n","epoch 2:(288/293) batch || training time for 2 batch 2.9321212768554688 || training loss 0.02312173042446375 ||\n","epoch 2:(290/293) batch || training time for 2 batch 2.9864556789398193 || training loss 0.1450302079319954 ||\n","epoch 2:(292/293) batch || training time for 2 batch 2.9170684814453125 || training loss 0.0204972792416811 ||\n","epoch 3:(2/293) batch || training time for 2 batch 4.389779090881348 || training loss 0.1544501855969429 ||\n","epoch 3:(4/293) batch || training time for 2 batch 2.9085018634796143 || training loss 0.023797590285539627 ||\n","epoch 3:(6/293) batch || training time for 2 batch 2.9210429191589355 || training loss 0.06091406010091305 ||\n","epoch 3:(8/293) batch || training time for 2 batch 2.9453186988830566 || training loss 0.0626550130546093 ||\n","epoch 3:(10/293) batch || training time for 2 batch 2.917167901992798 || training loss 0.017778117209672928 ||\n","epoch 3:(12/293) batch || training time for 2 batch 2.8909189701080322 || training loss 0.019635653123259544 ||\n","epoch 3:(14/293) batch || training time for 2 batch 2.9439890384674072 || training loss 0.061099844984710217 ||\n","epoch 3:(16/293) batch || training time for 2 batch 2.962033748626709 || training loss 0.061865548603236675 ||\n","epoch 3:(18/293) batch || training time for 2 batch 2.9101576805114746 || training loss 0.039036231115460396 ||\n","epoch 3:(20/293) batch || training time for 2 batch 2.9625115394592285 || training loss 0.0622247438877821 ||\n","epoch 3:(22/293) batch || training time for 2 batch 2.9484329223632812 || training loss 0.06018627807497978 ||\n","epoch 3:(24/293) batch || training time for 2 batch 2.950652837753296 || training loss 0.07040330953896046 ||\n","epoch 3:(26/293) batch || training time for 2 batch 2.929626941680908 || training loss 0.018532581627368927 ||\n","epoch 3:(28/293) batch || training time for 2 batch 2.9341344833374023 || training loss 0.062470315024256706 ||\n","epoch 3:(30/293) batch || training time for 2 batch 2.993241310119629 || training loss 0.05740842316299677 ||\n","epoch 3:(32/293) batch || training time for 2 batch 2.954019546508789 || training loss 0.019164412282407284 ||\n","epoch 3:(34/293) batch || training time for 2 batch 2.9677069187164307 || training loss 0.016529809683561325 ||\n","epoch 3:(36/293) batch || training time for 2 batch 3.024317741394043 || training loss 0.12703139334917068 ||\n","epoch 3:(38/293) batch || training time for 2 batch 2.9724068641662598 || training loss 0.016594214364886284 ||\n","epoch 3:(40/293) batch || training time for 2 batch 3.012657880783081 || training loss 0.08241374045610428 ||\n","epoch 3:(42/293) batch || training time for 2 batch 2.9669992923736572 || training loss 0.01582259964197874 ||\n","epoch 3:(44/293) batch || training time for 2 batch 2.9366295337677 || training loss 0.018013816326856613 ||\n","epoch 3:(46/293) batch || training time for 2 batch 2.94687557220459 || training loss 0.015513991937041283 ||\n","epoch 3:(48/293) batch || training time for 2 batch 2.945361852645874 || training loss 0.01524437591433525 ||\n","epoch 3:(50/293) batch || training time for 2 batch 2.929196834564209 || training loss 0.015528498217463493 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 17.020874977111816s\n","epoch 3:(52/293) batch || training time for 2 batch 23.373730421066284 || training loss 0.03616985306143761 ||\n","epoch 3:(54/293) batch || training time for 2 batch 2.8992743492126465 || training loss 0.05724701099097729 ||\n","epoch 3:(56/293) batch || training time for 2 batch 2.9205305576324463 || training loss 0.05884018912911415 ||\n","epoch 3:(58/293) batch || training time for 2 batch 2.874992847442627 || training loss 0.016017393209040165 ||\n","epoch 3:(60/293) batch || training time for 2 batch 2.921111583709717 || training loss 0.07432344742119312 ||\n","epoch 3:(62/293) batch || training time for 2 batch 2.9608795642852783 || training loss 0.013721726834774017 ||\n","epoch 3:(64/293) batch || training time for 2 batch 2.9463467597961426 || training loss 0.05304234009236097 ||\n","epoch 3:(66/293) batch || training time for 2 batch 2.9759459495544434 || training loss 0.07768022082746029 ||\n","epoch 3:(68/293) batch || training time for 2 batch 2.9782660007476807 || training loss 0.030062195844948292 ||\n","epoch 3:(70/293) batch || training time for 2 batch 3.019927501678467 || training loss 0.03594384342432022 ||\n","epoch 3:(72/293) batch || training time for 2 batch 3.010639190673828 || training loss 0.07993018627166748 ||\n","epoch 3:(74/293) batch || training time for 2 batch 3.026049852371216 || training loss 0.012488011736422777 ||\n","epoch 3:(76/293) batch || training time for 2 batch 3.042304754257202 || training loss 0.011099401395767927 ||\n","epoch 3:(78/293) batch || training time for 2 batch 3.0194239616394043 || training loss 0.013087139464914799 ||\n","epoch 3:(80/293) batch || training time for 2 batch 2.9840035438537598 || training loss 0.0573133509606123 ||\n","epoch 3:(82/293) batch || training time for 2 batch 2.971841812133789 || training loss 0.058437738101929426 ||\n","epoch 3:(84/293) batch || training time for 2 batch 2.958352565765381 || training loss 0.05843023583292961 ||\n","epoch 3:(86/293) batch || training time for 2 batch 2.9690449237823486 || training loss 0.05859097931534052 ||\n","epoch 3:(88/293) batch || training time for 2 batch 2.95491886138916 || training loss 0.03512886073440313 ||\n","epoch 3:(90/293) batch || training time for 2 batch 2.9478368759155273 || training loss 0.10721937194466591 ||\n","epoch 3:(92/293) batch || training time for 2 batch 2.9119677543640137 || training loss 0.011656749993562698 ||\n","epoch 3:(94/293) batch || training time for 2 batch 2.8981711864471436 || training loss 0.01233942387625575 ||\n","epoch 3:(96/293) batch || training time for 2 batch 2.899864435195923 || training loss 0.012511727400124073 ||\n","epoch 3:(98/293) batch || training time for 2 batch 2.9063262939453125 || training loss 0.08406287617981434 ||\n","epoch 3:(100/293) batch || training time for 2 batch 2.9300954341888428 || training loss 0.05796531215310097 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 16.68986201286316s\n","epoch 3:(102/293) batch || training time for 2 batch 23.095519065856934 || training loss 0.10598742961883545 ||\n","epoch 3:(104/293) batch || training time for 2 batch 2.892454147338867 || training loss 0.060165236704051495 ||\n","epoch 3:(106/293) batch || training time for 2 batch 2.8900935649871826 || training loss 0.06707200780510902 ||\n","epoch 3:(108/293) batch || training time for 2 batch 2.9187746047973633 || training loss 0.05909356568008661 ||\n","epoch 3:(110/293) batch || training time for 2 batch 2.9508485794067383 || training loss 0.05804983526468277 ||\n","epoch 3:(112/293) batch || training time for 2 batch 2.9414918422698975 || training loss 0.013459856621921062 ||\n","epoch 3:(114/293) batch || training time for 2 batch 2.9812252521514893 || training loss 0.05599570460617542 ||\n","epoch 3:(116/293) batch || training time for 2 batch 2.992295503616333 || training loss 0.05796042550355196 ||\n","epoch 3:(118/293) batch || training time for 2 batch 3.031465530395508 || training loss 0.1285274475812912 ||\n","epoch 3:(120/293) batch || training time for 2 batch 2.9871253967285156 || training loss 0.013729242142289877 ||\n","epoch 3:(122/293) batch || training time for 2 batch 2.99733304977417 || training loss 0.011580968275666237 ||\n","epoch 3:(124/293) batch || training time for 2 batch 3.001497268676758 || training loss 0.012313525192439556 ||\n","epoch 3:(126/293) batch || training time for 2 batch 3.0580697059631348 || training loss 0.1250760331749916 ||\n","epoch 3:(128/293) batch || training time for 2 batch 3.11271333694458 || training loss 0.05754400044679642 ||\n","epoch 3:(130/293) batch || training time for 2 batch 2.959207057952881 || training loss 0.012939823791384697 ||\n","epoch 3:(132/293) batch || training time for 2 batch 3.0066912174224854 || training loss 0.010480214841663837 ||\n","epoch 3:(134/293) batch || training time for 2 batch 2.9711570739746094 || training loss 0.010983673855662346 ||\n","epoch 3:(136/293) batch || training time for 2 batch 2.971043109893799 || training loss 0.11071885749697685 ||\n","epoch 3:(138/293) batch || training time for 2 batch 2.9997193813323975 || training loss 0.15476149320602417 ||\n","epoch 3:(140/293) batch || training time for 2 batch 2.95943546295166 || training loss 0.056296633556485176 ||\n","epoch 3:(142/293) batch || training time for 2 batch 2.8983843326568604 || training loss 0.010710722301155329 ||\n","epoch 3:(144/293) batch || training time for 2 batch 2.9155638217926025 || training loss 0.056767852045595646 ||\n","epoch 3:(146/293) batch || training time for 2 batch 2.939519166946411 || training loss 0.058424300979822874 ||\n","epoch 3:(148/293) batch || training time for 2 batch 2.8919529914855957 || training loss 0.011114372871816158 ||\n","epoch 3:(150/293) batch || training time for 2 batch 2.8870503902435303 || training loss 0.010554460808634758 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 16.799582719802856s\n","epoch 3:(152/293) batch || training time for 2 batch 23.147234201431274 || training loss 0.011103076860308647 ||\n","epoch 3:(154/293) batch || training time for 2 batch 2.8906090259552 || training loss 0.010548936203122139 ||\n","epoch 3:(156/293) batch || training time for 2 batch 2.903701066970825 || training loss 0.010110542643815279 ||\n","epoch 3:(158/293) batch || training time for 2 batch 2.9379496574401855 || training loss 0.061341199558228254 ||\n","epoch 3:(160/293) batch || training time for 2 batch 2.978799343109131 || training loss 0.10144725441932678 ||\n","epoch 3:(162/293) batch || training time for 2 batch 2.9561052322387695 || training loss 0.05538980755954981 ||\n","epoch 3:(164/293) batch || training time for 2 batch 2.9585952758789062 || training loss 0.05899975635111332 ||\n","epoch 3:(166/293) batch || training time for 2 batch 2.967494249343872 || training loss 0.030779032967984676 ||\n","epoch 3:(168/293) batch || training time for 2 batch 3.022718667984009 || training loss 0.03672461770474911 ||\n","epoch 3:(170/293) batch || training time for 2 batch 3.026125431060791 || training loss 0.057929348200559616 ||\n","epoch 3:(172/293) batch || training time for 2 batch 3.0251383781433105 || training loss 0.03274914342910051 ||\n","epoch 3:(174/293) batch || training time for 2 batch 3.0234806537628174 || training loss 0.00994532834738493 ||\n","epoch 3:(176/293) batch || training time for 2 batch 3.0268232822418213 || training loss 0.010204230435192585 ||\n","epoch 3:(178/293) batch || training time for 2 batch 3.017763137817383 || training loss 0.03215924045071006 ||\n","epoch 3:(180/293) batch || training time for 2 batch 3.015475273132324 || training loss 0.009041950106620789 ||\n","epoch 3:(182/293) batch || training time for 2 batch 2.9965007305145264 || training loss 0.08357298374176025 ||\n","epoch 3:(184/293) batch || training time for 2 batch 2.998248815536499 || training loss 0.05818513687700033 ||\n","epoch 3:(186/293) batch || training time for 2 batch 2.9577629566192627 || training loss 0.10228418186306953 ||\n","epoch 3:(188/293) batch || training time for 2 batch 2.9337244033813477 || training loss 0.008519777562469244 ||\n","epoch 3:(190/293) batch || training time for 2 batch 2.924638032913208 || training loss 0.11765327211469412 ||\n","epoch 3:(192/293) batch || training time for 2 batch 2.908417224884033 || training loss 0.05453679244965315 ||\n","epoch 3:(194/293) batch || training time for 2 batch 2.9174981117248535 || training loss 0.03117180895060301 ||\n","epoch 3:(196/293) batch || training time for 2 batch 2.8934645652770996 || training loss 0.05554666090756655 ||\n","epoch 3:(198/293) batch || training time for 2 batch 2.961667776107788 || training loss 0.05761951208114624 ||\n","1.3421772800000004e-05\n","epoch 3:(200/293) batch || training time for 2 batch 2.9023048877716064 || training loss 0.059237935580313206 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 16.719212293624878s\n","epoch 3:(202/293) batch || training time for 2 batch 23.087560415267944 || training loss 0.05715227406471968 ||\n","epoch 3:(204/293) batch || training time for 2 batch 2.8792450428009033 || training loss 0.008846834301948547 ||\n","epoch 3:(206/293) batch || training time for 2 batch 2.9482460021972656 || training loss 0.10689385235309601 ||\n","epoch 3:(208/293) batch || training time for 2 batch 2.923604726791382 || training loss 0.10481894295662642 ||\n","epoch 3:(210/293) batch || training time for 2 batch 2.904151678085327 || training loss 0.03172771353274584 ||\n","epoch 3:(212/293) batch || training time for 2 batch 2.9207379817962646 || training loss 0.008133979979902506 ||\n","epoch 3:(214/293) batch || training time for 2 batch 2.9238765239715576 || training loss 0.00870991824194789 ||\n","epoch 3:(216/293) batch || training time for 2 batch 2.9700753688812256 || training loss 0.05588021408766508 ||\n","epoch 3:(218/293) batch || training time for 2 batch 3.001584768295288 || training loss 0.026556525379419327 ||\n","epoch 3:(220/293) batch || training time for 2 batch 2.9890048503875732 || training loss 0.009573464281857014 ||\n","epoch 3:(222/293) batch || training time for 2 batch 3.0209622383117676 || training loss 0.08147375099360943 ||\n","epoch 3:(224/293) batch || training time for 2 batch 3.028592109680176 || training loss 0.008975563570857048 ||\n","epoch 3:(226/293) batch || training time for 2 batch 3.0184338092803955 || training loss 0.008364774286746979 ||\n","epoch 3:(228/293) batch || training time for 2 batch 3.0202078819274902 || training loss 0.008064104709774256 ||\n","epoch 3:(230/293) batch || training time for 2 batch 3.0130794048309326 || training loss 0.008347305934876204 ||\n","epoch 3:(232/293) batch || training time for 2 batch 2.9967427253723145 || training loss 0.05946427630260587 ||\n","epoch 3:(234/293) batch || training time for 2 batch 2.9540657997131348 || training loss 0.007886089850217104 ||\n","epoch 3:(236/293) batch || training time for 2 batch 2.9710848331451416 || training loss 0.057971508940681815 ||\n","epoch 3:(238/293) batch || training time for 2 batch 2.9804885387420654 || training loss 0.10775765776634216 ||\n","epoch 3:(240/293) batch || training time for 2 batch 2.960392475128174 || training loss 0.0992981567978859 ||\n","epoch 3:(242/293) batch || training time for 2 batch 2.914374589920044 || training loss 0.00924583850428462 ||\n","epoch 3:(244/293) batch || training time for 2 batch 2.922785520553589 || training loss 0.0569606889039278 ||\n","epoch 3:(246/293) batch || training time for 2 batch 2.8688933849334717 || training loss 0.008588339667767286 ||\n","epoch 3:(248/293) batch || training time for 2 batch 2.939715623855591 || training loss 0.056798617355525494 ||\n","epoch 3:(250/293) batch || training time for 2 batch 2.899296998977661 || training loss 0.007717464817687869 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 16.764297246932983s\n","epoch 3:(252/293) batch || training time for 2 batch 23.13348126411438 || training loss 0.008041780674830079 ||\n","epoch 3:(254/293) batch || training time for 2 batch 2.8650381565093994 || training loss 0.008382233791053295 ||\n","epoch 3:(256/293) batch || training time for 2 batch 2.8947384357452393 || training loss 0.0076202889904379845 ||\n","epoch 3:(258/293) batch || training time for 2 batch 2.899507999420166 || training loss 0.031233859714120626 ||\n","epoch 3:(260/293) batch || training time for 2 batch 2.9084038734436035 || training loss 0.05934045882895589 ||\n","epoch 3:(262/293) batch || training time for 2 batch 2.9353184700012207 || training loss 0.0074305187445133924 ||\n","epoch 3:(264/293) batch || training time for 2 batch 2.973254680633545 || training loss 0.0823024227283895 ||\n","epoch 3:(266/293) batch || training time for 2 batch 3.0440125465393066 || training loss 0.08598467707633972 ||\n","epoch 3:(268/293) batch || training time for 2 batch 3.0001308917999268 || training loss 0.05498241703025997 ||\n","epoch 3:(270/293) batch || training time for 2 batch 3.044938325881958 || training loss 0.06180862104520202 ||\n","epoch 3:(272/293) batch || training time for 2 batch 3.0149190425872803 || training loss 0.006783028366044164 ||\n","epoch 3:(274/293) batch || training time for 2 batch 3.016334056854248 || training loss 0.007579389493912458 ||\n","epoch 3:(276/293) batch || training time for 2 batch 3.041858434677124 || training loss 0.031130968825891614 ||\n","epoch 3:(278/293) batch || training time for 2 batch 3.0592234134674072 || training loss 0.1064831092953682 ||\n","epoch 3:(280/293) batch || training time for 2 batch 3.049631357192993 || training loss 0.05689588934183121 ||\n","epoch 3:(282/293) batch || training time for 2 batch 3.0175163745880127 || training loss 0.01018398068845272 ||\n","epoch 3:(284/293) batch || training time for 2 batch 2.970454692840576 || training loss 0.007019835291430354 ||\n","epoch 3:(286/293) batch || training time for 2 batch 2.9755635261535645 || training loss 0.05694834142923355 ||\n","epoch 3:(288/293) batch || training time for 2 batch 2.9792356491088867 || training loss 0.05915463273413479 ||\n","epoch 3:(290/293) batch || training time for 2 batch 2.956824779510498 || training loss 0.058410140220075846 ||\n","epoch 3:(292/293) batch || training time for 2 batch 2.9651057720184326 || training loss 0.16422203043475747 ||\n","epoch 4:(2/293) batch || training time for 2 batch 4.354303359985352 || training loss 0.010139663005247712 ||\n","epoch 4:(4/293) batch || training time for 2 batch 2.894190788269043 || training loss 0.006916488520801067 ||\n","epoch 4:(6/293) batch || training time for 2 batch 2.904463529586792 || training loss 0.006570158991962671 ||\n","epoch 4:(8/293) batch || training time for 2 batch 2.8985373973846436 || training loss 0.006703921593725681 ||\n","epoch 4:(10/293) batch || training time for 2 batch 2.9084722995758057 || training loss 0.006407297216355801 ||\n","epoch 4:(12/293) batch || training time for 2 batch 2.9479267597198486 || training loss 0.10694745182991028 ||\n","epoch 4:(14/293) batch || training time for 2 batch 2.9081239700317383 || training loss 0.0073403846472501755 ||\n","epoch 4:(16/293) batch || training time for 2 batch 2.9026033878326416 || training loss 0.0065245055593550205 ||\n","epoch 4:(18/293) batch || training time for 2 batch 2.911823034286499 || training loss 0.007259492762386799 ||\n","epoch 4:(20/293) batch || training time for 2 batch 2.916295289993286 || training loss 0.007451903074979782 ||\n","epoch 4:(22/293) batch || training time for 2 batch 2.9373161792755127 || training loss 0.006870498415082693 ||\n","epoch 4:(24/293) batch || training time for 2 batch 2.899543523788452 || training loss 0.007061769952997565 ||\n","epoch 4:(26/293) batch || training time for 2 batch 2.977982521057129 || training loss 0.005994203966110945 ||\n","epoch 4:(28/293) batch || training time for 2 batch 2.9522759914398193 || training loss 0.02170293708331883 ||\n","epoch 4:(30/293) batch || training time for 2 batch 2.982806444168091 || training loss 0.01889916486106813 ||\n","epoch 4:(32/293) batch || training time for 2 batch 2.99253249168396 || training loss 0.05606798781082034 ||\n","epoch 4:(34/293) batch || training time for 2 batch 2.987307071685791 || training loss 0.006994578056037426 ||\n","epoch 4:(36/293) batch || training time for 2 batch 2.9744691848754883 || training loss 0.03184972004964948 ||\n","epoch 4:(38/293) batch || training time for 2 batch 2.9699652194976807 || training loss 0.030823338543996215 ||\n","epoch 4:(40/293) batch || training time for 2 batch 2.945289373397827 || training loss 0.006686714012175798 ||\n","epoch 4:(42/293) batch || training time for 2 batch 2.9387593269348145 || training loss 0.006632134784013033 ||\n","epoch 4:(44/293) batch || training time for 2 batch 2.9941275119781494 || training loss 0.05537744425237179 ||\n","epoch 4:(46/293) batch || training time for 2 batch 2.932809591293335 || training loss 0.007252390030771494 ||\n","epoch 4:(48/293) batch || training time for 2 batch 2.9465091228485107 || training loss 0.0057390471920371056 ||\n","epoch 4:(50/293) batch || training time for 2 batch 2.9485366344451904 || training loss 0.05695102829486132 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 16.77834439277649s\n","epoch 4:(52/293) batch || training time for 2 batch 23.130383491516113 || training loss 0.005830578040331602 ||\n","epoch 4:(54/293) batch || training time for 2 batch 2.90043568611145 || training loss 0.005796927725896239 ||\n","epoch 4:(56/293) batch || training time for 2 batch 2.89334774017334 || training loss 0.011477126274257898 ||\n","epoch 4:(58/293) batch || training time for 2 batch 2.9021832942962646 || training loss 0.05384685983881354 ||\n","epoch 4:(60/293) batch || training time for 2 batch 2.9188108444213867 || training loss 0.0061970180831849575 ||\n","epoch 4:(62/293) batch || training time for 2 batch 2.9437875747680664 || training loss 0.005471595795825124 ||\n","epoch 4:(64/293) batch || training time for 2 batch 3.001802444458008 || training loss 0.05548305297270417 ||\n","epoch 4:(66/293) batch || training time for 2 batch 3.0089502334594727 || training loss 0.005258947145193815 ||\n","epoch 4:(68/293) batch || training time for 2 batch 3.0157229900360107 || training loss 0.05699050333350897 ||\n","epoch 4:(70/293) batch || training time for 2 batch 3.0144999027252197 || training loss 0.005817071534693241 ||\n","epoch 4:(72/293) batch || training time for 2 batch 3.012237310409546 || training loss 0.030413156375288963 ||\n","epoch 4:(74/293) batch || training time for 2 batch 3.0215322971343994 || training loss 0.027030469616875052 ||\n","epoch 4:(76/293) batch || training time for 2 batch 3.0141382217407227 || training loss 0.005658414447680116 ||\n","epoch 4:(78/293) batch || training time for 2 batch 3.010690212249756 || training loss 0.0055650651920586824 ||\n","epoch 4:(80/293) batch || training time for 2 batch 2.9911370277404785 || training loss 0.005576683674007654 ||\n","epoch 4:(82/293) batch || training time for 2 batch 2.9789085388183594 || training loss 0.005505261244252324 ||\n","epoch 4:(84/293) batch || training time for 2 batch 2.970815420150757 || training loss 0.057739099487662315 ||\n","epoch 4:(86/293) batch || training time for 2 batch 2.9522554874420166 || training loss 0.0313494591973722 ||\n","epoch 4:(88/293) batch || training time for 2 batch 2.9477121829986572 || training loss 0.005932072177529335 ||\n","epoch 4:(90/293) batch || training time for 2 batch 2.8939709663391113 || training loss 0.00754765491001308 ||\n","epoch 4:(92/293) batch || training time for 2 batch 2.9048240184783936 || training loss 0.0056986757554113865 ||\n","epoch 4:(94/293) batch || training time for 2 batch 2.9122676849365234 || training loss 0.005330193089321256 ||\n","epoch 4:(96/293) batch || training time for 2 batch 2.8991312980651855 || training loss 0.005336913745850325 ||\n","epoch 4:(98/293) batch || training time for 2 batch 2.910048723220825 || training loss 0.005860874895006418 ||\n","epoch 4:(100/293) batch || training time for 2 batch 2.9053688049316406 || training loss 0.005458224797621369 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 16.864392042160034s\n","epoch 4:(102/293) batch || training time for 2 batch 23.19833731651306 || training loss 0.005707764765247703 ||\n","epoch 4:(104/293) batch || training time for 2 batch 2.9161229133605957 || training loss 0.05490763112902641 ||\n","epoch 4:(106/293) batch || training time for 2 batch 2.8666698932647705 || training loss 0.005614585941657424 ||\n","epoch 4:(108/293) batch || training time for 2 batch 2.9136621952056885 || training loss 0.005084598902612925 ||\n","epoch 4:(110/293) batch || training time for 2 batch 2.9207470417022705 || training loss 0.005570817505940795 ||\n","epoch 4:(112/293) batch || training time for 2 batch 2.9607410430908203 || training loss 0.004789427388459444 ||\n","epoch 4:(114/293) batch || training time for 2 batch 2.9269022941589355 || training loss 0.005462142871692777 ||\n","epoch 4:(116/293) batch || training time for 2 batch 2.9497768878936768 || training loss 0.0067500893492251635 ||\n","epoch 4:(118/293) batch || training time for 2 batch 2.9849352836608887 || training loss 0.004827008116990328 ||\n","epoch 4:(120/293) batch || training time for 2 batch 3.0287539958953857 || training loss 0.005309316795319319 ||\n","epoch 4:(122/293) batch || training time for 2 batch 3.0098626613616943 || training loss 0.02967697475105524 ||\n","epoch 4:(124/293) batch || training time for 2 batch 3.049489974975586 || training loss 0.0046500591561198235 ||\n","epoch 4:(126/293) batch || training time for 2 batch 3.0364177227020264 || training loss 0.005293333204463124 ||\n","epoch 4:(128/293) batch || training time for 2 batch 3.070502996444702 || training loss 0.05663734953850508 ||\n","epoch 4:(130/293) batch || training time for 2 batch 3.0545971393585205 || training loss 0.055098708253353834 ||\n","epoch 4:(132/293) batch || training time for 2 batch 2.9978723526000977 || training loss 0.05399454780854285 ||\n","epoch 4:(134/293) batch || training time for 2 batch 2.975431442260742 || training loss 0.004548682365566492 ||\n","epoch 4:(136/293) batch || training time for 2 batch 2.9237427711486816 || training loss 0.005524866748601198 ||\n","epoch 4:(138/293) batch || training time for 2 batch 2.9635822772979736 || training loss 0.054558486910536885 ||\n","epoch 4:(140/293) batch || training time for 2 batch 2.9325642585754395 || training loss 0.05655915243551135 ||\n","epoch 4:(142/293) batch || training time for 2 batch 2.911628484725952 || training loss 0.004509267630055547 ||\n","epoch 4:(144/293) batch || training time for 2 batch 2.9213497638702393 || training loss 0.004387608729302883 ||\n","epoch 4:(146/293) batch || training time for 2 batch 2.892049551010132 || training loss 0.004491394618526101 ||\n","epoch 4:(148/293) batch || training time for 2 batch 2.9092824459075928 || training loss 0.014387506758794188 ||\n","epoch 4:(150/293) batch || training time for 2 batch 2.932180881500244 || training loss 0.05560109531506896 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 17.479870796203613s\n","epoch 4:(152/293) batch || training time for 2 batch 23.769582986831665 || training loss 0.004878142382949591 ||\n","epoch 4:(154/293) batch || training time for 2 batch 2.8776612281799316 || training loss 0.012127464171499014 ||\n","epoch 4:(156/293) batch || training time for 2 batch 2.891030788421631 || training loss 0.004456803668290377 ||\n","epoch 4:(158/293) batch || training time for 2 batch 2.907041072845459 || training loss 0.004644986940547824 ||\n","epoch 4:(160/293) batch || training time for 2 batch 2.9171621799468994 || training loss 0.004753880202770233 ||\n","epoch 4:(162/293) batch || training time for 2 batch 2.9668824672698975 || training loss 0.004426468629390001 ||\n","epoch 4:(164/293) batch || training time for 2 batch 2.962750196456909 || training loss 0.004308672156184912 ||\n","epoch 4:(166/293) batch || training time for 2 batch 2.95202374458313 || training loss 0.004298324463889003 ||\n","epoch 4:(168/293) batch || training time for 2 batch 3.0090551376342773 || training loss 0.05974225630052388 ||\n","epoch 4:(170/293) batch || training time for 2 batch 2.994133949279785 || training loss 0.004045661771669984 ||\n","epoch 4:(172/293) batch || training time for 2 batch 3.0330090522766113 || training loss 0.011610857676714659 ||\n","epoch 4:(174/293) batch || training time for 2 batch 3.0426430702209473 || training loss 0.004251537146046758 ||\n","epoch 4:(176/293) batch || training time for 2 batch 3.0189521312713623 || training loss 0.003942703362554312 ||\n","epoch 4:(178/293) batch || training time for 2 batch 2.9984076023101807 || training loss 0.004598322324454784 ||\n","epoch 4:(180/293) batch || training time for 2 batch 2.9906184673309326 || training loss 0.014039009809494019 ||\n","epoch 4:(182/293) batch || training time for 2 batch 2.9908955097198486 || training loss 0.004051858093589544 ||\n","epoch 4:(184/293) batch || training time for 2 batch 2.9740564823150635 || training loss 0.005924859317019582 ||\n","epoch 4:(186/293) batch || training time for 2 batch 2.9346845149993896 || training loss 0.004875198006629944 ||\n","epoch 4:(188/293) batch || training time for 2 batch 2.930434465408325 || training loss 0.004079269710928202 ||\n","epoch 4:(190/293) batch || training time for 2 batch 2.9134459495544434 || training loss 0.004050206393003464 ||\n","epoch 4:(192/293) batch || training time for 2 batch 2.950916290283203 || training loss 0.05551363411359489 ||\n","epoch 4:(194/293) batch || training time for 2 batch 2.9307708740234375 || training loss 0.05757884483318776 ||\n","epoch 4:(196/293) batch || training time for 2 batch 2.9156105518341064 || training loss 0.003921262454241514 ||\n","epoch 4:(198/293) batch || training time for 2 batch 2.9096009731292725 || training loss 0.05595126422122121 ||\n","1.0737418240000003e-05\n","epoch 4:(200/293) batch || training time for 2 batch 2.902090311050415 || training loss 0.004190749838016927 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 18.441211223602295s\n","epoch 4:(202/293) batch || training time for 2 batch 24.855399131774902 || training loss 0.10571493208408356 ||\n","epoch 4:(204/293) batch || training time for 2 batch 2.8923819065093994 || training loss 0.003852779744192958 ||\n","epoch 4:(206/293) batch || training time for 2 batch 2.8960800170898438 || training loss 0.0035996015649288893 ||\n","epoch 4:(208/293) batch || training time for 2 batch 2.9044032096862793 || training loss 0.0037836208939552307 ||\n","epoch 4:(210/293) batch || training time for 2 batch 2.9165585041046143 || training loss 0.0038825543597340584 ||\n","epoch 4:(212/293) batch || training time for 2 batch 2.951808214187622 || training loss 0.0037424960173666477 ||\n","epoch 4:(214/293) batch || training time for 2 batch 2.9552061557769775 || training loss 0.003727894159965217 ||\n","epoch 4:(216/293) batch || training time for 2 batch 2.9691884517669678 || training loss 0.00498604029417038 ||\n","epoch 4:(218/293) batch || training time for 2 batch 2.987257242202759 || training loss 0.003639109549112618 ||\n","epoch 4:(220/293) batch || training time for 2 batch 3.0345356464385986 || training loss 0.003688998054713011 ||\n","epoch 4:(222/293) batch || training time for 2 batch 2.987652540206909 || training loss 0.0038479601498693228 ||\n","epoch 4:(224/293) batch || training time for 2 batch 2.993398904800415 || training loss 0.003738519619219005 ||\n","epoch 4:(226/293) batch || training time for 2 batch 3.0809314250946045 || training loss 0.057048294227570295 ||\n","epoch 4:(228/293) batch || training time for 2 batch 3.0355639457702637 || training loss 0.0037282274570316076 ||\n","epoch 4:(230/293) batch || training time for 2 batch 3.0058939456939697 || training loss 0.0035617658868432045 ||\n","epoch 4:(232/293) batch || training time for 2 batch 2.999380350112915 || training loss 0.0034995315363630652 ||\n","epoch 4:(234/293) batch || training time for 2 batch 2.996034622192383 || training loss 0.0033957951236516237 ||\n","epoch 4:(236/293) batch || training time for 2 batch 2.9323487281799316 || training loss 0.0036794220795854926 ||\n","epoch 4:(238/293) batch || training time for 2 batch 2.9305946826934814 || training loss 0.003470873925834894 ||\n","epoch 4:(240/293) batch || training time for 2 batch 2.911098003387451 || training loss 0.009498794563114643 ||\n","epoch 4:(242/293) batch || training time for 2 batch 2.8872742652893066 || training loss 0.004044726258143783 ||\n","epoch 4:(244/293) batch || training time for 2 batch 2.922553539276123 || training loss 0.003824842511676252 ||\n","epoch 4:(246/293) batch || training time for 2 batch 2.9238147735595703 || training loss 0.0038799152243882418 ||\n","epoch 4:(248/293) batch || training time for 2 batch 2.9277775287628174 || training loss 0.05454014893621206 ||\n","epoch 4:(250/293) batch || training time for 2 batch 2.90818452835083 || training loss 0.0034520894987508655 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 25.1225323677063s\n","epoch 4:(252/293) batch || training time for 2 batch 31.424171209335327 || training loss 0.0033129779621958733 ||\n","epoch 4:(254/293) batch || training time for 2 batch 2.8711464405059814 || training loss 0.004700005287304521 ||\n","epoch 4:(256/293) batch || training time for 2 batch 2.8618814945220947 || training loss 0.006117359269410372 ||\n","epoch 4:(258/293) batch || training time for 2 batch 2.8997764587402344 || training loss 0.00328317703679204 ||\n","epoch 4:(260/293) batch || training time for 2 batch 2.90278959274292 || training loss 0.003414090140722692 ||\n","epoch 4:(262/293) batch || training time for 2 batch 2.9518470764160156 || training loss 0.05868380726315081 ||\n","epoch 4:(264/293) batch || training time for 2 batch 2.9470605850219727 || training loss 0.00331935565918684 ||\n","epoch 4:(266/293) batch || training time for 2 batch 2.985414981842041 || training loss 0.008354832185432315 ||\n","epoch 4:(268/293) batch || training time for 2 batch 2.9830784797668457 || training loss 0.003273837734013796 ||\n","epoch 4:(270/293) batch || training time for 2 batch 2.9981861114501953 || training loss 0.003415771177969873 ||\n","epoch 4:(272/293) batch || training time for 2 batch 3.005943775177002 || training loss 0.003054617904126644 ||\n","epoch 4:(274/293) batch || training time for 2 batch 3.029452323913574 || training loss 0.0032176246168091893 ||\n","epoch 4:(276/293) batch || training time for 2 batch 3.0587542057037354 || training loss 0.030722974450327456 ||\n","epoch 4:(278/293) batch || training time for 2 batch 3.0579569339752197 || training loss 0.0032356728333979845 ||\n","epoch 4:(280/293) batch || training time for 2 batch 3.0278923511505127 || training loss 0.0031194102484732866 ||\n","epoch 4:(282/293) batch || training time for 2 batch 2.9826598167419434 || training loss 0.003357703913934529 ||\n","epoch 4:(284/293) batch || training time for 2 batch 2.9995923042297363 || training loss 0.05870620359200984 ||\n","epoch 4:(286/293) batch || training time for 2 batch 2.943894624710083 || training loss 0.003201177576556802 ||\n","epoch 4:(288/293) batch || training time for 2 batch 2.9587395191192627 || training loss 0.0029230834916234016 ||\n","epoch 4:(290/293) batch || training time for 2 batch 2.91217303276062 || training loss 0.003100907546468079 ||\n","epoch 4:(292/293) batch || training time for 2 batch 2.926568031311035 || training loss 0.003719374304637313 ||\n","epoch 5:(2/293) batch || training time for 2 batch 4.351781606674194 || training loss 0.005152473575435579 ||\n","epoch 5:(4/293) batch || training time for 2 batch 2.9107980728149414 || training loss 0.0031093209981918335 ||\n","epoch 5:(6/293) batch || training time for 2 batch 2.9168713092803955 || training loss 0.003079458954744041 ||\n","epoch 5:(8/293) batch || training time for 2 batch 2.8808891773223877 || training loss 0.0035674802493304014 ||\n","epoch 5:(10/293) batch || training time for 2 batch 2.906585931777954 || training loss 0.0030608377419412136 ||\n","epoch 5:(12/293) batch || training time for 2 batch 2.94469952583313 || training loss 0.057549623190425336 ||\n","epoch 5:(14/293) batch || training time for 2 batch 2.9353983402252197 || training loss 0.003992342157289386 ||\n","epoch 5:(16/293) batch || training time for 2 batch 2.8906633853912354 || training loss 0.0030556187266483903 ||\n","epoch 5:(18/293) batch || training time for 2 batch 2.9451828002929688 || training loss 0.004751348518766463 ||\n","epoch 5:(20/293) batch || training time for 2 batch 2.9270708560943604 || training loss 0.04508192581124604 ||\n","epoch 5:(22/293) batch || training time for 2 batch 2.9361274242401123 || training loss 0.002963162027299404 ||\n","epoch 5:(24/293) batch || training time for 2 batch 2.9402236938476562 || training loss 0.0028043072670698166 ||\n","epoch 5:(26/293) batch || training time for 2 batch 2.947047233581543 || training loss 0.002969741355627775 ||\n","epoch 5:(28/293) batch || training time for 2 batch 3.0012879371643066 || training loss 0.003410221543163061 ||\n","epoch 5:(30/293) batch || training time for 2 batch 2.9468507766723633 || training loss 0.0029944514390081167 ||\n","epoch 5:(32/293) batch || training time for 2 batch 2.938915491104126 || training loss 0.00305569008924067 ||\n","epoch 5:(34/293) batch || training time for 2 batch 3.0058436393737793 || training loss 0.05338191124610603 ||\n","epoch 5:(36/293) batch || training time for 2 batch 3.0031423568725586 || training loss 0.003038784023374319 ||\n","epoch 5:(38/293) batch || training time for 2 batch 2.989272356033325 || training loss 0.007431349135003984 ||\n","epoch 5:(40/293) batch || training time for 2 batch 2.986936330795288 || training loss 0.0028176240157335997 ||\n","epoch 5:(42/293) batch || training time for 2 batch 2.94643497467041 || training loss 0.003072317922487855 ||\n","epoch 5:(44/293) batch || training time for 2 batch 2.920745372772217 || training loss 0.002873509540222585 ||\n","epoch 5:(46/293) batch || training time for 2 batch 2.9297919273376465 || training loss 0.002765694633126259 ||\n","epoch 5:(48/293) batch || training time for 2 batch 2.9714913368225098 || training loss 0.0027098162099719048 ||\n","epoch 5:(50/293) batch || training time for 2 batch 2.93791127204895 || training loss 0.002561145694926381 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 41.354567527770996s\n","epoch 5:(52/293) batch || training time for 2 batch 47.717525243759155 || training loss 0.0030347254360094666 ||\n","epoch 5:(54/293) batch || training time for 2 batch 2.866231918334961 || training loss 0.002824741182848811 ||\n","epoch 5:(56/293) batch || training time for 2 batch 2.9062869548797607 || training loss 0.003013055771589279 ||\n","epoch 5:(58/293) batch || training time for 2 batch 2.9065346717834473 || training loss 0.002667932538315654 ||\n","epoch 5:(60/293) batch || training time for 2 batch 2.942784309387207 || training loss 0.0025683656567707658 ||\n","epoch 5:(62/293) batch || training time for 2 batch 2.994295358657837 || training loss 0.057490186765789986 ||\n","epoch 5:(64/293) batch || training time for 2 batch 2.9547834396362305 || training loss 0.0026757960440590978 ||\n","epoch 5:(66/293) batch || training time for 2 batch 2.9655423164367676 || training loss 0.0027516314294189215 ||\n","epoch 5:(68/293) batch || training time for 2 batch 3.009739398956299 || training loss 0.05583584890700877 ||\n","epoch 5:(70/293) batch || training time for 2 batch 2.997683525085449 || training loss 0.002608781447634101 ||\n","epoch 5:(72/293) batch || training time for 2 batch 3.025339126586914 || training loss 0.0025622111279517412 ||\n","epoch 5:(74/293) batch || training time for 2 batch 3.031360626220703 || training loss 0.002516350941732526 ||\n","epoch 5:(76/293) batch || training time for 2 batch 3.0618021488189697 || training loss 0.05965454701799899 ||\n","epoch 5:(78/293) batch || training time for 2 batch 2.978945016860962 || training loss 0.002864354057237506 ||\n","epoch 5:(80/293) batch || training time for 2 batch 3.000433921813965 || training loss 0.004237563582137227 ||\n","epoch 5:(82/293) batch || training time for 2 batch 2.9811904430389404 || training loss 0.0025847218930721283 ||\n","epoch 5:(84/293) batch || training time for 2 batch 2.9656684398651123 || training loss 0.0034360303543508053 ||\n","epoch 5:(86/293) batch || training time for 2 batch 2.9374310970306396 || training loss 0.0025820638984441757 ||\n","epoch 5:(88/293) batch || training time for 2 batch 2.939995527267456 || training loss 0.0026610917411744595 ||\n","epoch 5:(90/293) batch || training time for 2 batch 2.9262351989746094 || training loss 0.0024682919029146433 ||\n","epoch 5:(92/293) batch || training time for 2 batch 2.9134771823883057 || training loss 0.0029172132490202785 ||\n","epoch 5:(94/293) batch || training time for 2 batch 2.9134273529052734 || training loss 0.0031851677922531962 ||\n","epoch 5:(96/293) batch || training time for 2 batch 2.901151657104492 || training loss 0.002391761285252869 ||\n","epoch 5:(98/293) batch || training time for 2 batch 2.9081788063049316 || training loss 0.0029223827878013253 ||\n","epoch 5:(100/293) batch || training time for 2 batch 2.8916385173797607 || training loss 0.0024475366808474064 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 50.3570613861084s\n","epoch 5:(102/293) batch || training time for 2 batch 56.69474816322327 || training loss 0.002387032494880259 ||\n","epoch 5:(104/293) batch || training time for 2 batch 2.875338554382324 || training loss 0.002453351393342018 ||\n","epoch 5:(106/293) batch || training time for 2 batch 2.914077043533325 || training loss 0.029279500478878617 ||\n","epoch 5:(108/293) batch || training time for 2 batch 2.893432855606079 || training loss 0.0024027888430282474 ||\n","epoch 5:(110/293) batch || training time for 2 batch 2.958232879638672 || training loss 0.002408703905530274 ||\n","epoch 5:(112/293) batch || training time for 2 batch 2.946383237838745 || training loss 0.00296409847214818 ||\n","epoch 5:(114/293) batch || training time for 2 batch 2.9645280838012695 || training loss 0.0033400250831618905 ||\n","epoch 5:(116/293) batch || training time for 2 batch 2.9898273944854736 || training loss 0.006522004259750247 ||\n","epoch 5:(118/293) batch || training time for 2 batch 3.005143404006958 || training loss 0.0024670457933098078 ||\n","epoch 5:(120/293) batch || training time for 2 batch 3.0191400051116943 || training loss 0.002265104209072888 ||\n","epoch 5:(122/293) batch || training time for 2 batch 3.015439510345459 || training loss 0.006494959583505988 ||\n","epoch 5:(124/293) batch || training time for 2 batch 3.02862548828125 || training loss 0.0022651845356449485 ||\n","epoch 5:(126/293) batch || training time for 2 batch 3.0385313034057617 || training loss 0.002340236445888877 ||\n","epoch 5:(128/293) batch || training time for 2 batch 3.0147292613983154 || training loss 0.0024725779658183455 ||\n","epoch 5:(130/293) batch || training time for 2 batch 3.0111849308013916 || training loss 0.05627174349501729 ||\n","epoch 5:(132/293) batch || training time for 2 batch 2.976837635040283 || training loss 0.0022155146580189466 ||\n","epoch 5:(134/293) batch || training time for 2 batch 2.946850061416626 || training loss 0.009688220452517271 ||\n","epoch 5:(136/293) batch || training time for 2 batch 2.932745933532715 || training loss 0.0033551660599187016 ||\n","epoch 5:(138/293) batch || training time for 2 batch 2.9581124782562256 || training loss 0.0024966998025774956 ||\n","epoch 5:(140/293) batch || training time for 2 batch 2.9078266620635986 || training loss 0.002251016441732645 ||\n","epoch 5:(142/293) batch || training time for 2 batch 2.896205186843872 || training loss 0.002245091483928263 ||\n","epoch 5:(144/293) batch || training time for 2 batch 2.904388189315796 || training loss 0.002138103707693517 ||\n","epoch 5:(146/293) batch || training time for 2 batch 2.9005000591278076 || training loss 0.002261177753098309 ||\n","epoch 5:(148/293) batch || training time for 2 batch 2.8944332599639893 || training loss 0.0021115040872246027 ||\n","epoch 5:(150/293) batch || training time for 2 batch 2.875187873840332 || training loss 0.0020960390102118254 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 57.98099493980408s\n","epoch 5:(152/293) batch || training time for 2 batch 64.396901845932 || training loss 0.0021205905359238386 ||\n","epoch 5:(154/293) batch || training time for 2 batch 2.8808865547180176 || training loss 0.002473892644047737 ||\n","epoch 5:(156/293) batch || training time for 2 batch 2.889859437942505 || training loss 0.0021768506849184632 ||\n","epoch 5:(158/293) batch || training time for 2 batch 2.9144039154052734 || training loss 0.002040745923295617 ||\n","epoch 5:(160/293) batch || training time for 2 batch 2.923905611038208 || training loss 0.0023376457393169403 ||\n","epoch 5:(162/293) batch || training time for 2 batch 2.9160995483398438 || training loss 0.004956862307153642 ||\n","epoch 5:(164/293) batch || training time for 2 batch 2.9434192180633545 || training loss 0.0021418104879558086 ||\n","epoch 5:(166/293) batch || training time for 2 batch 2.964775562286377 || training loss 0.0021465938771143556 ||\n","epoch 5:(168/293) batch || training time for 2 batch 2.974440336227417 || training loss 0.002119253622367978 ||\n","epoch 5:(170/293) batch || training time for 2 batch 3.0085437297821045 || training loss 0.002282256376929581 ||\n","epoch 5:(172/293) batch || training time for 2 batch 2.990013599395752 || training loss 0.002160506905056536 ||\n","epoch 5:(174/293) batch || training time for 2 batch 3.010599374771118 || training loss 0.002078158431686461 ||\n","epoch 5:(176/293) batch || training time for 2 batch 3.001673460006714 || training loss 0.002106782514601946 ||\n","epoch 5:(178/293) batch || training time for 2 batch 3.029822587966919 || training loss 0.001966490934137255 ||\n","epoch 5:(180/293) batch || training time for 2 batch 2.9922308921813965 || training loss 0.0018472314113751054 ||\n","epoch 5:(182/293) batch || training time for 2 batch 2.953927993774414 || training loss 0.0018933751853182912 ||\n","epoch 5:(184/293) batch || training time for 2 batch 2.9357802867889404 || training loss 0.0018516367999836802 ||\n","epoch 5:(186/293) batch || training time for 2 batch 2.960808277130127 || training loss 0.0020038000657223165 ||\n","epoch 5:(188/293) batch || training time for 2 batch 2.9641573429107666 || training loss 0.0019919968908652663 ||\n","epoch 5:(190/293) batch || training time for 2 batch 2.908254384994507 || training loss 0.002193535561673343 ||\n","epoch 5:(192/293) batch || training time for 2 batch 2.9154279232025146 || training loss 0.001976174651645124 ||\n","epoch 5:(194/293) batch || training time for 2 batch 2.8865506649017334 || training loss 0.0019251556368544698 ||\n","epoch 5:(196/293) batch || training time for 2 batch 2.9409232139587402 || training loss 0.0025207403814420104 ||\n","epoch 5:(198/293) batch || training time for 2 batch 2.903979539871216 || training loss 0.0020898603834211826 ||\n","8.589934592000004e-06\n","epoch 5:(200/293) batch || training time for 2 batch 2.9196510314941406 || training loss 0.001832789508625865 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 62.880249977111816s\n","epoch 5:(202/293) batch || training time for 2 batch 69.20429921150208 || training loss 0.002058301819488406 ||\n","epoch 5:(204/293) batch || training time for 2 batch 2.861649990081787 || training loss 0.0020701547618955374 ||\n","epoch 5:(206/293) batch || training time for 2 batch 2.8913214206695557 || training loss 0.0018109536031261086 ||\n","epoch 5:(208/293) batch || training time for 2 batch 2.9196388721466064 || training loss 0.0017555769300088286 ||\n","epoch 5:(210/293) batch || training time for 2 batch 2.919884443283081 || training loss 0.0017251463723368943 ||\n","epoch 5:(212/293) batch || training time for 2 batch 2.954972267150879 || training loss 0.028268161870073527 ||\n","epoch 5:(214/293) batch || training time for 2 batch 2.966822385787964 || training loss 0.0020970181794837117 ||\n","epoch 5:(216/293) batch || training time for 2 batch 2.978286027908325 || training loss 0.0018578385934233665 ||\n","epoch 5:(218/293) batch || training time for 2 batch 2.972334146499634 || training loss 0.0017577439430169761 ||\n","epoch 5:(220/293) batch || training time for 2 batch 3.007702350616455 || training loss 0.0017606555484235287 ||\n","epoch 5:(222/293) batch || training time for 2 batch 3.0166382789611816 || training loss 0.0017413528985343874 ||\n","epoch 5:(224/293) batch || training time for 2 batch 3.02620792388916 || training loss 0.0018031747313216329 ||\n","epoch 5:(226/293) batch || training time for 2 batch 3.0353755950927734 || training loss 0.0018547306535765529 ||\n","epoch 5:(228/293) batch || training time for 2 batch 3.01472544670105 || training loss 0.0017164627788588405 ||\n","epoch 5:(230/293) batch || training time for 2 batch 2.9950621128082275 || training loss 0.0017513102502562106 ||\n","epoch 5:(232/293) batch || training time for 2 batch 2.9729270935058594 || training loss 0.024859920609742403 ||\n","epoch 5:(234/293) batch || training time for 2 batch 2.9389431476593018 || training loss 0.0018497499986551702 ||\n","epoch 5:(236/293) batch || training time for 2 batch 2.938739776611328 || training loss 0.0018286170670762658 ||\n","epoch 5:(238/293) batch || training time for 2 batch 2.964553117752075 || training loss 0.06250694236950949 ||\n","epoch 5:(240/293) batch || training time for 2 batch 2.9431111812591553 || training loss 0.001968619879335165 ||\n","epoch 5:(242/293) batch || training time for 2 batch 2.919252634048462 || training loss 0.0017267678631469607 ||\n","epoch 5:(244/293) batch || training time for 2 batch 2.9104840755462646 || training loss 0.0019202360417693853 ||\n","epoch 5:(246/293) batch || training time for 2 batch 2.9242684841156006 || training loss 0.0017514837090857327 ||\n","epoch 5:(248/293) batch || training time for 2 batch 2.924523115158081 || training loss 0.054200554266572 ||\n","epoch 5:(250/293) batch || training time for 2 batch 2.89704966545105 || training loss 0.0017804157687351108 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 63.0513334274292s\n","epoch 5:(252/293) batch || training time for 2 batch 69.38508605957031 || training loss 0.0016819610609672964 ||\n","epoch 5:(254/293) batch || training time for 2 batch 2.895116090774536 || training loss 0.00163445615908131 ||\n","epoch 5:(256/293) batch || training time for 2 batch 2.9188320636749268 || training loss 0.002988411346450448 ||\n","epoch 5:(258/293) batch || training time for 2 batch 2.8931355476379395 || training loss 0.001572278211824596 ||\n","epoch 5:(260/293) batch || training time for 2 batch 2.913489580154419 || training loss 0.0016864098142832518 ||\n","epoch 5:(262/293) batch || training time for 2 batch 2.955962896347046 || training loss 0.0015896862023510039 ||\n","epoch 5:(264/293) batch || training time for 2 batch 2.9702341556549072 || training loss 0.0016303230659104884 ||\n","epoch 5:(266/293) batch || training time for 2 batch 3.0205161571502686 || training loss 0.05412750644609332 ||\n","epoch 5:(268/293) batch || training time for 2 batch 2.9963510036468506 || training loss 0.0016015359433367848 ||\n","epoch 5:(270/293) batch || training time for 2 batch 3.0116734504699707 || training loss 0.0016146334819495678 ||\n","epoch 5:(272/293) batch || training time for 2 batch 3.0267951488494873 || training loss 0.0016907040844671428 ||\n","epoch 5:(274/293) batch || training time for 2 batch 3.0316433906555176 || training loss 0.0015738473739475012 ||\n","epoch 5:(276/293) batch || training time for 2 batch 3.0332846641540527 || training loss 0.0016714978264644742 ||\n","epoch 5:(278/293) batch || training time for 2 batch 3.0159435272216797 || training loss 0.0017661512829363346 ||\n","epoch 5:(280/293) batch || training time for 2 batch 3.0020759105682373 || training loss 0.0052605848759412766 ||\n","epoch 5:(282/293) batch || training time for 2 batch 2.955536127090454 || training loss 0.0016306559555232525 ||\n","epoch 5:(284/293) batch || training time for 2 batch 2.9598541259765625 || training loss 0.0016491771093569696 ||\n","epoch 5:(286/293) batch || training time for 2 batch 2.9356842041015625 || training loss 0.001677417487371713 ||\n","epoch 5:(288/293) batch || training time for 2 batch 2.9699811935424805 || training loss 0.002327192749362439 ||\n","epoch 5:(290/293) batch || training time for 2 batch 2.8940374851226807 || training loss 0.01190260169096291 ||\n","epoch 5:(292/293) batch || training time for 2 batch 2.9129765033721924 || training loss 0.0015270962030626833 ||\n","epoch 6:(2/293) batch || training time for 2 batch 4.401656866073608 || training loss 0.002838951244484633 ||\n","epoch 6:(4/293) batch || training time for 2 batch 2.9362292289733887 || training loss 0.001884802128188312 ||\n","epoch 6:(6/293) batch || training time for 2 batch 2.9162709712982178 || training loss 0.0017611334333196282 ||\n","epoch 6:(8/293) batch || training time for 2 batch 2.9028396606445312 || training loss 0.0015437320689670742 ||\n","epoch 6:(10/293) batch || training time for 2 batch 2.913163185119629 || training loss 0.001457869540899992 ||\n","epoch 6:(12/293) batch || training time for 2 batch 2.9346072673797607 || training loss 0.001702459529042244 ||\n","epoch 6:(14/293) batch || training time for 2 batch 2.9025940895080566 || training loss 0.0015197436441667378 ||\n","epoch 6:(16/293) batch || training time for 2 batch 2.929762125015259 || training loss 0.030471481615677476 ||\n","epoch 6:(18/293) batch || training time for 2 batch 2.927504301071167 || training loss 0.0018469515489414334 ||\n","epoch 6:(20/293) batch || training time for 2 batch 2.9405624866485596 || training loss 0.002661272301338613 ||\n","epoch 6:(22/293) batch || training time for 2 batch 2.953092098236084 || training loss 0.0015634412411600351 ||\n","epoch 6:(24/293) batch || training time for 2 batch 2.9596023559570312 || training loss 0.0018845807062461972 ||\n","epoch 6:(26/293) batch || training time for 2 batch 2.970201253890991 || training loss 0.0015421583084389567 ||\n","epoch 6:(28/293) batch || training time for 2 batch 2.9672796726226807 || training loss 0.0015075637493282557 ||\n","epoch 6:(30/293) batch || training time for 2 batch 2.9467201232910156 || training loss 0.0014246609061956406 ||\n","epoch 6:(32/293) batch || training time for 2 batch 2.9524974822998047 || training loss 0.0014626097399741411 ||\n","epoch 6:(34/293) batch || training time for 2 batch 2.9751346111297607 || training loss 0.0014429314178414643 ||\n","epoch 6:(36/293) batch || training time for 2 batch 2.9790420532226562 || training loss 0.0014395035104826093 ||\n","epoch 6:(38/293) batch || training time for 2 batch 2.9856836795806885 || training loss 0.0014135072124190629 ||\n","epoch 6:(40/293) batch || training time for 2 batch 2.969508171081543 || training loss 0.0017659730510786176 ||\n","epoch 6:(42/293) batch || training time for 2 batch 2.956547498703003 || training loss 0.0013990217703394592 ||\n","epoch 6:(44/293) batch || training time for 2 batch 2.967747211456299 || training loss 0.001412097248248756 ||\n","epoch 6:(46/293) batch || training time for 2 batch 2.955644369125366 || training loss 0.0014143794542178512 ||\n","epoch 6:(48/293) batch || training time for 2 batch 2.9737296104431152 || training loss 0.0015666283434256911 ||\n","epoch 6:(50/293) batch || training time for 2 batch 2.907512664794922 || training loss 0.0015932482201606035 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 70.56319189071655s\n","epoch 6:(52/293) batch || training time for 2 batch 76.92305588722229 || training loss 0.0014715299475938082 ||\n","epoch 6:(54/293) batch || training time for 2 batch 2.886500358581543 || training loss 0.0014327923418022692 ||\n","epoch 6:(56/293) batch || training time for 2 batch 2.909275770187378 || training loss 0.0012952025281265378 ||\n","epoch 6:(58/293) batch || training time for 2 batch 2.9348526000976562 || training loss 0.0015143165946938097 ||\n","epoch 6:(60/293) batch || training time for 2 batch 2.9262163639068604 || training loss 0.0014596304390579462 ||\n","epoch 6:(62/293) batch || training time for 2 batch 2.9419667720794678 || training loss 0.001436609833035618 ||\n","epoch 6:(64/293) batch || training time for 2 batch 2.983858346939087 || training loss 0.0014171437360346317 ||\n","epoch 6:(66/293) batch || training time for 2 batch 2.972327709197998 || training loss 0.001425539841875434 ||\n","epoch 6:(68/293) batch || training time for 2 batch 2.9915990829467773 || training loss 0.0015037774574011564 ||\n","epoch 6:(70/293) batch || training time for 2 batch 2.971377372741699 || training loss 0.0016044217627495527 ||\n","epoch 6:(72/293) batch || training time for 2 batch 3.040525197982788 || training loss 0.0013647344312630594 ||\n","epoch 6:(74/293) batch || training time for 2 batch 3.0501229763031006 || training loss 0.0019428307423368096 ||\n","epoch 6:(76/293) batch || training time for 2 batch 3.0288281440734863 || training loss 0.0013677268871106207 ||\n","epoch 6:(78/293) batch || training time for 2 batch 2.9889488220214844 || training loss 0.0013896601740270853 ||\n","epoch 6:(80/293) batch || training time for 2 batch 2.9871299266815186 || training loss 0.0014102497370913625 ||\n","epoch 6:(82/293) batch || training time for 2 batch 2.938242197036743 || training loss 0.0014458872028626502 ||\n","epoch 6:(84/293) batch || training time for 2 batch 2.961575746536255 || training loss 0.004999290220439434 ||\n","epoch 6:(86/293) batch || training time for 2 batch 2.9356136322021484 || training loss 0.001397990738041699 ||\n","epoch 6:(88/293) batch || training time for 2 batch 2.9245967864990234 || training loss 0.001805861946195364 ||\n","epoch 6:(90/293) batch || training time for 2 batch 2.9321646690368652 || training loss 0.0012910080258734524 ||\n","epoch 6:(92/293) batch || training time for 2 batch 2.9201173782348633 || training loss 0.002009419724345207 ||\n","epoch 6:(94/293) batch || training time for 2 batch 2.916242837905884 || training loss 0.00137237214948982 ||\n","epoch 6:(96/293) batch || training time for 2 batch 2.8937628269195557 || training loss 0.0012708803405985236 ||\n","epoch 6:(98/293) batch || training time for 2 batch 2.8775675296783447 || training loss 0.001363254152238369 ||\n","epoch 6:(100/293) batch || training time for 2 batch 2.932830333709717 || training loss 0.0013002226478420198 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 64.93163681030273s\n","epoch 6:(102/293) batch || training time for 2 batch 71.34244513511658 || training loss 0.001273882226087153 ||\n","epoch 6:(104/293) batch || training time for 2 batch 2.906845808029175 || training loss 0.0012470381334424019 ||\n","epoch 6:(106/293) batch || training time for 2 batch 2.9108169078826904 || training loss 0.001329842780251056 ||\n","epoch 6:(108/293) batch || training time for 2 batch 2.918912649154663 || training loss 0.0013543149107135832 ||\n","epoch 6:(110/293) batch || training time for 2 batch 2.9642550945281982 || training loss 0.002430334803648293 ||\n","epoch 6:(112/293) batch || training time for 2 batch 2.958240270614624 || training loss 0.001298538118135184 ||\n","epoch 6:(114/293) batch || training time for 2 batch 2.9723715782165527 || training loss 0.001229205634444952 ||\n","epoch 6:(116/293) batch || training time for 2 batch 2.9927978515625 || training loss 0.0012914510443806648 ||\n","epoch 6:(118/293) batch || training time for 2 batch 3.0044591426849365 || training loss 0.0017180208815261722 ||\n","epoch 6:(120/293) batch || training time for 2 batch 3.0942630767822266 || training loss 0.0025566848926246166 ||\n","epoch 6:(122/293) batch || training time for 2 batch 3.0303773880004883 || training loss 0.001510474772658199 ||\n","epoch 6:(124/293) batch || training time for 2 batch 3.016268491744995 || training loss 0.0013474522274918854 ||\n","epoch 6:(126/293) batch || training time for 2 batch 3.015690326690674 || training loss 0.0013051385758444667 ||\n","epoch 6:(128/293) batch || training time for 2 batch 3.023712158203125 || training loss 0.001476756064221263 ||\n","epoch 6:(130/293) batch || training time for 2 batch 2.9885776042938232 || training loss 0.0012728132423944771 ||\n","epoch 6:(132/293) batch || training time for 2 batch 2.9670519828796387 || training loss 0.0012654876336455345 ||\n","epoch 6:(134/293) batch || training time for 2 batch 2.9388844966888428 || training loss 0.0012714224867522717 ||\n","epoch 6:(136/293) batch || training time for 2 batch 2.934920072555542 || training loss 0.0012552207917906344 ||\n","epoch 6:(138/293) batch || training time for 2 batch 2.949096441268921 || training loss 0.028532498807180673 ||\n","epoch 6:(140/293) batch || training time for 2 batch 2.9367105960845947 || training loss 0.0012385057052597404 ||\n","epoch 6:(142/293) batch || training time for 2 batch 2.9036097526550293 || training loss 0.0014764211373403668 ||\n","epoch 6:(144/293) batch || training time for 2 batch 2.918713331222534 || training loss 0.001314476947300136 ||\n","epoch 6:(146/293) batch || training time for 2 batch 2.889404773712158 || training loss 0.0012930401717312634 ||\n","epoch 6:(148/293) batch || training time for 2 batch 2.9137730598449707 || training loss 0.0011504028807394207 ||\n","epoch 6:(150/293) batch || training time for 2 batch 2.8983678817749023 || training loss 0.0011786386603489518 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 66.27837681770325s\n","epoch 6:(152/293) batch || training time for 2 batch 72.67316341400146 || training loss 0.001143957197200507 ||\n","epoch 6:(154/293) batch || training time for 2 batch 2.8759396076202393 || training loss 0.0011391178704798222 ||\n","epoch 6:(156/293) batch || training time for 2 batch 2.8986902236938477 || training loss 0.0012441364815458655 ||\n","epoch 6:(158/293) batch || training time for 2 batch 2.9308643341064453 || training loss 0.0012351629557088017 ||\n","epoch 6:(160/293) batch || training time for 2 batch 2.9356777667999268 || training loss 0.03152373316697776 ||\n","epoch 6:(162/293) batch || training time for 2 batch 2.921035051345825 || training loss 0.0042272425489500165 ||\n","epoch 6:(164/293) batch || training time for 2 batch 2.9641740322113037 || training loss 0.0011006526765413582 ||\n","epoch 6:(166/293) batch || training time for 2 batch 2.9623303413391113 || training loss 0.009975876193493605 ||\n","epoch 6:(168/293) batch || training time for 2 batch 3.00691294670105 || training loss 0.0011182568268850446 ||\n","epoch 6:(170/293) batch || training time for 2 batch 3.006521463394165 || training loss 0.0011993913212791085 ||\n","epoch 6:(172/293) batch || training time for 2 batch 3.000098943710327 || training loss 0.00636061851400882 ||\n","epoch 6:(174/293) batch || training time for 2 batch 3.0240859985351562 || training loss 0.0010831961408257484 ||\n","epoch 6:(176/293) batch || training time for 2 batch 3.0325801372528076 || training loss 0.0010945997200906277 ||\n","epoch 6:(178/293) batch || training time for 2 batch 3.036681890487671 || training loss 0.0011511070188134909 ||\n","epoch 6:(180/293) batch || training time for 2 batch 3.0012500286102295 || training loss 0.0038197492249310017 ||\n","epoch 6:(182/293) batch || training time for 2 batch 2.93892765045166 || training loss 0.0017546906601637602 ||\n","epoch 6:(184/293) batch || training time for 2 batch 2.9533019065856934 || training loss 0.0010480830678716302 ||\n","epoch 6:(186/293) batch || training time for 2 batch 2.9167656898498535 || training loss 0.0012509035877883434 ||\n","epoch 6:(188/293) batch || training time for 2 batch 2.9399945735931396 || training loss 0.0011134109226986766 ||\n","epoch 6:(190/293) batch || training time for 2 batch 2.90682053565979 || training loss 0.001216074568219483 ||\n","epoch 6:(192/293) batch || training time for 2 batch 2.9364895820617676 || training loss 0.0011187285417690873 ||\n","epoch 6:(194/293) batch || training time for 2 batch 2.911367177963257 || training loss 0.001033788314089179 ||\n","epoch 6:(196/293) batch || training time for 2 batch 2.9135971069335938 || training loss 0.0011157389963045716 ||\n","epoch 6:(198/293) batch || training time for 2 batch 2.91279673576355 || training loss 0.0010455961455591023 ||\n","6.871947673600003e-06\n","epoch 6:(200/293) batch || training time for 2 batch 2.8845183849334717 || training loss 0.0011736887972801924 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 71.28309607505798s\n","epoch 6:(202/293) batch || training time for 2 batch 77.67501378059387 || training loss 0.0010381658794358373 ||\n","epoch 6:(204/293) batch || training time for 2 batch 2.901329755783081 || training loss 0.001016978407278657 ||\n","epoch 6:(206/293) batch || training time for 2 batch 2.899632692337036 || training loss 0.001055522181559354 ||\n","epoch 6:(208/293) batch || training time for 2 batch 2.8968114852905273 || training loss 0.0010594942141324282 ||\n","epoch 6:(210/293) batch || training time for 2 batch 2.943114757537842 || training loss 0.0010777099523693323 ||\n","epoch 6:(212/293) batch || training time for 2 batch 2.9749441146850586 || training loss 0.0012862386065535247 ||\n","epoch 6:(214/293) batch || training time for 2 batch 2.950490951538086 || training loss 0.0011367573169991374 ||\n","epoch 6:(216/293) batch || training time for 2 batch 3.0026698112487793 || training loss 0.0014577609836123884 ||\n","epoch 6:(218/293) batch || training time for 2 batch 2.9924182891845703 || training loss 0.0009555893484503031 ||\n","epoch 6:(220/293) batch || training time for 2 batch 3.0121262073516846 || training loss 0.0011387859121896327 ||\n","epoch 6:(222/293) batch || training time for 2 batch 3.016130208969116 || training loss 0.0010679111583158374 ||\n","epoch 6:(224/293) batch || training time for 2 batch 3.0311686992645264 || training loss 0.0010294768144376576 ||\n","epoch 6:(226/293) batch || training time for 2 batch 3.0240426063537598 || training loss 0.0009317242365796119 ||\n","epoch 6:(228/293) batch || training time for 2 batch 2.9815211296081543 || training loss 0.0011491845943965018 ||\n","epoch 6:(230/293) batch || training time for 2 batch 3.011230945587158 || training loss 0.0016821566387079656 ||\n","epoch 6:(232/293) batch || training time for 2 batch 3.008392572402954 || training loss 0.0011849417351186275 ||\n","epoch 6:(234/293) batch || training time for 2 batch 2.9557344913482666 || training loss 0.013116509187966585 ||\n","epoch 6:(236/293) batch || training time for 2 batch 2.9386534690856934 || training loss 0.005667734774760902 ||\n","epoch 6:(238/293) batch || training time for 2 batch 2.9156363010406494 || training loss 0.0010459409677423537 ||\n","epoch 6:(240/293) batch || training time for 2 batch 2.935260534286499 || training loss 0.0011721545015461743 ||\n","epoch 6:(242/293) batch || training time for 2 batch 2.909219741821289 || training loss 0.0008536014938727021 ||\n","epoch 6:(244/293) batch || training time for 2 batch 2.9077394008636475 || training loss 0.0010281791328452528 ||\n","epoch 6:(246/293) batch || training time for 2 batch 2.9181265830993652 || training loss 0.0010390310198999941 ||\n","epoch 6:(248/293) batch || training time for 2 batch 2.9099392890930176 || training loss 0.0010514578607399017 ||\n","epoch 6:(250/293) batch || training time for 2 batch 2.8956103324890137 || training loss 0.0010140595550183207 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 72.07501196861267s\n","epoch 6:(252/293) batch || training time for 2 batch 78.44842314720154 || training loss 0.001010845066048205 ||\n","epoch 6:(254/293) batch || training time for 2 batch 2.897808790206909 || training loss 0.0010746209882199764 ||\n","epoch 6:(256/293) batch || training time for 2 batch 2.896942138671875 || training loss 0.0009888893109746277 ||\n","epoch 6:(258/293) batch || training time for 2 batch 2.9216816425323486 || training loss 0.0009756319341249764 ||\n","epoch 6:(260/293) batch || training time for 2 batch 2.928751230239868 || training loss 0.0010142610408365726 ||\n","epoch 6:(262/293) batch || training time for 2 batch 2.9483814239501953 || training loss 0.0009262976527679712 ||\n","epoch 6:(264/293) batch || training time for 2 batch 2.9574148654937744 || training loss 0.0009775444632396102 ||\n","epoch 6:(266/293) batch || training time for 2 batch 2.974466323852539 || training loss 0.0009849395719356835 ||\n","epoch 6:(268/293) batch || training time for 2 batch 2.9929323196411133 || training loss 0.0009519502345938236 ||\n","epoch 6:(270/293) batch || training time for 2 batch 3.013139247894287 || training loss 0.000940583850024268 ||\n","epoch 6:(272/293) batch || training time for 2 batch 3.023660659790039 || training loss 0.0009681439260020852 ||\n","epoch 6:(274/293) batch || training time for 2 batch 3.030305862426758 || training loss 0.0009314809576608241 ||\n","epoch 6:(276/293) batch || training time for 2 batch 3.0380053520202637 || training loss 0.0009898674907162786 ||\n","epoch 6:(278/293) batch || training time for 2 batch 3.037750482559204 || training loss 0.00108036317396909 ||\n","epoch 6:(280/293) batch || training time for 2 batch 2.9721086025238037 || training loss 0.0010257912799715996 ||\n","epoch 6:(282/293) batch || training time for 2 batch 2.93501877784729 || training loss 0.01134415651904419 ||\n","epoch 6:(284/293) batch || training time for 2 batch 2.9729042053222656 || training loss 0.0009048120118677616 ||\n","epoch 6:(286/293) batch || training time for 2 batch 2.9479520320892334 || training loss 0.0009501535096205771 ||\n","epoch 6:(288/293) batch || training time for 2 batch 2.939422845840454 || training loss 0.0010963201348204166 ||\n","epoch 6:(290/293) batch || training time for 2 batch 2.922229766845703 || training loss 0.0008761687786318362 ||\n","epoch 6:(292/293) batch || training time for 2 batch 2.913668394088745 || training loss 0.0010312804952263832 ||\n","epoch 7:(2/293) batch || training time for 2 batch 4.3689751625061035 || training loss 0.0014149859780445695 ||\n","epoch 7:(4/293) batch || training time for 2 batch 2.915872573852539 || training loss 0.0009617494361009449 ||\n","epoch 7:(6/293) batch || training time for 2 batch 2.8946030139923096 || training loss 0.0010719462297856808 ||\n","epoch 7:(8/293) batch || training time for 2 batch 2.8910467624664307 || training loss 0.001041580457240343 ||\n","epoch 7:(10/293) batch || training time for 2 batch 2.8965110778808594 || training loss 0.000858204992255196 ||\n","epoch 7:(12/293) batch || training time for 2 batch 2.9074819087982178 || training loss 0.0008625193149782717 ||\n","epoch 7:(14/293) batch || training time for 2 batch 2.9064016342163086 || training loss 0.0015424806042574346 ||\n","epoch 7:(16/293) batch || training time for 2 batch 2.9022889137268066 || training loss 0.000822825328214094 ||\n","epoch 7:(18/293) batch || training time for 2 batch 2.931182861328125 || training loss 0.0009158788889180869 ||\n","epoch 7:(20/293) batch || training time for 2 batch 2.9270474910736084 || training loss 0.0008274313295260072 ||\n","epoch 7:(22/293) batch || training time for 2 batch 2.969313621520996 || training loss 0.0009010181238409132 ||\n","epoch 7:(24/293) batch || training time for 2 batch 2.971942663192749 || training loss 0.0008088291506282985 ||\n","epoch 7:(26/293) batch || training time for 2 batch 2.958611488342285 || training loss 0.0009274704207200557 ||\n","epoch 7:(28/293) batch || training time for 2 batch 2.9208924770355225 || training loss 0.0009836838580667973 ||\n","epoch 7:(30/293) batch || training time for 2 batch 3.005192995071411 || training loss 0.0009179124026559293 ||\n","epoch 7:(32/293) batch || training time for 2 batch 2.9782233238220215 || training loss 0.0008576234977226704 ||\n","epoch 7:(34/293) batch || training time for 2 batch 2.9633121490478516 || training loss 0.0009081388416234404 ||\n","epoch 7:(36/293) batch || training time for 2 batch 2.964621067047119 || training loss 0.0008351067081093788 ||\n","epoch 7:(38/293) batch || training time for 2 batch 2.9761853218078613 || training loss 0.000819032487925142 ||\n","epoch 7:(40/293) batch || training time for 2 batch 2.964920997619629 || training loss 0.0008351746655534953 ||\n","epoch 7:(42/293) batch || training time for 2 batch 2.9636037349700928 || training loss 0.0008424452098552138 ||\n","epoch 7:(44/293) batch || training time for 2 batch 2.949768304824829 || training loss 0.0008633429242763668 ||\n","epoch 7:(46/293) batch || training time for 2 batch 2.9524104595184326 || training loss 0.0008270118851214647 ||\n","epoch 7:(48/293) batch || training time for 2 batch 2.9523298740386963 || training loss 0.004246262949891388 ||\n","epoch 7:(50/293) batch || training time for 2 batch 2.9460387229919434 || training loss 0.0009355275251436979 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 77.23778438568115s\n","epoch 7:(52/293) batch || training time for 2 batch 83.62981176376343 || training loss 0.0008379865030292422 ||\n","epoch 7:(54/293) batch || training time for 2 batch 2.8663434982299805 || training loss 0.0008986438624560833 ||\n","epoch 7:(56/293) batch || training time for 2 batch 2.9157447814941406 || training loss 0.0013735475949943066 ||\n","epoch 7:(58/293) batch || training time for 2 batch 2.9069085121154785 || training loss 0.00481076751020737 ||\n","epoch 7:(60/293) batch || training time for 2 batch 2.9293463230133057 || training loss 0.0008559624257031828 ||\n","epoch 7:(62/293) batch || training time for 2 batch 2.949587821960449 || training loss 0.0008203867182601243 ||\n","epoch 7:(64/293) batch || training time for 2 batch 2.9874162673950195 || training loss 0.0008674290147610009 ||\n","epoch 7:(66/293) batch || training time for 2 batch 2.98250675201416 || training loss 0.0008261943003162742 ||\n","epoch 7:(68/293) batch || training time for 2 batch 2.982539653778076 || training loss 0.0007991680759005249 ||\n","epoch 7:(70/293) batch || training time for 2 batch 3.019479274749756 || training loss 0.0008787107944954187 ||\n","epoch 7:(72/293) batch || training time for 2 batch 3.037942409515381 || training loss 0.0007292753434740007 ||\n","epoch 7:(74/293) batch || training time for 2 batch 3.048400640487671 || training loss 0.0008728860411792994 ||\n","epoch 7:(76/293) batch || training time for 2 batch 3.027113199234009 || training loss 0.003989853343227878 ||\n","epoch 7:(78/293) batch || training time for 2 batch 3.0111470222473145 || training loss 0.0008095992961898446 ||\n","epoch 7:(80/293) batch || training time for 2 batch 3.0014455318450928 || training loss 0.0008084966684691608 ||\n","epoch 7:(82/293) batch || training time for 2 batch 2.992658853530884 || training loss 0.0014451078895945102 ||\n","epoch 7:(84/293) batch || training time for 2 batch 2.940096139907837 || training loss 0.0007552243478130549 ||\n","epoch 7:(86/293) batch || training time for 2 batch 2.934596300125122 || training loss 0.0008100888808257878 ||\n","epoch 7:(88/293) batch || training time for 2 batch 2.931586265563965 || training loss 0.000836379622342065 ||\n","epoch 7:(90/293) batch || training time for 2 batch 2.9161124229431152 || training loss 0.0008411572489421815 ||\n","epoch 7:(92/293) batch || training time for 2 batch 2.9034743309020996 || training loss 0.0007866440864745528 ||\n","epoch 7:(94/293) batch || training time for 2 batch 2.913292646408081 || training loss 0.000794498628238216 ||\n","epoch 7:(96/293) batch || training time for 2 batch 2.9012362957000732 || training loss 0.0008362203661818057 ||\n","epoch 7:(98/293) batch || training time for 2 batch 2.920642614364624 || training loss 0.0007712137303315103 ||\n","epoch 7:(100/293) batch || training time for 2 batch 2.957056760787964 || training loss 0.0008578845008742064 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 66.97229242324829s\n","epoch 7:(102/293) batch || training time for 2 batch 73.34212923049927 || training loss 0.0007786491769365966 ||\n","epoch 7:(104/293) batch || training time for 2 batch 2.8860838413238525 || training loss 0.0007321214652620256 ||\n","epoch 7:(106/293) batch || training time for 2 batch 2.896712303161621 || training loss 0.0008036050130613148 ||\n","epoch 7:(108/293) batch || training time for 2 batch 2.9370718002319336 || training loss 0.0007665296725463122 ||\n","epoch 7:(110/293) batch || training time for 2 batch 2.944091320037842 || training loss 0.0007790303789079189 ||\n","epoch 7:(112/293) batch || training time for 2 batch 2.9811246395111084 || training loss 0.0007189793977886438 ||\n","epoch 7:(114/293) batch || training time for 2 batch 2.986279010772705 || training loss 0.0009042246965691447 ||\n","epoch 7:(116/293) batch || training time for 2 batch 2.9528756141662598 || training loss 0.0016728627670090646 ||\n","epoch 7:(118/293) batch || training time for 2 batch 2.9935226440429688 || training loss 0.000698131654644385 ||\n","epoch 7:(120/293) batch || training time for 2 batch 3.0059075355529785 || training loss 0.0008174649847205728 ||\n","epoch 7:(122/293) batch || training time for 2 batch 3.0136592388153076 || training loss 0.0008633853867650032 ||\n","epoch 7:(124/293) batch || training time for 2 batch 3.014832019805908 || training loss 0.0008244589262176305 ||\n","epoch 7:(126/293) batch || training time for 2 batch 3.034452438354492 || training loss 0.0007704523450229317 ||\n","epoch 7:(128/293) batch || training time for 2 batch 3.0190494060516357 || training loss 0.010237302805762738 ||\n","epoch 7:(130/293) batch || training time for 2 batch 2.997377634048462 || training loss 0.0008232780091930181 ||\n","epoch 7:(132/293) batch || training time for 2 batch 2.975998878479004 || training loss 0.0008341632492374629 ||\n","epoch 7:(134/293) batch || training time for 2 batch 2.9853570461273193 || training loss 0.001486249064328149 ||\n","epoch 7:(136/293) batch || training time for 2 batch 2.9599297046661377 || training loss 0.0008030719473026693 ||\n","epoch 7:(138/293) batch || training time for 2 batch 2.9313390254974365 || training loss 0.0007497227634303272 ||\n","epoch 7:(140/293) batch || training time for 2 batch 2.9230573177337646 || training loss 0.0007796366699039936 ||\n","epoch 7:(142/293) batch || training time for 2 batch 2.905190944671631 || training loss 0.001185294589959085 ||\n","epoch 7:(144/293) batch || training time for 2 batch 2.908555030822754 || training loss 0.0007408710953313857 ||\n","epoch 7:(146/293) batch || training time for 2 batch 2.9190115928649902 || training loss 0.0008167457417584956 ||\n","epoch 7:(148/293) batch || training time for 2 batch 2.930203437805176 || training loss 0.0008912809717003256 ||\n","epoch 7:(150/293) batch || training time for 2 batch 2.900017261505127 || training loss 0.0007569277659058571 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 72.38575506210327s\n","epoch 7:(152/293) batch || training time for 2 batch 78.74623489379883 || training loss 0.0007359398296102881 ||\n","epoch 7:(154/293) batch || training time for 2 batch 2.9244706630706787 || training loss 0.0007144446135498583 ||\n","epoch 7:(156/293) batch || training time for 2 batch 2.9076647758483887 || training loss 0.0007409313984680921 ||\n","epoch 7:(158/293) batch || training time for 2 batch 2.907221794128418 || training loss 0.0007294261886272579 ||\n","epoch 7:(160/293) batch || training time for 2 batch 2.949131965637207 || training loss 0.0006515247223433107 ||\n","epoch 7:(162/293) batch || training time for 2 batch 2.943146228790283 || training loss 0.0007073695014696568 ||\n","epoch 7:(164/293) batch || training time for 2 batch 2.9798052310943604 || training loss 0.0006984400097280741 ||\n","epoch 7:(166/293) batch || training time for 2 batch 2.996690511703491 || training loss 0.000638726050965488 ||\n","epoch 7:(168/293) batch || training time for 2 batch 3.0164952278137207 || training loss 0.0008332148718181998 ||\n","epoch 7:(170/293) batch || training time for 2 batch 3.0428526401519775 || training loss 0.0012804946745745838 ||\n","epoch 7:(172/293) batch || training time for 2 batch 3.0507705211639404 || training loss 0.03340911876875907 ||\n","epoch 7:(174/293) batch || training time for 2 batch 3.0075626373291016 || training loss 0.009460416215006262 ||\n","epoch 7:(176/293) batch || training time for 2 batch 3.0323562622070312 || training loss 0.0006928503862582147 ||\n","epoch 7:(178/293) batch || training time for 2 batch 3.024517059326172 || training loss 0.0028677802765741944 ||\n","epoch 7:(180/293) batch || training time for 2 batch 2.986572265625 || training loss 0.0012426029134076089 ||\n","epoch 7:(182/293) batch || training time for 2 batch 2.932417631149292 || training loss 0.0007276547257788479 ||\n","epoch 7:(184/293) batch || training time for 2 batch 2.9511001110076904 || training loss 0.006198254617629573 ||\n","epoch 7:(186/293) batch || training time for 2 batch 2.9504356384277344 || training loss 0.0017338209436275065 ||\n","epoch 7:(188/293) batch || training time for 2 batch 2.957404613494873 || training loss 0.0008276744629256427 ||\n","epoch 7:(190/293) batch || training time for 2 batch 2.960602283477783 || training loss 0.0007735361868981272 ||\n","epoch 7:(192/293) batch || training time for 2 batch 2.966487407684326 || training loss 0.0007583699771203101 ||\n","epoch 7:(194/293) batch || training time for 2 batch 2.914254903793335 || training loss 0.0007305019826162606 ||\n","epoch 7:(196/293) batch || training time for 2 batch 2.9297823905944824 || training loss 0.0007519047940149903 ||\n","epoch 7:(198/293) batch || training time for 2 batch 2.957509756088257 || training loss 0.0006302472902461886 ||\n","5.497558138880003e-06\n","epoch 7:(200/293) batch || training time for 2 batch 2.912670850753784 || training loss 0.0007410826219711453 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 73.1386821269989s\n","epoch 7:(202/293) batch || training time for 2 batch 79.6335699558258 || training loss 0.000814804108813405 ||\n","epoch 7:(204/293) batch || training time for 2 batch 2.904836654663086 || training loss 0.000726472440874204 ||\n","epoch 7:(206/293) batch || training time for 2 batch 2.895411729812622 || training loss 0.0008885591814760119 ||\n","epoch 7:(208/293) batch || training time for 2 batch 2.913957118988037 || training loss 0.0010213447676505893 ||\n","epoch 7:(210/293) batch || training time for 2 batch 2.9431896209716797 || training loss 0.0006435824907384813 ||\n","epoch 7:(212/293) batch || training time for 2 batch 2.9514262676239014 || training loss 0.0006797912355978042 ||\n","epoch 7:(214/293) batch || training time for 2 batch 2.9678544998168945 || training loss 0.0006777812959626317 ||\n","epoch 7:(216/293) batch || training time for 2 batch 2.9914119243621826 || training loss 0.000842448731418699 ||\n","epoch 7:(218/293) batch || training time for 2 batch 3.0235297679901123 || training loss 0.0007274657546076924 ||\n","epoch 7:(220/293) batch || training time for 2 batch 3.0476109981536865 || training loss 0.0008083886059466749 ||\n","epoch 7:(222/293) batch || training time for 2 batch 3.0459811687469482 || training loss 0.0006380954582709819 ||\n","epoch 7:(224/293) batch || training time for 2 batch 3.033582925796509 || training loss 0.0006983024650253356 ||\n","epoch 7:(226/293) batch || training time for 2 batch 2.99924373626709 || training loss 0.000849286385346204 ||\n","epoch 7:(228/293) batch || training time for 2 batch 3.0028114318847656 || training loss 0.0006363748107105494 ||\n","epoch 7:(230/293) batch || training time for 2 batch 2.9894111156463623 || training loss 0.0008360907668247819 ||\n","epoch 7:(232/293) batch || training time for 2 batch 2.956240177154541 || training loss 0.0007038593175821006 ||\n","epoch 7:(234/293) batch || training time for 2 batch 2.9544694423675537 || training loss 0.0006811889179516584 ||\n","epoch 7:(236/293) batch || training time for 2 batch 2.9327938556671143 || training loss 0.0007015802839305252 ||\n","epoch 7:(238/293) batch || training time for 2 batch 2.93060040473938 || training loss 0.0006653059681411833 ||\n","epoch 7:(240/293) batch || training time for 2 batch 2.915809154510498 || training loss 0.0007274059753399342 ||\n","epoch 7:(242/293) batch || training time for 2 batch 2.9054815769195557 || training loss 0.0007158727676142007 ||\n","epoch 7:(244/293) batch || training time for 2 batch 2.9442222118377686 || training loss 0.0007607614679727703 ||\n","epoch 7:(246/293) batch || training time for 2 batch 2.9166486263275146 || training loss 0.0008014679187908769 ||\n","epoch 7:(248/293) batch || training time for 2 batch 2.9022748470306396 || training loss 0.0006964477943256497 ||\n","epoch 7:(250/293) batch || training time for 2 batch 2.8990731239318848 || training loss 0.0006754354399163276 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 72.9117226600647s\n","epoch 7:(252/293) batch || training time for 2 batch 79.30654764175415 || training loss 0.0009571213158778846 ||\n","epoch 7:(254/293) batch || training time for 2 batch 2.9150638580322266 || training loss 0.0006411762442439795 ||\n","epoch 7:(256/293) batch || training time for 2 batch 2.872904062271118 || training loss 0.0008790796273387969 ||\n","epoch 7:(258/293) batch || training time for 2 batch 2.9090733528137207 || training loss 0.000706108461599797 ||\n","epoch 7:(260/293) batch || training time for 2 batch 2.9156723022460938 || training loss 0.0005977285036351532 ||\n","epoch 7:(262/293) batch || training time for 2 batch 2.961653709411621 || training loss 0.0006200537900440395 ||\n","epoch 7:(264/293) batch || training time for 2 batch 2.989658832550049 || training loss 0.000805117015261203 ||\n","epoch 7:(266/293) batch || training time for 2 batch 2.9865972995758057 || training loss 0.0006069758092053235 ||\n","epoch 7:(268/293) batch || training time for 2 batch 2.990788698196411 || training loss 0.0007463951478712261 ||\n","epoch 7:(270/293) batch || training time for 2 batch 3.008058547973633 || training loss 0.006954854121431708 ||\n","epoch 7:(272/293) batch || training time for 2 batch 3.0060722827911377 || training loss 0.0006301126850303262 ||\n","epoch 7:(274/293) batch || training time for 2 batch 3.0660274028778076 || training loss 0.0007595086353830993 ||\n","epoch 7:(276/293) batch || training time for 2 batch 3.0191595554351807 || training loss 0.0006131825502961874 ||\n","epoch 7:(278/293) batch || training time for 2 batch 3.025564432144165 || training loss 0.0006547807715833187 ||\n","epoch 7:(280/293) batch || training time for 2 batch 3.013580799102783 || training loss 0.0007676182431168854 ||\n","epoch 7:(282/293) batch || training time for 2 batch 2.9728283882141113 || training loss 0.0006551593542098999 ||\n","epoch 7:(284/293) batch || training time for 2 batch 2.9470903873443604 || training loss 0.0006060453597456217 ||\n","epoch 7:(286/293) batch || training time for 2 batch 2.9157068729400635 || training loss 0.0006081067258492112 ||\n","epoch 7:(288/293) batch || training time for 2 batch 2.9237539768218994 || training loss 0.0005833239411003888 ||\n","epoch 7:(290/293) batch || training time for 2 batch 2.9323537349700928 || training loss 0.0006657413905486465 ||\n","epoch 7:(292/293) batch || training time for 2 batch 2.9134418964385986 || training loss 0.0006298573280218989 ||\n","epoch 8:(2/293) batch || training time for 2 batch 4.375110626220703 || training loss 0.0009520889725536108 ||\n","epoch 8:(4/293) batch || training time for 2 batch 2.908006429672241 || training loss 0.0006645633839070797 ||\n","epoch 8:(6/293) batch || training time for 2 batch 2.9155478477478027 || training loss 0.0006757827941328287 ||\n","epoch 8:(8/293) batch || training time for 2 batch 2.922102928161621 || training loss 0.0005732947611249983 ||\n","epoch 8:(10/293) batch || training time for 2 batch 2.917184829711914 || training loss 0.000678372394759208 ||\n","epoch 8:(12/293) batch || training time for 2 batch 2.897165060043335 || training loss 0.0006488791259471327 ||\n","epoch 8:(14/293) batch || training time for 2 batch 2.9126031398773193 || training loss 0.0006408223998732865 ||\n","epoch 8:(16/293) batch || training time for 2 batch 2.9459996223449707 || training loss 0.010844948294106871 ||\n","epoch 8:(18/293) batch || training time for 2 batch 2.9392616748809814 || training loss 0.0005885817226953804 ||\n","epoch 8:(20/293) batch || training time for 2 batch 2.9476654529571533 || training loss 0.0006234494503587484 ||\n","epoch 8:(22/293) batch || training time for 2 batch 2.9083590507507324 || training loss 0.0007538786740042269 ||\n","epoch 8:(24/293) batch || training time for 2 batch 2.9403254985809326 || training loss 0.0007733787642791867 ||\n","epoch 8:(26/293) batch || training time for 2 batch 2.938262701034546 || training loss 0.0005914557841606438 ||\n","epoch 8:(28/293) batch || training time for 2 batch 2.990272045135498 || training loss 0.0029067872965242714 ||\n","epoch 8:(30/293) batch || training time for 2 batch 2.96915340423584 || training loss 0.0006073310796637088 ||\n","epoch 8:(32/293) batch || training time for 2 batch 2.930933713912964 || training loss 0.0007282868027687073 ||\n","epoch 8:(34/293) batch || training time for 2 batch 2.9730329513549805 || training loss 0.0006157428433652967 ||\n","epoch 8:(36/293) batch || training time for 2 batch 2.9529764652252197 || training loss 0.002407722524367273 ||\n","epoch 8:(38/293) batch || training time for 2 batch 2.976473093032837 || training loss 0.000589128234423697 ||\n","epoch 8:(40/293) batch || training time for 2 batch 2.958266019821167 || training loss 0.0006112249102443457 ||\n","epoch 8:(42/293) batch || training time for 2 batch 2.9699864387512207 || training loss 0.000618608872173354 ||\n","epoch 8:(44/293) batch || training time for 2 batch 2.9362082481384277 || training loss 0.0005795492324978113 ||\n","epoch 8:(46/293) batch || training time for 2 batch 2.9582881927490234 || training loss 0.0005764034867752343 ||\n","epoch 8:(48/293) batch || training time for 2 batch 2.954228401184082 || training loss 0.0005916186491958797 ||\n","epoch 8:(50/293) batch || training time for 2 batch 2.926044225692749 || training loss 0.0005619087896775454 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 67.23714971542358s\n","epoch 8:(52/293) batch || training time for 2 batch 73.60869979858398 || training loss 0.0005442688125185668 ||\n","epoch 8:(54/293) batch || training time for 2 batch 2.9008567333221436 || training loss 0.0005287786771077663 ||\n","epoch 8:(56/293) batch || training time for 2 batch 2.9173991680145264 || training loss 0.0006122094637248665 ||\n","epoch 8:(58/293) batch || training time for 2 batch 2.9202511310577393 || training loss 0.006749585940269753 ||\n","epoch 8:(60/293) batch || training time for 2 batch 2.9303789138793945 || training loss 0.0006491286330856383 ||\n","epoch 8:(62/293) batch || training time for 2 batch 2.935899019241333 || training loss 0.0025332606164738536 ||\n","epoch 8:(64/293) batch || training time for 2 batch 2.9749062061309814 || training loss 0.0005415705381892622 ||\n","epoch 8:(66/293) batch || training time for 2 batch 3.0008225440979004 || training loss 0.0005843076505698264 ||\n","epoch 8:(68/293) batch || training time for 2 batch 3.0154998302459717 || training loss 0.0006808408943470567 ||\n","epoch 8:(70/293) batch || training time for 2 batch 3.0258352756500244 || training loss 0.0005948010075371712 ||\n","epoch 8:(72/293) batch || training time for 2 batch 3.031352996826172 || training loss 0.0005585357430391014 ||\n","epoch 8:(74/293) batch || training time for 2 batch 3.036428928375244 || training loss 0.00937094198889099 ||\n","epoch 8:(76/293) batch || training time for 2 batch 3.0335726737976074 || training loss 0.0005580805009230971 ||\n","epoch 8:(78/293) batch || training time for 2 batch 3.0103278160095215 || training loss 0.0005579461576417089 ||\n","epoch 8:(80/293) batch || training time for 2 batch 2.987133502960205 || training loss 0.0005301854107528925 ||\n","epoch 8:(82/293) batch || training time for 2 batch 2.947248935699463 || training loss 0.0005663848132826388 ||\n","epoch 8:(84/293) batch || training time for 2 batch 2.943225622177124 || training loss 0.0017118273972300813 ||\n","epoch 8:(86/293) batch || training time for 2 batch 2.961195468902588 || training loss 0.0005230508832028136 ||\n","epoch 8:(88/293) batch || training time for 2 batch 2.930542469024658 || training loss 0.0005840750818606466 ||\n","epoch 8:(90/293) batch || training time for 2 batch 2.918224334716797 || training loss 0.0005484152061399072 ||\n","epoch 8:(92/293) batch || training time for 2 batch 2.9355061054229736 || training loss 0.0005613774992525578 ||\n","epoch 8:(94/293) batch || training time for 2 batch 2.9120230674743652 || training loss 0.001475523691624403 ||\n","epoch 8:(96/293) batch || training time for 2 batch 2.9016661643981934 || training loss 0.0005002241232432425 ||\n","epoch 8:(98/293) batch || training time for 2 batch 2.9333791732788086 || training loss 0.0005703328351955861 ||\n","epoch 8:(100/293) batch || training time for 2 batch 2.934061288833618 || training loss 0.0005451262113638222 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 72.99109101295471s\n","epoch 8:(102/293) batch || training time for 2 batch 79.40240168571472 || training loss 0.0006707093270961195 ||\n","epoch 8:(104/293) batch || training time for 2 batch 2.8758537769317627 || training loss 0.0005510565242730081 ||\n","epoch 8:(106/293) batch || training time for 2 batch 2.917689800262451 || training loss 0.0005519323167391121 ||\n","epoch 8:(108/293) batch || training time for 2 batch 2.924046039581299 || training loss 0.0005867986183147877 ||\n","epoch 8:(110/293) batch || training time for 2 batch 2.940263271331787 || training loss 0.0006253647152334452 ||\n","epoch 8:(112/293) batch || training time for 2 batch 2.91804575920105 || training loss 0.000565092108445242 ||\n","epoch 8:(114/293) batch || training time for 2 batch 3.008237838745117 || training loss 0.0005533583462238312 ||\n","epoch 8:(116/293) batch || training time for 2 batch 3.010087251663208 || training loss 0.0005794812168460339 ||\n","epoch 8:(118/293) batch || training time for 2 batch 3.02336049079895 || training loss 0.0006303436821326613 ||\n","epoch 8:(120/293) batch || training time for 2 batch 3.0255422592163086 || training loss 0.000535530096385628 ||\n","epoch 8:(122/293) batch || training time for 2 batch 3.045281171798706 || training loss 0.000543306436156854 ||\n","epoch 8:(124/293) batch || training time for 2 batch 3.0517001152038574 || training loss 0.0005565603205468506 ||\n","epoch 8:(126/293) batch || training time for 2 batch 3.0313899517059326 || training loss 0.0004954640317009762 ||\n","epoch 8:(128/293) batch || training time for 2 batch 3.016613006591797 || training loss 0.0005803705425933003 ||\n","epoch 8:(130/293) batch || training time for 2 batch 2.982790946960449 || training loss 0.0004883912624791265 ||\n","epoch 8:(132/293) batch || training time for 2 batch 2.9377007484436035 || training loss 0.0005859193333890289 ||\n","epoch 8:(134/293) batch || training time for 2 batch 2.9290385246276855 || training loss 0.0005423715920187533 ||\n","epoch 8:(136/293) batch || training time for 2 batch 2.9207029342651367 || training loss 0.0004899899504380301 ||\n","epoch 8:(138/293) batch || training time for 2 batch 2.9106616973876953 || training loss 0.0005513242504093796 ||\n","epoch 8:(140/293) batch || training time for 2 batch 2.972107410430908 || training loss 0.0005019990494474769 ||\n","epoch 8:(142/293) batch || training time for 2 batch 2.923872470855713 || training loss 0.0007347161299549043 ||\n","epoch 8:(144/293) batch || training time for 2 batch 2.949918031692505 || training loss 0.0005076391971670091 ||\n","epoch 8:(146/293) batch || training time for 2 batch 2.9246344566345215 || training loss 0.0004900805361103266 ||\n","epoch 8:(148/293) batch || training time for 2 batch 2.9240481853485107 || training loss 0.0038946193526498973 ||\n","epoch 8:(150/293) batch || training time for 2 batch 2.899050712585449 || training loss 0.0026595452218316495 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 71.05250334739685s\n","epoch 8:(152/293) batch || training time for 2 batch 77.38410711288452 || training loss 0.0005838821234647185 ||\n","epoch 8:(154/293) batch || training time for 2 batch 2.8894269466400146 || training loss 0.000538894790224731 ||\n","epoch 8:(156/293) batch || training time for 2 batch 2.9065258502960205 || training loss 0.0010217369999736547 ||\n","epoch 8:(158/293) batch || training time for 2 batch 2.9251461029052734 || training loss 0.0004928858834318817 ||\n","epoch 8:(160/293) batch || training time for 2 batch 2.930436849594116 || training loss 0.0004979966906830668 ||\n","epoch 8:(162/293) batch || training time for 2 batch 2.9545016288757324 || training loss 0.000465498334961012 ||\n","epoch 8:(164/293) batch || training time for 2 batch 2.9716455936431885 || training loss 0.0005487283924594522 ||\n","epoch 8:(166/293) batch || training time for 2 batch 2.9942803382873535 || training loss 0.0005008934531360865 ||\n","epoch 8:(168/293) batch || training time for 2 batch 2.9783992767333984 || training loss 0.0004761835443787277 ||\n","epoch 8:(170/293) batch || training time for 2 batch 2.996162176132202 || training loss 0.0006189669948071241 ||\n","epoch 8:(172/293) batch || training time for 2 batch 3.073720932006836 || training loss 0.0006757320661563426 ||\n","epoch 8:(174/293) batch || training time for 2 batch 3.036320209503174 || training loss 0.0005096692475490272 ||\n","epoch 8:(176/293) batch || training time for 2 batch 3.04545521736145 || training loss 0.0005517632671399042 ||\n","epoch 8:(178/293) batch || training time for 2 batch 3.013176202774048 || training loss 0.0005540050042327493 ||\n","epoch 8:(180/293) batch || training time for 2 batch 2.983992338180542 || training loss 0.0004862937639700249 ||\n","epoch 8:(182/293) batch || training time for 2 batch 2.9371981620788574 || training loss 0.0004940794897265732 ||\n","epoch 8:(184/293) batch || training time for 2 batch 2.9686012268066406 || training loss 0.0004971159069100395 ||\n","epoch 8:(186/293) batch || training time for 2 batch 2.9392600059509277 || training loss 0.0005358044581953436 ||\n","epoch 8:(188/293) batch || training time for 2 batch 2.9335901737213135 || training loss 0.00047164228453766555 ||\n","epoch 8:(190/293) batch || training time for 2 batch 2.9275240898132324 || training loss 0.0004879680636804551 ||\n","epoch 8:(192/293) batch || training time for 2 batch 2.9024293422698975 || training loss 0.0007430363330058753 ||\n","epoch 8:(194/293) batch || training time for 2 batch 2.922590970993042 || training loss 0.0004696065734606236 ||\n","epoch 8:(196/293) batch || training time for 2 batch 2.911273956298828 || training loss 0.0005166590854059905 ||\n","epoch 8:(198/293) batch || training time for 2 batch 2.9190139770507812 || training loss 0.0004946219269186258 ||\n","4.398046511104002e-06\n","epoch 8:(200/293) batch || training time for 2 batch 2.932760238647461 || training loss 0.00047900780919007957 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 73.79485177993774s\n","epoch 8:(202/293) batch || training time for 2 batch 80.14076519012451 || training loss 0.0005734179576393217 ||\n","epoch 8:(204/293) batch || training time for 2 batch 2.9166369438171387 || training loss 0.0004379882593639195 ||\n","epoch 8:(206/293) batch || training time for 2 batch 2.90600848197937 || training loss 0.0004883528454229236 ||\n","epoch 8:(208/293) batch || training time for 2 batch 2.911735773086548 || training loss 0.0005345048266462982 ||\n","epoch 8:(210/293) batch || training time for 2 batch 2.9543230533599854 || training loss 0.00047422037459909916 ||\n","epoch 8:(212/293) batch || training time for 2 batch 2.954413652420044 || training loss 0.0004456165770534426 ||\n","epoch 8:(214/293) batch || training time for 2 batch 2.978774070739746 || training loss 0.0004947121342411265 ||\n","epoch 8:(216/293) batch || training time for 2 batch 3.0170493125915527 || training loss 0.0005651853571180254 ||\n","epoch 8:(218/293) batch || training time for 2 batch 3.0190982818603516 || training loss 0.00047930472646839917 ||\n","epoch 8:(220/293) batch || training time for 2 batch 3.049328327178955 || training loss 0.0008171944646164775 ||\n","epoch 8:(222/293) batch || training time for 2 batch 3.0321578979492188 || training loss 0.0004808636149391532 ||\n","epoch 8:(224/293) batch || training time for 2 batch 3.0398690700531006 || training loss 0.00047601782716810703 ||\n","epoch 8:(226/293) batch || training time for 2 batch 3.0134429931640625 || training loss 0.00045290132402442396 ||\n","epoch 8:(228/293) batch || training time for 2 batch 2.990344762802124 || training loss 0.0004711110086645931 ||\n","epoch 8:(230/293) batch || training time for 2 batch 2.970951557159424 || training loss 0.00042720476631075144 ||\n","epoch 8:(232/293) batch || training time for 2 batch 2.922708749771118 || training loss 0.0005104859592393041 ||\n","epoch 8:(234/293) batch || training time for 2 batch 2.9304182529449463 || training loss 0.0017222464957740158 ||\n","epoch 8:(236/293) batch || training time for 2 batch 2.925332546234131 || training loss 0.0004739634314319119 ||\n","epoch 8:(238/293) batch || training time for 2 batch 2.937696933746338 || training loss 0.0004428258107509464 ||\n","epoch 8:(240/293) batch || training time for 2 batch 2.929884433746338 || training loss 0.0005047665763413534 ||\n","epoch 8:(242/293) batch || training time for 2 batch 2.896416425704956 || training loss 0.0004866189701715484 ||\n","epoch 8:(244/293) batch || training time for 2 batch 2.9147276878356934 || training loss 0.0010611856705509126 ||\n","epoch 8:(246/293) batch || training time for 2 batch 2.909393787384033 || training loss 0.0005333544104360044 ||\n","epoch 8:(248/293) batch || training time for 2 batch 2.866058588027954 || training loss 0.0005115187668707222 ||\n","epoch 8:(250/293) batch || training time for 2 batch 2.9129798412323 || training loss 0.00047663686564192176 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 67.90717625617981s\n","epoch 8:(252/293) batch || training time for 2 batch 74.33968019485474 || training loss 0.0004957298224326223 ||\n","epoch 8:(254/293) batch || training time for 2 batch 2.882566213607788 || training loss 0.0004366211360320449 ||\n","epoch 8:(256/293) batch || training time for 2 batch 2.8909354209899902 || training loss 0.003440899046836421 ||\n","epoch 8:(258/293) batch || training time for 2 batch 2.908470630645752 || training loss 0.004726126964669675 ||\n","epoch 8:(260/293) batch || training time for 2 batch 2.9328370094299316 || training loss 0.0005391852610046044 ||\n","epoch 8:(262/293) batch || training time for 2 batch 2.9814019203186035 || training loss 0.0005313617875799537 ||\n","epoch 8:(264/293) batch || training time for 2 batch 2.962110996246338 || training loss 0.0006360734987538308 ||\n","epoch 8:(266/293) batch || training time for 2 batch 2.994497299194336 || training loss 0.00047482061199843884 ||\n","epoch 8:(268/293) batch || training time for 2 batch 3.011305093765259 || training loss 0.000420404045144096 ||\n","epoch 8:(270/293) batch || training time for 2 batch 3.029543399810791 || training loss 0.0011001585808116943 ||\n","epoch 8:(272/293) batch || training time for 2 batch 3.02327036857605 || training loss 0.00046981347259134054 ||\n","epoch 8:(274/293) batch || training time for 2 batch 3.026319980621338 || training loss 0.0004333235847298056 ||\n","epoch 8:(276/293) batch || training time for 2 batch 2.986614465713501 || training loss 0.0005984820600133389 ||\n","epoch 8:(278/293) batch || training time for 2 batch 2.9974474906921387 || training loss 0.00044621407869271934 ||\n","epoch 8:(280/293) batch || training time for 2 batch 2.9783055782318115 || training loss 0.00047989281301852316 ||\n","epoch 8:(282/293) batch || training time for 2 batch 2.9775705337524414 || training loss 0.0004193825734546408 ||\n","epoch 8:(284/293) batch || training time for 2 batch 2.9514379501342773 || training loss 0.00047533688484691083 ||\n","epoch 8:(286/293) batch || training time for 2 batch 2.9567182064056396 || training loss 0.00042389464215375483 ||\n","epoch 8:(288/293) batch || training time for 2 batch 2.934666156768799 || training loss 0.0004305433249101043 ||\n","epoch 8:(290/293) batch || training time for 2 batch 2.9266293048858643 || training loss 0.000583098764764145 ||\n","epoch 8:(292/293) batch || training time for 2 batch 2.9156854152679443 || training loss 0.0009941472089849412 ||\n","epoch 9:(2/293) batch || training time for 2 batch 4.385580062866211 || training loss 0.0007989695150172338 ||\n","epoch 9:(4/293) batch || training time for 2 batch 2.9025940895080566 || training loss 0.0004803908086614683 ||\n","epoch 9:(6/293) batch || training time for 2 batch 2.91098690032959 || training loss 0.0005292895948514342 ||\n","epoch 9:(8/293) batch || training time for 2 batch 2.8957602977752686 || training loss 0.00048232119297608733 ||\n","epoch 9:(10/293) batch || training time for 2 batch 2.917903184890747 || training loss 0.0004443526268005371 ||\n","epoch 9:(12/293) batch || training time for 2 batch 2.893653631210327 || training loss 0.0005308294930728152 ||\n","epoch 9:(14/293) batch || training time for 2 batch 2.9319119453430176 || training loss 0.0005181271553738043 ||\n","epoch 9:(16/293) batch || training time for 2 batch 2.913220167160034 || training loss 0.006586456947843544 ||\n","epoch 9:(18/293) batch || training time for 2 batch 2.942098379135132 || training loss 0.0004117543576285243 ||\n","epoch 9:(20/293) batch || training time for 2 batch 2.927016258239746 || training loss 0.0004460914060473442 ||\n","epoch 9:(22/293) batch || training time for 2 batch 2.9510207176208496 || training loss 0.0004339004517532885 ||\n","epoch 9:(24/293) batch || training time for 2 batch 2.9340827465057373 || training loss 0.001664257375523448 ||\n","epoch 9:(26/293) batch || training time for 2 batch 2.964087963104248 || training loss 0.0004365378845250234 ||\n","epoch 9:(28/293) batch || training time for 2 batch 2.9740233421325684 || training loss 0.0004272081860108301 ||\n","epoch 9:(30/293) batch || training time for 2 batch 2.993910312652588 || training loss 0.0004762815951835364 ||\n","epoch 9:(32/293) batch || training time for 2 batch 3.0035295486450195 || training loss 0.006652047217357904 ||\n","epoch 9:(34/293) batch || training time for 2 batch 2.9927029609680176 || training loss 0.0004380715836305171 ||\n","epoch 9:(36/293) batch || training time for 2 batch 2.9561047554016113 || training loss 0.00044707213237416 ||\n","epoch 9:(38/293) batch || training time for 2 batch 2.9549026489257812 || training loss 0.00044598225213121623 ||\n","epoch 9:(40/293) batch || training time for 2 batch 2.956606149673462 || training loss 0.00046335122897289693 ||\n","epoch 9:(42/293) batch || training time for 2 batch 2.9632256031036377 || training loss 0.00046921490866225213 ||\n","epoch 9:(44/293) batch || training time for 2 batch 2.946805238723755 || training loss 0.00041321292519569397 ||\n","epoch 9:(46/293) batch || training time for 2 batch 2.909726619720459 || training loss 0.0004744002362713218 ||\n","epoch 9:(48/293) batch || training time for 2 batch 2.93742036819458 || training loss 0.00038714881520718336 ||\n","epoch 9:(50/293) batch || training time for 2 batch 2.9156715869903564 || training loss 0.0005169890355318785 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 69.87337350845337s\n","epoch 9:(52/293) batch || training time for 2 batch 76.24464011192322 || training loss 0.0008914123463910073 ||\n","epoch 9:(54/293) batch || training time for 2 batch 2.900651454925537 || training loss 0.0009692840685602278 ||\n","epoch 9:(56/293) batch || training time for 2 batch 2.8774728775024414 || training loss 0.0004350661765784025 ||\n","epoch 9:(58/293) batch || training time for 2 batch 2.955277681350708 || training loss 0.00043179214117117226 ||\n","epoch 9:(60/293) batch || training time for 2 batch 2.940812110900879 || training loss 0.0006607335526496172 ||\n","epoch 9:(62/293) batch || training time for 2 batch 2.9540822505950928 || training loss 0.00043427105993032455 ||\n","epoch 9:(64/293) batch || training time for 2 batch 2.9991862773895264 || training loss 0.0004963736864738166 ||\n","epoch 9:(66/293) batch || training time for 2 batch 3.0253024101257324 || training loss 0.0006691888265777379 ||\n","epoch 9:(68/293) batch || training time for 2 batch 3.031334400177002 || training loss 0.00038831986603327096 ||\n","epoch 9:(70/293) batch || training time for 2 batch 3.053705930709839 || training loss 0.00040639768121764064 ||\n","epoch 9:(72/293) batch || training time for 2 batch 3.015083074569702 || training loss 0.00041075296758208424 ||\n","epoch 9:(74/293) batch || training time for 2 batch 3.0189156532287598 || training loss 0.00045742091606371105 ||\n","epoch 9:(76/293) batch || training time for 2 batch 3.0079879760742188 || training loss 0.000449377010227181 ||\n","epoch 9:(78/293) batch || training time for 2 batch 2.9899017810821533 || training loss 0.0006903321918798611 ||\n","epoch 9:(80/293) batch || training time for 2 batch 2.983464002609253 || training loss 0.000424367084633559 ||\n","epoch 9:(82/293) batch || training time for 2 batch 2.9694364070892334 || training loss 0.00043163003283552825 ||\n","epoch 9:(84/293) batch || training time for 2 batch 2.9289417266845703 || training loss 0.000458660681033507 ||\n","epoch 9:(86/293) batch || training time for 2 batch 2.9093644618988037 || training loss 0.00044952600728720427 ||\n","epoch 9:(88/293) batch || training time for 2 batch 2.920041561126709 || training loss 0.00041105065611191094 ||\n","epoch 9:(90/293) batch || training time for 2 batch 2.951521873474121 || training loss 0.00039965166070032865 ||\n","epoch 9:(92/293) batch || training time for 2 batch 2.9016237258911133 || training loss 0.0004242329014232382 ||\n","epoch 9:(94/293) batch || training time for 2 batch 2.895651340484619 || training loss 0.000507855846080929 ||\n","epoch 9:(96/293) batch || training time for 2 batch 2.911104202270508 || training loss 0.0006526107754325494 ||\n","epoch 9:(98/293) batch || training time for 2 batch 2.93692946434021 || training loss 0.00044167524902150035 ||\n","epoch 9:(100/293) batch || training time for 2 batch 2.911499261856079 || training loss 0.0022354323155013844 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 75.7682192325592s\n","epoch 9:(102/293) batch || training time for 2 batch 82.16553664207458 || training loss 0.0004013827128801495 ||\n","epoch 9:(104/293) batch || training time for 2 batch 2.8857455253601074 || training loss 0.0004351451643742621 ||\n","epoch 9:(106/293) batch || training time for 2 batch 2.90739369392395 || training loss 0.0019604656554292887 ||\n","epoch 9:(108/293) batch || training time for 2 batch 2.9267327785491943 || training loss 0.00043603588710539043 ||\n","epoch 9:(110/293) batch || training time for 2 batch 2.9526593685150146 || training loss 0.00047211386845447123 ||\n","epoch 9:(112/293) batch || training time for 2 batch 2.954817771911621 || training loss 0.000388555257814005 ||\n","epoch 9:(114/293) batch || training time for 2 batch 2.983076333999634 || training loss 0.00042325875256210566 ||\n","epoch 9:(116/293) batch || training time for 2 batch 2.9947872161865234 || training loss 0.008788082574028522 ||\n","epoch 9:(118/293) batch || training time for 2 batch 2.9974398612976074 || training loss 0.00044287004857324064 ||\n","epoch 9:(120/293) batch || training time for 2 batch 3.031108856201172 || training loss 0.0004197809030301869 ||\n","epoch 9:(122/293) batch || training time for 2 batch 3.0330123901367188 || training loss 0.0003957720327889547 ||\n","epoch 9:(124/293) batch || training time for 2 batch 3.040761709213257 || training loss 0.00041643720760475844 ||\n","epoch 9:(126/293) batch || training time for 2 batch 3.0235702991485596 || training loss 0.0004101086815353483 ||\n","epoch 9:(128/293) batch || training time for 2 batch 2.956883192062378 || training loss 0.00043406826443970203 ||\n","epoch 9:(130/293) batch || training time for 2 batch 2.975987195968628 || training loss 0.0003825852763839066 ||\n","epoch 9:(132/293) batch || training time for 2 batch 2.9583678245544434 || training loss 0.00043174145685043186 ||\n","epoch 9:(134/293) batch || training time for 2 batch 2.922084093093872 || training loss 0.0006384030857589096 ||\n","epoch 9:(136/293) batch || training time for 2 batch 2.941615581512451 || training loss 0.0006039431900717318 ||\n","epoch 9:(138/293) batch || training time for 2 batch 2.936366081237793 || training loss 0.00039167728391475976 ||\n","epoch 9:(140/293) batch || training time for 2 batch 2.9158124923706055 || training loss 0.00047412583080586046 ||\n","epoch 9:(142/293) batch || training time for 2 batch 2.911492109298706 || training loss 0.0004238711699144915 ||\n","epoch 9:(144/293) batch || training time for 2 batch 2.8991801738739014 || training loss 0.00042019107786472887 ||\n","epoch 9:(146/293) batch || training time for 2 batch 2.906061887741089 || training loss 0.00040751729102339596 ||\n","epoch 9:(148/293) batch || training time for 2 batch 2.8778042793273926 || training loss 0.0018152630073018372 ||\n","epoch 9:(150/293) batch || training time for 2 batch 2.907024383544922 || training loss 0.0004273883532732725 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 66.87744736671448s\n","epoch 9:(152/293) batch || training time for 2 batch 73.28665947914124 || training loss 0.007416762819048017 ||\n","epoch 9:(154/293) batch || training time for 2 batch 2.916090488433838 || training loss 0.0004848830576520413 ||\n","epoch 9:(156/293) batch || training time for 2 batch 2.8722727298736572 || training loss 0.000402598045184277 ||\n","epoch 9:(158/293) batch || training time for 2 batch 2.8996641635894775 || training loss 0.0004090390575584024 ||\n","epoch 9:(160/293) batch || training time for 2 batch 2.9475090503692627 || training loss 0.00043175499013159424 ||\n","epoch 9:(162/293) batch || training time for 2 batch 2.9439280033111572 || training loss 0.0004324874607846141 ||\n","epoch 9:(164/293) batch || training time for 2 batch 2.950542688369751 || training loss 0.0004288472409825772 ||\n","epoch 9:(166/293) batch || training time for 2 batch 3.036675214767456 || training loss 0.0006166124949231744 ||\n","epoch 9:(168/293) batch || training time for 2 batch 3.015934944152832 || training loss 0.0004067873669555411 ||\n","epoch 9:(170/293) batch || training time for 2 batch 3.011864423751831 || training loss 0.00044537124631460756 ||\n","epoch 9:(172/293) batch || training time for 2 batch 3.0313403606414795 || training loss 0.00043064252531621605 ||\n","epoch 9:(174/293) batch || training time for 2 batch 3.0271477699279785 || training loss 0.0003995334991486743 ||\n","epoch 9:(176/293) batch || training time for 2 batch 3.0139598846435547 || training loss 0.0004026398964924738 ||\n","epoch 9:(178/293) batch || training time for 2 batch 3.0268168449401855 || training loss 0.00041398921166546643 ||\n","epoch 9:(180/293) batch || training time for 2 batch 3.007063388824463 || training loss 0.00046595881576649845 ||\n","epoch 9:(182/293) batch || training time for 2 batch 2.9548118114471436 || training loss 0.0003771577321458608 ||\n","epoch 9:(184/293) batch || training time for 2 batch 2.966597318649292 || training loss 0.004384416752145626 ||\n","epoch 9:(186/293) batch || training time for 2 batch 2.9259696006774902 || training loss 0.0004410038236528635 ||\n","epoch 9:(188/293) batch || training time for 2 batch 2.914682388305664 || training loss 0.0004994402988813818 ||\n","epoch 9:(190/293) batch || training time for 2 batch 2.9115538597106934 || training loss 0.0003772420604946092 ||\n","epoch 9:(192/293) batch || training time for 2 batch 2.8992538452148438 || training loss 0.001343064708635211 ||\n","epoch 9:(194/293) batch || training time for 2 batch 2.883283853530884 || training loss 0.00037726589653175324 ||\n","epoch 9:(196/293) batch || training time for 2 batch 2.8902711868286133 || training loss 0.00039851364272180945 ||\n","epoch 9:(198/293) batch || training time for 2 batch 2.881012201309204 || training loss 0.00038123039121273905 ||\n","3.518437208883202e-06\n","epoch 9:(200/293) batch || training time for 2 batch 2.9023075103759766 || training loss 0.0004098719800822437 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 68.82973408699036s\n","epoch 9:(202/293) batch || training time for 2 batch 75.19389081001282 || training loss 0.0004076794139109552 ||\n","epoch 9:(204/293) batch || training time for 2 batch 2.8616116046905518 || training loss 0.0015131102409213781 ||\n","epoch 9:(206/293) batch || training time for 2 batch 2.8950958251953125 || training loss 0.0018900178838521242 ||\n","epoch 9:(208/293) batch || training time for 2 batch 2.9168105125427246 || training loss 0.0003626126126619056 ||\n","epoch 9:(210/293) batch || training time for 2 batch 2.9362807273864746 || training loss 0.0003874261601595208 ||\n","epoch 9:(212/293) batch || training time for 2 batch 2.947078227996826 || training loss 0.0004123889666516334 ||\n","epoch 9:(214/293) batch || training time for 2 batch 2.9831559658050537 || training loss 0.0004733549430966377 ||\n","epoch 9:(216/293) batch || training time for 2 batch 2.995595932006836 || training loss 0.00043534525320865214 ||\n","epoch 9:(218/293) batch || training time for 2 batch 2.9919261932373047 || training loss 0.00045523999142460525 ||\n","epoch 9:(220/293) batch || training time for 2 batch 3.0189032554626465 || training loss 0.0003803765430347994 ||\n","epoch 9:(222/293) batch || training time for 2 batch 3.0527236461639404 || training loss 0.00038701159064657986 ||\n","epoch 9:(224/293) batch || training time for 2 batch 3.095149278640747 || training loss 0.0004382492625154555 ||\n","epoch 9:(226/293) batch || training time for 2 batch 3.059636116027832 || training loss 0.0014609979698434472 ||\n","epoch 9:(228/293) batch || training time for 2 batch 2.9668173789978027 || training loss 0.0004706797917606309 ||\n","epoch 9:(230/293) batch || training time for 2 batch 2.9796879291534424 || training loss 0.00041309877997264266 ||\n","epoch 9:(232/293) batch || training time for 2 batch 2.9430997371673584 || training loss 0.00034633972973097116 ||\n","epoch 9:(234/293) batch || training time for 2 batch 2.949394702911377 || training loss 0.0006788347236579284 ||\n","epoch 9:(236/293) batch || training time for 2 batch 2.9268381595611572 || training loss 0.00041240530845243484 ||\n","epoch 9:(238/293) batch || training time for 2 batch 2.923461437225342 || training loss 0.00037794592208229005 ||\n","epoch 9:(240/293) batch || training time for 2 batch 2.8898258209228516 || training loss 0.0003946813667425886 ||\n","epoch 9:(242/293) batch || training time for 2 batch 2.925008773803711 || training loss 0.0003459000581642613 ||\n","epoch 9:(244/293) batch || training time for 2 batch 2.909457206726074 || training loss 0.00039788927824702114 ||\n","epoch 9:(246/293) batch || training time for 2 batch 2.9066200256347656 || training loss 0.00043413357343524694 ||\n","epoch 9:(248/293) batch || training time for 2 batch 2.919180393218994 || training loss 0.0007249143527587876 ||\n","epoch 9:(250/293) batch || training time for 2 batch 2.8968429565429688 || training loss 0.0006176091847009957 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 68.60199427604675s\n","epoch 9:(252/293) batch || training time for 2 batch 74.91709685325623 || training loss 0.00042784103425219655 ||\n","epoch 9:(254/293) batch || training time for 2 batch 2.888101577758789 || training loss 0.00040090797119773924 ||\n","epoch 9:(256/293) batch || training time for 2 batch 2.9047768115997314 || training loss 0.0003629025159170851 ||\n","epoch 9:(258/293) batch || training time for 2 batch 2.9052956104278564 || training loss 0.00040334666846320033 ||\n","epoch 9:(260/293) batch || training time for 2 batch 2.946690320968628 || training loss 0.00040501133480574936 ||\n","epoch 9:(262/293) batch || training time for 2 batch 2.9544129371643066 || training loss 0.0009666761907283217 ||\n","epoch 9:(264/293) batch || training time for 2 batch 2.999933958053589 || training loss 0.00038597063394263387 ||\n","epoch 9:(266/293) batch || training time for 2 batch 2.9686577320098877 || training loss 0.0005600812291959301 ||\n","epoch 9:(268/293) batch || training time for 2 batch 2.9457414150238037 || training loss 0.00038703528116457164 ||\n","epoch 9:(270/293) batch || training time for 2 batch 3.0169644355773926 || training loss 0.00035555256181396544 ||\n","epoch 9:(272/293) batch || training time for 2 batch 3.0135207176208496 || training loss 0.00044496048940345645 ||\n","epoch 9:(274/293) batch || training time for 2 batch 3.0410120487213135 || training loss 0.0004293825477361679 ||\n","epoch 9:(276/293) batch || training time for 2 batch 3.007234811782837 || training loss 0.00042224164644721895 ||\n","epoch 9:(278/293) batch || training time for 2 batch 3.0406713485717773 || training loss 0.005154167301952839 ||\n","epoch 9:(280/293) batch || training time for 2 batch 2.977984666824341 || training loss 0.0003630377468653023 ||\n","epoch 9:(282/293) batch || training time for 2 batch 2.9702718257904053 || training loss 0.0004121657693758607 ||\n","epoch 9:(284/293) batch || training time for 2 batch 2.965146541595459 || training loss 0.00039837538497522473 ||\n","epoch 9:(286/293) batch || training time for 2 batch 2.9521114826202393 || training loss 0.00037352934305090457 ||\n","epoch 9:(288/293) batch || training time for 2 batch 2.9265456199645996 || training loss 0.00042501394636929035 ||\n","epoch 9:(290/293) batch || training time for 2 batch 2.887211561203003 || training loss 0.0004579432716127485 ||\n","epoch 9:(292/293) batch || training time for 2 batch 2.9395341873168945 || training loss 0.0004017695609945804 ||\n","epoch 10:(2/293) batch || training time for 2 batch 4.38723611831665 || training loss 0.0005326608952600509 ||\n","epoch 10:(4/293) batch || training time for 2 batch 2.897033452987671 || training loss 0.0003826560277957469 ||\n","epoch 10:(6/293) batch || training time for 2 batch 2.892324209213257 || training loss 0.00044183812860865146 ||\n","epoch 10:(8/293) batch || training time for 2 batch 2.8966002464294434 || training loss 0.0003382590366527438 ||\n","epoch 10:(10/293) batch || training time for 2 batch 2.917018413543701 || training loss 0.0003675579355331138 ||\n","epoch 10:(12/293) batch || training time for 2 batch 2.8957247734069824 || training loss 0.00040124275255948305 ||\n","epoch 10:(14/293) batch || training time for 2 batch 2.8991410732269287 || training loss 0.0003510563838062808 ||\n","epoch 10:(16/293) batch || training time for 2 batch 2.897517204284668 || training loss 0.000354483665432781 ||\n","epoch 10:(18/293) batch || training time for 2 batch 2.954836130142212 || training loss 0.00036412099143490195 ||\n","epoch 10:(20/293) batch || training time for 2 batch 2.920156240463257 || training loss 0.00036360656667966396 ||\n","epoch 10:(22/293) batch || training time for 2 batch 2.915508270263672 || training loss 0.0003835118841379881 ||\n","epoch 10:(24/293) batch || training time for 2 batch 2.9206457138061523 || training loss 0.0004315798287279904 ||\n","epoch 10:(26/293) batch || training time for 2 batch 2.9761312007904053 || training loss 0.0003512685216264799 ||\n","epoch 10:(28/293) batch || training time for 2 batch 2.930588722229004 || training loss 0.0003853403322864324 ||\n","epoch 10:(30/293) batch || training time for 2 batch 2.977609157562256 || training loss 0.00036421173717826605 ||\n","epoch 10:(32/293) batch || training time for 2 batch 2.9561843872070312 || training loss 0.00036514943349175155 ||\n","epoch 10:(34/293) batch || training time for 2 batch 2.945223331451416 || training loss 0.0003928024525521323 ||\n","epoch 10:(36/293) batch || training time for 2 batch 2.980499744415283 || training loss 0.0003297276853118092 ||\n","epoch 10:(38/293) batch || training time for 2 batch 2.967155933380127 || training loss 0.0003890860534738749 ||\n","epoch 10:(40/293) batch || training time for 2 batch 2.931532859802246 || training loss 0.0003550723777152598 ||\n","epoch 10:(42/293) batch || training time for 2 batch 2.9418623447418213 || training loss 0.00041960342787206173 ||\n","epoch 10:(44/293) batch || training time for 2 batch 2.928717851638794 || training loss 0.00036138444556854665 ||\n","epoch 10:(46/293) batch || training time for 2 batch 2.963013172149658 || training loss 0.00036572337558027357 ||\n","epoch 10:(48/293) batch || training time for 2 batch 2.971425771713257 || training loss 0.0004221494309604168 ||\n","epoch 10:(50/293) batch || training time for 2 batch 2.9442129135131836 || training loss 0.00038893397140782326 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 75.98025679588318s\n","epoch 10:(52/293) batch || training time for 2 batch 82.31414556503296 || training loss 0.0010735646064858884 ||\n","epoch 10:(54/293) batch || training time for 2 batch 2.891103982925415 || training loss 0.00037811535003129393 ||\n","epoch 10:(56/293) batch || training time for 2 batch 2.9172441959381104 || training loss 0.0003685701231006533 ||\n","epoch 10:(58/293) batch || training time for 2 batch 2.913724660873413 || training loss 0.006921357722603716 ||\n","epoch 10:(60/293) batch || training time for 2 batch 2.941805362701416 || training loss 0.00036685512168332934 ||\n","epoch 10:(62/293) batch || training time for 2 batch 2.957146644592285 || training loss 0.0003611856373026967 ||\n","epoch 10:(64/293) batch || training time for 2 batch 2.963056802749634 || training loss 0.0008217865834012628 ||\n","epoch 10:(66/293) batch || training time for 2 batch 3.006286382675171 || training loss 0.00035793012648355216 ||\n","epoch 10:(68/293) batch || training time for 2 batch 3.0032336711883545 || training loss 0.00034666112333070487 ||\n","epoch 10:(70/293) batch || training time for 2 batch 3.002755641937256 || training loss 0.00035088528238702565 ||\n","epoch 10:(72/293) batch || training time for 2 batch 3.033684015274048 || training loss 0.00048741827777121216 ||\n","epoch 10:(74/293) batch || training time for 2 batch 3.0283432006835938 || training loss 0.00036132581590209156 ||\n","epoch 10:(76/293) batch || training time for 2 batch 3.0447945594787598 || training loss 0.00037506542867049575 ||\n","epoch 10:(78/293) batch || training time for 2 batch 3.002332925796509 || training loss 0.00037589838029816747 ||\n","epoch 10:(80/293) batch || training time for 2 batch 2.970085859298706 || training loss 0.00034063574275933206 ||\n","epoch 10:(82/293) batch || training time for 2 batch 2.9607200622558594 || training loss 0.00036205646756570786 ||\n","epoch 10:(84/293) batch || training time for 2 batch 2.959764003753662 || training loss 0.00035833452420774847 ||\n","epoch 10:(86/293) batch || training time for 2 batch 2.9279394149780273 || training loss 0.000380326178856194 ||\n","epoch 10:(88/293) batch || training time for 2 batch 2.9353909492492676 || training loss 0.0037283168494468555 ||\n","epoch 10:(90/293) batch || training time for 2 batch 2.906776189804077 || training loss 0.00035468146961648017 ||\n","epoch 10:(92/293) batch || training time for 2 batch 2.8944613933563232 || training loss 0.0005005606799386442 ||\n","epoch 10:(94/293) batch || training time for 2 batch 2.893141508102417 || training loss 0.00034087710082530975 ||\n","epoch 10:(96/293) batch || training time for 2 batch 2.9134464263916016 || training loss 0.0003713531041285023 ||\n","epoch 10:(98/293) batch || training time for 2 batch 2.9152212142944336 || training loss 0.0003612053842516616 ||\n","epoch 10:(100/293) batch || training time for 2 batch 2.9336602687835693 || training loss 0.0005209095543250442 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 63.906986713409424s\n","epoch 10:(102/293) batch || training time for 2 batch 70.22086906433105 || training loss 0.0049761903210310265 ||\n","epoch 10:(104/293) batch || training time for 2 batch 2.889803171157837 || training loss 0.0004080731305293739 ||\n","epoch 10:(106/293) batch || training time for 2 batch 2.9515388011932373 || training loss 0.0006764456629753113 ||\n","epoch 10:(108/293) batch || training time for 2 batch 2.9067108631134033 || training loss 0.0003608972911024466 ||\n","epoch 10:(110/293) batch || training time for 2 batch 2.9173779487609863 || training loss 0.006175918897497468 ||\n","epoch 10:(112/293) batch || training time for 2 batch 2.956747055053711 || training loss 0.00036307336995378137 ||\n","epoch 10:(114/293) batch || training time for 2 batch 2.9726595878601074 || training loss 0.0005118769040564075 ||\n","epoch 10:(116/293) batch || training time for 2 batch 2.9932644367218018 || training loss 0.0003304943966213614 ||\n","epoch 10:(118/293) batch || training time for 2 batch 3.024395227432251 || training loss 0.0003342994605191052 ||\n","epoch 10:(120/293) batch || training time for 2 batch 3.026746988296509 || training loss 0.00047231183270923793 ||\n","epoch 10:(122/293) batch || training time for 2 batch 2.997877836227417 || training loss 0.0003400365822017193 ||\n","epoch 10:(124/293) batch || training time for 2 batch 2.996535539627075 || training loss 0.00032931973692029715 ||\n","epoch 10:(126/293) batch || training time for 2 batch 3.0367064476013184 || training loss 0.0021589252282865345 ||\n","epoch 10:(128/293) batch || training time for 2 batch 2.999678373336792 || training loss 0.00033423074637539685 ||\n","epoch 10:(130/293) batch || training time for 2 batch 2.979987144470215 || training loss 0.0006016874976921827 ||\n","epoch 10:(132/293) batch || training time for 2 batch 2.949918508529663 || training loss 0.0003521203761920333 ||\n","epoch 10:(134/293) batch || training time for 2 batch 2.9492719173431396 || training loss 0.003618846894823946 ||\n","epoch 10:(136/293) batch || training time for 2 batch 2.9327070713043213 || training loss 0.0003819144912995398 ||\n","epoch 10:(138/293) batch || training time for 2 batch 2.915032386779785 || training loss 0.0003353277570568025 ||\n","epoch 10:(140/293) batch || training time for 2 batch 2.9186482429504395 || training loss 0.000361460741260089 ||\n","epoch 10:(142/293) batch || training time for 2 batch 2.9167182445526123 || training loss 0.00034061892074532807 ||\n","epoch 10:(144/293) batch || training time for 2 batch 2.9139890670776367 || training loss 0.00033102984889410436 ||\n","epoch 10:(146/293) batch || training time for 2 batch 2.9092113971710205 || training loss 0.0011433414765633643 ||\n","epoch 10:(148/293) batch || training time for 2 batch 2.871927261352539 || training loss 0.00034008991497103125 ||\n","epoch 10:(150/293) batch || training time for 2 batch 2.8942782878875732 || training loss 0.0003388822660781443 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 65.53380680084229s\n","epoch 10:(152/293) batch || training time for 2 batch 71.90341472625732 || training loss 0.00034438210423104465 ||\n","epoch 10:(154/293) batch || training time for 2 batch 2.886962890625 || training loss 0.00034129117557313293 ||\n","epoch 10:(156/293) batch || training time for 2 batch 2.913966178894043 || training loss 0.00034206881537102163 ||\n","epoch 10:(158/293) batch || training time for 2 batch 2.902862548828125 || training loss 0.0003464480978436768 ||\n","epoch 10:(160/293) batch || training time for 2 batch 2.936185359954834 || training loss 0.0013591813039965928 ||\n","epoch 10:(162/293) batch || training time for 2 batch 2.9681901931762695 || training loss 0.000631906499620527 ||\n","epoch 10:(164/293) batch || training time for 2 batch 2.9632556438446045 || training loss 0.00034201372181996703 ||\n","epoch 10:(166/293) batch || training time for 2 batch 2.967418670654297 || training loss 0.00040010109660215676 ||\n","epoch 10:(168/293) batch || training time for 2 batch 3.006287097930908 || training loss 0.0003189830167684704 ||\n","epoch 10:(170/293) batch || training time for 2 batch 2.98740291595459 || training loss 0.000344677857356146 ||\n","epoch 10:(172/293) batch || training time for 2 batch 3.018145799636841 || training loss 0.0003525257343426347 ||\n","epoch 10:(174/293) batch || training time for 2 batch 3.0306005477905273 || training loss 0.00033657941094134003 ||\n","epoch 10:(176/293) batch || training time for 2 batch 3.004896402359009 || training loss 0.0003137638559564948 ||\n","epoch 10:(178/293) batch || training time for 2 batch 3.051030397415161 || training loss 0.00045028245949652046 ||\n","epoch 10:(180/293) batch || training time for 2 batch 2.9722213745117188 || training loss 0.0003422704612603411 ||\n","epoch 10:(182/293) batch || training time for 2 batch 2.977381706237793 || training loss 0.0003817919350694865 ||\n","epoch 10:(184/293) batch || training time for 2 batch 2.943307876586914 || training loss 0.0004263666778570041 ||\n","epoch 10:(186/293) batch || training time for 2 batch 2.915975332260132 || training loss 0.00035664811730384827 ||\n","epoch 10:(188/293) batch || training time for 2 batch 2.948472738265991 || training loss 0.0003270288143539801 ||\n","epoch 10:(190/293) batch || training time for 2 batch 2.9137425422668457 || training loss 0.00035283018951304257 ||\n","epoch 10:(192/293) batch || training time for 2 batch 2.892498016357422 || training loss 0.000374768947949633 ||\n","epoch 10:(194/293) batch || training time for 2 batch 2.9137535095214844 || training loss 0.0003234653268009424 ||\n","epoch 10:(196/293) batch || training time for 2 batch 2.8925440311431885 || training loss 0.001400554261635989 ||\n","epoch 10:(198/293) batch || training time for 2 batch 2.9088075160980225 || training loss 0.0003065616765525192 ||\n","2.814749767106562e-06\n","epoch 10:(200/293) batch || training time for 2 batch 2.9131522178649902 || training loss 0.0003673380415420979 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 73.94130659103394s\n","epoch 10:(202/293) batch || training time for 2 batch 80.33651304244995 || training loss 0.000313592201564461 ||\n","epoch 10:(204/293) batch || training time for 2 batch 2.8854005336761475 || training loss 0.00031572319858241826 ||\n","epoch 10:(206/293) batch || training time for 2 batch 2.896120548248291 || training loss 0.0003314588393550366 ||\n","epoch 10:(208/293) batch || training time for 2 batch 2.899796485900879 || training loss 0.00032533405465073884 ||\n","epoch 10:(210/293) batch || training time for 2 batch 2.926790952682495 || training loss 0.000399784606997855 ||\n","epoch 10:(212/293) batch || training time for 2 batch 2.9172446727752686 || training loss 0.0016043112118495628 ||\n","epoch 10:(214/293) batch || training time for 2 batch 2.955986976623535 || training loss 0.0020433570607565343 ||\n","epoch 10:(216/293) batch || training time for 2 batch 2.9871938228607178 || training loss 0.0004160208918619901 ||\n","epoch 10:(218/293) batch || training time for 2 batch 2.984971284866333 || training loss 0.0003664616815513 ||\n","epoch 10:(220/293) batch || training time for 2 batch 2.9994773864746094 || training loss 0.00037900479219388217 ||\n","epoch 10:(222/293) batch || training time for 2 batch 3.025021553039551 || training loss 0.0003771618939936161 ||\n","epoch 10:(224/293) batch || training time for 2 batch 3.047680139541626 || training loss 0.0003244846302550286 ||\n","epoch 10:(226/293) batch || training time for 2 batch 3.023399829864502 || training loss 0.00032964872661978006 ||\n","epoch 10:(228/293) batch || training time for 2 batch 3.028646945953369 || training loss 0.0003358102694619447 ||\n","epoch 10:(230/293) batch || training time for 2 batch 3.0149786472320557 || training loss 0.0009352556080557406 ||\n","epoch 10:(232/293) batch || training time for 2 batch 2.9480156898498535 || training loss 0.00033937572152353823 ||\n","epoch 10:(234/293) batch || training time for 2 batch 2.9566490650177 || training loss 0.0005004639533581212 ||\n","epoch 10:(236/293) batch || training time for 2 batch 2.9445064067840576 || training loss 0.000357700846507214 ||\n","epoch 10:(238/293) batch || training time for 2 batch 2.9544930458068848 || training loss 0.00040879740845412016 ||\n","epoch 10:(240/293) batch || training time for 2 batch 2.9218990802764893 || training loss 0.0003404361195862293 ||\n","epoch 10:(242/293) batch || training time for 2 batch 2.9303300380706787 || training loss 0.0003699068329297006 ||\n","epoch 10:(244/293) batch || training time for 2 batch 2.9189627170562744 || training loss 0.00034979697375092655 ||\n","epoch 10:(246/293) batch || training time for 2 batch 2.8923420906066895 || training loss 0.0003255289630033076 ||\n","epoch 10:(248/293) batch || training time for 2 batch 2.895812511444092 || training loss 0.00032484400435350835 ||\n","epoch 10:(250/293) batch || training time for 2 batch 2.9195163249969482 || training loss 0.0004633868229575455 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 70.12217378616333s\n","epoch 10:(252/293) batch || training time for 2 batch 76.42195701599121 || training loss 0.00031652822508476675 ||\n","epoch 10:(254/293) batch || training time for 2 batch 2.894718647003174 || training loss 0.00048219479504041374 ||\n","epoch 10:(256/293) batch || training time for 2 batch 2.8868813514709473 || training loss 0.0016553076566196978 ||\n","epoch 10:(258/293) batch || training time for 2 batch 2.928056240081787 || training loss 0.00031153816962614655 ||\n","epoch 10:(260/293) batch || training time for 2 batch 2.9331743717193604 || training loss 0.0003253938048146665 ||\n","epoch 10:(262/293) batch || training time for 2 batch 2.9511401653289795 || training loss 0.00033578474540263414 ||\n","epoch 10:(264/293) batch || training time for 2 batch 2.9582388401031494 || training loss 0.0003538556338753551 ||\n","epoch 10:(266/293) batch || training time for 2 batch 2.9763472080230713 || training loss 0.0003083944320678711 ||\n","epoch 10:(268/293) batch || training time for 2 batch 3.0085792541503906 || training loss 0.000359791680239141 ||\n","epoch 10:(270/293) batch || training time for 2 batch 3.0347108840942383 || training loss 0.00036110296787228435 ||\n","epoch 10:(272/293) batch || training time for 2 batch 3.035890579223633 || training loss 0.0003379172121640295 ||\n","epoch 10:(274/293) batch || training time for 2 batch 3.0350120067596436 || training loss 0.00035180117993149906 ||\n","epoch 10:(276/293) batch || training time for 2 batch 3.0467097759246826 || training loss 0.0003703505208250135 ||\n","epoch 10:(278/293) batch || training time for 2 batch 3.027433156967163 || training loss 0.00028629295411519706 ||\n","epoch 10:(280/293) batch || training time for 2 batch 2.97729754447937 || training loss 0.0003746461879927665 ||\n","epoch 10:(282/293) batch || training time for 2 batch 2.950730800628662 || training loss 0.0004476250323932618 ||\n","epoch 10:(284/293) batch || training time for 2 batch 2.9644711017608643 || training loss 0.00042765308171510696 ||\n","epoch 10:(286/293) batch || training time for 2 batch 2.921825647354126 || training loss 0.00035415989987086505 ||\n","epoch 10:(288/293) batch || training time for 2 batch 2.9409079551696777 || training loss 0.00031573817250318825 ||\n","epoch 10:(290/293) batch || training time for 2 batch 2.9280526638031006 || training loss 0.000328221038216725 ||\n","epoch 10:(292/293) batch || training time for 2 batch 2.9131829738616943 || training loss 0.0003345712320879102 ||\n","epoch 11:(2/293) batch || training time for 2 batch 4.3592307567596436 || training loss 0.0005032069602748379 ||\n","epoch 11:(4/293) batch || training time for 2 batch 2.900923252105713 || training loss 0.00032640682184137404 ||\n","epoch 11:(6/293) batch || training time for 2 batch 2.916384220123291 || training loss 0.0003288065199740231 ||\n","epoch 11:(8/293) batch || training time for 2 batch 2.8861382007598877 || training loss 0.00032009510323405266 ||\n","epoch 11:(10/293) batch || training time for 2 batch 2.908292055130005 || training loss 0.000382307538529858 ||\n","epoch 11:(12/293) batch || training time for 2 batch 2.9163310527801514 || training loss 0.0003299623494967818 ||\n","epoch 11:(14/293) batch || training time for 2 batch 2.9212300777435303 || training loss 0.0003946519718738273 ||\n","epoch 11:(16/293) batch || training time for 2 batch 2.9048211574554443 || training loss 0.00032428673875983804 ||\n","epoch 11:(18/293) batch || training time for 2 batch 2.900437593460083 || training loss 0.00035190186463296413 ||\n","epoch 11:(20/293) batch || training time for 2 batch 2.9352962970733643 || training loss 0.00033398625964764506 ||\n","epoch 11:(22/293) batch || training time for 2 batch 2.9147963523864746 || training loss 0.0006572131242137402 ||\n","epoch 11:(24/293) batch || training time for 2 batch 2.9052698612213135 || training loss 0.00036068755434826016 ||\n","epoch 11:(26/293) batch || training time for 2 batch 2.9304966926574707 || training loss 0.000377214775653556 ||\n","epoch 11:(28/293) batch || training time for 2 batch 2.9398984909057617 || training loss 0.00034082451020367444 ||\n","epoch 11:(30/293) batch || training time for 2 batch 2.932431936264038 || training loss 0.0003161614004056901 ||\n","epoch 11:(32/293) batch || training time for 2 batch 2.9356777667999268 || training loss 0.0019734163070097566 ||\n","epoch 11:(34/293) batch || training time for 2 batch 2.936652898788452 || training loss 0.00030267867259681225 ||\n","epoch 11:(36/293) batch || training time for 2 batch 2.962379217147827 || training loss 0.0003290717431809753 ||\n","epoch 11:(38/293) batch || training time for 2 batch 2.947920560836792 || training loss 0.0002982659498229623 ||\n","epoch 11:(40/293) batch || training time for 2 batch 2.9499917030334473 || training loss 0.00031029005185700953 ||\n","epoch 11:(42/293) batch || training time for 2 batch 2.9610466957092285 || training loss 0.0003260298108216375 ||\n","epoch 11:(44/293) batch || training time for 2 batch 2.9563496112823486 || training loss 0.00032190240744967014 ||\n","epoch 11:(46/293) batch || training time for 2 batch 2.9467666149139404 || training loss 0.00034052078262902796 ||\n","epoch 11:(48/293) batch || training time for 2 batch 2.933074474334717 || training loss 0.0002959394041681662 ||\n","epoch 11:(50/293) batch || training time for 2 batch 2.9370203018188477 || training loss 0.00034366571344435215 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 76.0698492527008s\n","epoch 11:(52/293) batch || training time for 2 batch 82.37157225608826 || training loss 0.0004678170953411609 ||\n","epoch 11:(54/293) batch || training time for 2 batch 2.8942065238952637 || training loss 0.00029092580371070653 ||\n","epoch 11:(56/293) batch || training time for 2 batch 2.8995718955993652 || training loss 0.0003174648591084406 ||\n","epoch 11:(58/293) batch || training time for 2 batch 2.9065451622009277 || training loss 0.00032654374081175774 ||\n","epoch 11:(60/293) batch || training time for 2 batch 2.939458131790161 || training loss 0.0003292333130957559 ||\n","epoch 11:(62/293) batch || training time for 2 batch 2.959554433822632 || training loss 0.00032167104654945433 ||\n","epoch 11:(64/293) batch || training time for 2 batch 2.9898407459259033 || training loss 0.00028774089878425 ||\n","epoch 11:(66/293) batch || training time for 2 batch 3.0056638717651367 || training loss 0.00031337149266619235 ||\n","epoch 11:(68/293) batch || training time for 2 batch 3.0079410076141357 || training loss 0.0002909514878410846 ||\n","epoch 11:(70/293) batch || training time for 2 batch 3.005483388900757 || training loss 0.0003282102261437103 ||\n","epoch 11:(72/293) batch || training time for 2 batch 3.0495760440826416 || training loss 0.0004501900839386508 ||\n","epoch 11:(74/293) batch || training time for 2 batch 3.0464954376220703 || training loss 0.00037227639404591173 ||\n","epoch 11:(76/293) batch || training time for 2 batch 3.0180587768554688 || training loss 0.0003144571528537199 ||\n","epoch 11:(78/293) batch || training time for 2 batch 3.0073673725128174 || training loss 0.00028945814119651914 ||\n","epoch 11:(80/293) batch || training time for 2 batch 2.9952332973480225 || training loss 0.00031189977016765624 ||\n","epoch 11:(82/293) batch || training time for 2 batch 2.9594566822052 || training loss 0.0003357154200784862 ||\n","epoch 11:(84/293) batch || training time for 2 batch 2.944653034210205 || training loss 0.000284293171716854 ||\n","epoch 11:(86/293) batch || training time for 2 batch 2.90130615234375 || training loss 0.0024142784241121262 ||\n","epoch 11:(88/293) batch || training time for 2 batch 2.91019344329834 || training loss 0.00036445376463234425 ||\n","epoch 11:(90/293) batch || training time for 2 batch 2.8984148502349854 || training loss 0.0003171606658725068 ||\n","epoch 11:(92/293) batch || training time for 2 batch 2.9310145378112793 || training loss 0.0003025272017112002 ||\n","epoch 11:(94/293) batch || training time for 2 batch 2.9157984256744385 || training loss 0.00032818080217111856 ||\n","epoch 11:(96/293) batch || training time for 2 batch 2.886439800262451 || training loss 0.00029217274277471006 ||\n","epoch 11:(98/293) batch || training time for 2 batch 2.90513014793396 || training loss 0.00031504660728387535 ||\n","epoch 11:(100/293) batch || training time for 2 batch 2.9109556674957275 || training loss 0.0002986644976772368 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 71.15995264053345s\n","epoch 11:(102/293) batch || training time for 2 batch 77.50444149971008 || training loss 0.0003024773468496278 ||\n","epoch 11:(104/293) batch || training time for 2 batch 2.8820223808288574 || training loss 0.0003348689206177369 ||\n","epoch 11:(106/293) batch || training time for 2 batch 2.9200339317321777 || training loss 0.0003131145640509203 ||\n","epoch 11:(108/293) batch || training time for 2 batch 2.912360906600952 || training loss 0.0002855599013855681 ||\n","epoch 11:(110/293) batch || training time for 2 batch 2.916943311691284 || training loss 0.0003204564709449187 ||\n","epoch 11:(112/293) batch || training time for 2 batch 2.971097946166992 || training loss 0.00032084867416415364 ||\n","epoch 11:(114/293) batch || training time for 2 batch 2.958455801010132 || training loss 0.0006288878212217242 ||\n","epoch 11:(116/293) batch || training time for 2 batch 2.9851016998291016 || training loss 0.0020461789099499583 ||\n","epoch 11:(118/293) batch || training time for 2 batch 2.9967894554138184 || training loss 0.00030402625270653516 ||\n","epoch 11:(120/293) batch || training time for 2 batch 2.9843544960021973 || training loss 0.0003085885546170175 ||\n","epoch 11:(122/293) batch || training time for 2 batch 3.0472514629364014 || training loss 0.0003211309085600078 ||\n","epoch 11:(124/293) batch || training time for 2 batch 3.0499982833862305 || training loss 0.0003854079986922443 ||\n","epoch 11:(126/293) batch || training time for 2 batch 3.032601833343506 || training loss 0.00030773470643907785 ||\n","epoch 11:(128/293) batch || training time for 2 batch 3.0467417240142822 || training loss 0.0003877449780702591 ||\n","epoch 11:(130/293) batch || training time for 2 batch 3.0139052867889404 || training loss 0.00032474874751642346 ||\n","epoch 11:(132/293) batch || training time for 2 batch 2.994560718536377 || training loss 0.0003246448759455234 ||\n","epoch 11:(134/293) batch || training time for 2 batch 2.9339559078216553 || training loss 0.0005082248972030357 ||\n","epoch 11:(136/293) batch || training time for 2 batch 2.924927234649658 || training loss 0.0003565356746548787 ||\n","epoch 11:(138/293) batch || training time for 2 batch 2.930832862854004 || training loss 0.0002880944521166384 ||\n","epoch 11:(140/293) batch || training time for 2 batch 2.9148926734924316 || training loss 0.0003167167888022959 ||\n","epoch 11:(142/293) batch || training time for 2 batch 2.8812456130981445 || training loss 0.00032133022614289075 ||\n","epoch 11:(144/293) batch || training time for 2 batch 2.9086759090423584 || training loss 0.0005874186317669228 ||\n","epoch 11:(146/293) batch || training time for 2 batch 2.8829665184020996 || training loss 0.00028639158699661493 ||\n","epoch 11:(148/293) batch || training time for 2 batch 2.8938090801239014 || training loss 0.00032196028041653335 ||\n","epoch 11:(150/293) batch || training time for 2 batch 2.9176299571990967 || training loss 0.0004570783639792353 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 73.86532759666443s\n","epoch 11:(152/293) batch || training time for 2 batch 80.1258955001831 || training loss 0.00028997540357522666 ||\n","epoch 11:(154/293) batch || training time for 2 batch 2.884384870529175 || training loss 0.00027630641125142574 ||\n","epoch 11:(156/293) batch || training time for 2 batch 2.88685941696167 || training loss 0.00029226671904325485 ||\n","epoch 11:(158/293) batch || training time for 2 batch 2.915632486343384 || training loss 0.00030519976280629635 ||\n","epoch 11:(160/293) batch || training time for 2 batch 2.928475856781006 || training loss 0.0003109017270617187 ||\n","epoch 11:(162/293) batch || training time for 2 batch 2.930056095123291 || training loss 0.0005742728244513273 ||\n","epoch 11:(164/293) batch || training time for 2 batch 2.9385228157043457 || training loss 0.00030634184076916426 ||\n","epoch 11:(166/293) batch || training time for 2 batch 3.009160041809082 || training loss 0.00029062929388601333 ||\n","epoch 11:(168/293) batch || training time for 2 batch 2.9789509773254395 || training loss 0.0003250996378483251 ||\n","epoch 11:(170/293) batch || training time for 2 batch 3.006291627883911 || training loss 0.0003091310791205615 ||\n","epoch 11:(172/293) batch || training time for 2 batch 2.9889373779296875 || training loss 0.00035231231595389545 ||\n","epoch 11:(174/293) batch || training time for 2 batch 3.0236403942108154 || training loss 0.0002811885642586276 ||\n","epoch 11:(176/293) batch || training time for 2 batch 2.9801924228668213 || training loss 0.0004389816604088992 ||\n","epoch 11:(178/293) batch || training time for 2 batch 3.0304501056671143 || training loss 0.00034314696677029133 ||\n","epoch 11:(180/293) batch || training time for 2 batch 2.9937779903411865 || training loss 0.0002991124492837116 ||\n","epoch 11:(182/293) batch || training time for 2 batch 2.976821184158325 || training loss 0.0003110892866970971 ||\n","epoch 11:(184/293) batch || training time for 2 batch 2.9476022720336914 || training loss 0.00028511747950688004 ||\n","epoch 11:(186/293) batch || training time for 2 batch 2.960237503051758 || training loss 0.00033835184876807034 ||\n","epoch 11:(188/293) batch || training time for 2 batch 2.908276081085205 || training loss 0.000292752287350595 ||\n","epoch 11:(190/293) batch || training time for 2 batch 2.9333279132843018 || training loss 0.0002932646166300401 ||\n","epoch 11:(192/293) batch || training time for 2 batch 2.9162464141845703 || training loss 0.00032517194631509483 ||\n","epoch 11:(194/293) batch || training time for 2 batch 2.9022281169891357 || training loss 0.0003069949016207829 ||\n","epoch 11:(196/293) batch || training time for 2 batch 2.913703680038452 || training loss 0.00032076243951451033 ||\n","epoch 11:(198/293) batch || training time for 2 batch 2.898681163787842 || training loss 0.0024928126949816942 ||\n","2.2517998136852495e-06\n","epoch 11:(200/293) batch || training time for 2 batch 2.904794454574585 || training loss 0.0002826339041348547 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 69.04881358146667s\n","epoch 11:(202/293) batch || training time for 2 batch 75.38858556747437 || training loss 0.0003233400348108262 ||\n","epoch 11:(204/293) batch || training time for 2 batch 2.8811540603637695 || training loss 0.0002903276472352445 ||\n","epoch 11:(206/293) batch || training time for 2 batch 2.865541696548462 || training loss 0.04608255115454085 ||\n","epoch 11:(208/293) batch || training time for 2 batch 2.9111838340759277 || training loss 0.0002969788038171828 ||\n","epoch 11:(210/293) batch || training time for 2 batch 2.892848253250122 || training loss 0.0003280915261711925 ||\n","epoch 11:(212/293) batch || training time for 2 batch 2.9547390937805176 || training loss 0.0003463721659500152 ||\n","epoch 11:(214/293) batch || training time for 2 batch 2.976591110229492 || training loss 0.0002956063544843346 ||\n","epoch 11:(216/293) batch || training time for 2 batch 2.9725894927978516 || training loss 0.0002900924300774932 ||\n","epoch 11:(218/293) batch || training time for 2 batch 3.0141985416412354 || training loss 0.0003173862351104617 ||\n","epoch 11:(220/293) batch || training time for 2 batch 3.003635883331299 || training loss 0.00030145882919896394 ||\n","epoch 11:(222/293) batch || training time for 2 batch 3.035987138748169 || training loss 0.0006995668809395283 ||\n","epoch 11:(224/293) batch || training time for 2 batch 3.010793447494507 || training loss 0.00030524983594659716 ||\n","epoch 11:(226/293) batch || training time for 2 batch 3.0464491844177246 || training loss 0.00030553997203242034 ||\n","epoch 11:(228/293) batch || training time for 2 batch 2.990588426589966 || training loss 0.00045196281280368567 ||\n","epoch 11:(230/293) batch || training time for 2 batch 2.9466552734375 || training loss 0.00031013171246740967 ||\n","epoch 11:(232/293) batch || training time for 2 batch 2.956922769546509 || training loss 0.0002993239613715559 ||\n","epoch 11:(234/293) batch || training time for 2 batch 2.950164318084717 || training loss 0.0003165050147799775 ||\n","epoch 11:(236/293) batch || training time for 2 batch 2.9135513305664062 || training loss 0.0003110395045951009 ||\n","epoch 11:(238/293) batch || training time for 2 batch 2.9203529357910156 || training loss 0.0002801047812681645 ||\n","epoch 11:(240/293) batch || training time for 2 batch 2.9321043491363525 || training loss 0.0003029876243090257 ||\n","epoch 11:(242/293) batch || training time for 2 batch 2.9177584648132324 || training loss 0.000272874123766087 ||\n","epoch 11:(244/293) batch || training time for 2 batch 2.9084789752960205 || training loss 0.00029018089117016643 ||\n","epoch 11:(246/293) batch || training time for 2 batch 2.894010066986084 || training loss 0.0003059791342820972 ||\n","epoch 11:(248/293) batch || training time for 2 batch 2.8948328495025635 || training loss 0.0002784161770250648 ||\n","epoch 11:(250/293) batch || training time for 2 batch 2.9069151878356934 || training loss 0.00029257712594699115 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 68.89232778549194s\n","epoch 11:(252/293) batch || training time for 2 batch 75.21924352645874 || training loss 0.0002641509781824425 ||\n","epoch 11:(254/293) batch || training time for 2 batch 2.8708958625793457 || training loss 0.00027472031069919467 ||\n","epoch 11:(256/293) batch || training time for 2 batch 2.9107046127319336 || training loss 0.0002742924989433959 ||\n","epoch 11:(258/293) batch || training time for 2 batch 2.9241487979888916 || training loss 0.0002716304734349251 ||\n","epoch 11:(260/293) batch || training time for 2 batch 2.94626522064209 || training loss 0.00035903058596886694 ||\n","epoch 11:(262/293) batch || training time for 2 batch 2.95424222946167 || training loss 0.0014616438711527735 ||\n","epoch 11:(264/293) batch || training time for 2 batch 2.978773832321167 || training loss 0.0003047696372959763 ||\n","epoch 11:(266/293) batch || training time for 2 batch 2.970224142074585 || training loss 0.00026840678765438497 ||\n","epoch 11:(268/293) batch || training time for 2 batch 2.9611423015594482 || training loss 0.0034057428274536505 ||\n","epoch 11:(270/293) batch || training time for 2 batch 2.9965105056762695 || training loss 0.004723857418866828 ||\n","epoch 11:(272/293) batch || training time for 2 batch 3.02933931350708 || training loss 0.000293886725557968 ||\n","epoch 11:(274/293) batch || training time for 2 batch 3.06205415725708 || training loss 0.0007162504334701225 ||\n","epoch 11:(276/293) batch || training time for 2 batch 3.0035018920898438 || training loss 0.0002734311274252832 ||\n","epoch 11:(278/293) batch || training time for 2 batch 3.0334413051605225 || training loss 0.0003267868305556476 ||\n","epoch 11:(280/293) batch || training time for 2 batch 2.979247570037842 || training loss 0.019126852974295616 ||\n","epoch 11:(282/293) batch || training time for 2 batch 2.972277879714966 || training loss 0.0002831445017363876 ||\n","epoch 11:(284/293) batch || training time for 2 batch 2.9354727268218994 || training loss 0.00033256839378736913 ||\n","epoch 11:(286/293) batch || training time for 2 batch 2.935472249984741 || training loss 0.0003035395056940615 ||\n","epoch 11:(288/293) batch || training time for 2 batch 2.9370503425598145 || training loss 0.001253544061910361 ||\n","epoch 11:(290/293) batch || training time for 2 batch 2.9238176345825195 || training loss 0.0002926721645053476 ||\n","epoch 11:(292/293) batch || training time for 2 batch 2.919675827026367 || training loss 0.0010928148694802076 ||\n","epoch 12:(2/293) batch || training time for 2 batch 4.386918544769287 || training loss 0.0004961362283211201 ||\n","epoch 12:(4/293) batch || training time for 2 batch 2.901527166366577 || training loss 0.00027907348703593016 ||\n","epoch 12:(6/293) batch || training time for 2 batch 2.9168529510498047 || training loss 0.0004153401532676071 ||\n","epoch 12:(8/293) batch || training time for 2 batch 2.896775007247925 || training loss 0.0003234640462324023 ||\n","epoch 12:(10/293) batch || training time for 2 batch 2.896261215209961 || training loss 0.0003122603229712695 ||\n","epoch 12:(12/293) batch || training time for 2 batch 2.8944714069366455 || training loss 0.00027620032778941095 ||\n","epoch 12:(14/293) batch || training time for 2 batch 2.8942644596099854 || training loss 0.00029933264886494726 ||\n","epoch 12:(16/293) batch || training time for 2 batch 2.9285082817077637 || training loss 0.00027876891545020044 ||\n","epoch 12:(18/293) batch || training time for 2 batch 2.898655652999878 || training loss 0.0003658831992652267 ||\n","epoch 12:(20/293) batch || training time for 2 batch 2.9323034286499023 || training loss 0.0003006340120919049 ||\n","epoch 12:(22/293) batch || training time for 2 batch 2.966710090637207 || training loss 0.0004431242705322802 ||\n","epoch 12:(24/293) batch || training time for 2 batch 2.936952590942383 || training loss 0.00031973066506907344 ||\n","epoch 12:(26/293) batch || training time for 2 batch 2.951997995376587 || training loss 0.00029761664336547256 ||\n","epoch 12:(28/293) batch || training time for 2 batch 2.9360029697418213 || training loss 0.00027999794110655785 ||\n","epoch 12:(30/293) batch || training time for 2 batch 2.931100606918335 || training loss 0.0032858020858839154 ||\n","epoch 12:(32/293) batch || training time for 2 batch 2.979255199432373 || training loss 0.0002725641388678923 ||\n","epoch 12:(34/293) batch || training time for 2 batch 2.960089683532715 || training loss 0.0003074883425142616 ||\n","epoch 12:(36/293) batch || training time for 2 batch 2.9355874061584473 || training loss 0.00037107948446646333 ||\n","epoch 12:(38/293) batch || training time for 2 batch 2.941446304321289 || training loss 0.0002839307126123458 ||\n","epoch 12:(40/293) batch || training time for 2 batch 2.9769465923309326 || training loss 0.0002914198412327096 ||\n","epoch 12:(42/293) batch || training time for 2 batch 2.933022975921631 || training loss 0.0002922141138697043 ||\n","epoch 12:(44/293) batch || training time for 2 batch 2.9615426063537598 || training loss 0.0003045479825232178 ||\n","epoch 12:(46/293) batch || training time for 2 batch 2.9705865383148193 || training loss 0.0002679687167983502 ||\n","epoch 12:(48/293) batch || training time for 2 batch 2.930840015411377 || training loss 0.0013287867477629334 ||\n","epoch 12:(50/293) batch || training time for 2 batch 2.9445924758911133 || training loss 0.00027509647770784795 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 64.56572079658508s\n","epoch 12:(52/293) batch || training time for 2 batch 70.93372392654419 || training loss 0.00041315407725051045 ||\n","epoch 12:(54/293) batch || training time for 2 batch 2.8705403804779053 || training loss 0.00029990790062583983 ||\n","epoch 12:(56/293) batch || training time for 2 batch 2.9227776527404785 || training loss 0.0002839056251104921 ||\n","epoch 12:(58/293) batch || training time for 2 batch 2.922833204269409 || training loss 0.0003033928369404748 ||\n","epoch 12:(60/293) batch || training time for 2 batch 2.9079151153564453 || training loss 0.00028404365002643317 ||\n","epoch 12:(62/293) batch || training time for 2 batch 2.9512810707092285 || training loss 0.00042903245775960386 ||\n","epoch 12:(64/293) batch || training time for 2 batch 2.968165874481201 || training loss 0.0004067761037731543 ||\n","epoch 12:(66/293) batch || training time for 2 batch 2.973262071609497 || training loss 0.00031950762786436826 ||\n","epoch 12:(68/293) batch || training time for 2 batch 2.9945738315582275 || training loss 0.00028014116105623543 ||\n","epoch 12:(70/293) batch || training time for 2 batch 3.0235579013824463 || training loss 0.00025832379469648004 ||\n","epoch 12:(72/293) batch || training time for 2 batch 3.0638301372528076 || training loss 0.0004398910969030112 ||\n","epoch 12:(74/293) batch || training time for 2 batch 3.0427188873291016 || training loss 0.00030792830511927605 ||\n","epoch 12:(76/293) batch || training time for 2 batch 3.0266001224517822 || training loss 0.0002927255700342357 ||\n","epoch 12:(78/293) batch || training time for 2 batch 3.05303692817688 || training loss 0.0018897693080361933 ||\n","epoch 12:(80/293) batch || training time for 2 batch 2.9802141189575195 || training loss 0.0003035947447642684 ||\n","epoch 12:(82/293) batch || training time for 2 batch 2.970141887664795 || training loss 0.0005488585156854242 ||\n","epoch 12:(84/293) batch || training time for 2 batch 2.9476022720336914 || training loss 0.0002462936317897402 ||\n","epoch 12:(86/293) batch || training time for 2 batch 2.9442059993743896 || training loss 0.0004178397066425532 ||\n","epoch 12:(88/293) batch || training time for 2 batch 2.922805070877075 || training loss 0.0003197199839632958 ||\n","epoch 12:(90/293) batch || training time for 2 batch 2.9099583625793457 || training loss 0.00027925727772526443 ||\n","epoch 12:(92/293) batch || training time for 2 batch 2.9300568103790283 || training loss 0.0017683749611023813 ||\n","epoch 12:(94/293) batch || training time for 2 batch 2.896055221557617 || training loss 0.0003071711107622832 ||\n","epoch 12:(96/293) batch || training time for 2 batch 2.9051573276519775 || training loss 0.0002655578718986362 ||\n","epoch 12:(98/293) batch || training time for 2 batch 2.89857816696167 || training loss 0.00029086312861181796 ||\n","epoch 12:(100/293) batch || training time for 2 batch 2.8983006477355957 || training loss 0.00025603150425013155 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 64.51440072059631s\n","epoch 12:(102/293) batch || training time for 2 batch 70.84838795661926 || training loss 0.00030515056278090924 ||\n","epoch 12:(104/293) batch || training time for 2 batch 2.881608247756958 || training loss 0.005386665259720758 ||\n","epoch 12:(106/293) batch || training time for 2 batch 2.883408308029175 || training loss 0.0003219792852178216 ||\n","epoch 12:(108/293) batch || training time for 2 batch 2.903240203857422 || training loss 0.0002599150611786172 ||\n","epoch 12:(110/293) batch || training time for 2 batch 2.9177961349487305 || training loss 0.00030617156880907714 ||\n","epoch 12:(112/293) batch || training time for 2 batch 2.9535489082336426 || training loss 0.00041604856960475445 ||\n","epoch 12:(114/293) batch || training time for 2 batch 2.9418344497680664 || training loss 0.0002917997189797461 ||\n","epoch 12:(116/293) batch || training time for 2 batch 2.948542833328247 || training loss 0.00026082986732944846 ||\n","epoch 12:(118/293) batch || training time for 2 batch 2.9827160835266113 || training loss 0.0002689572866074741 ||\n","epoch 12:(120/293) batch || training time for 2 batch 2.987302303314209 || training loss 0.00034323320141993463 ||\n","epoch 12:(122/293) batch || training time for 2 batch 3.0044357776641846 || training loss 0.0002655485295690596 ||\n","epoch 12:(124/293) batch || training time for 2 batch 3.0382208824157715 || training loss 0.0003484196786303073 ||\n","epoch 12:(126/293) batch || training time for 2 batch 3.013545036315918 || training loss 0.000297090140520595 ||\n","epoch 12:(128/293) batch || training time for 2 batch 3.0109846591949463 || training loss 0.0002549853816162795 ||\n","epoch 12:(130/293) batch || training time for 2 batch 2.978886365890503 || training loss 0.0019466123194433749 ||\n","epoch 12:(132/293) batch || training time for 2 batch 2.9723968505859375 || training loss 0.00026610272470861673 ||\n","epoch 12:(134/293) batch || training time for 2 batch 2.9394595623016357 || training loss 0.0002786486002150923 ||\n","epoch 12:(136/293) batch || training time for 2 batch 2.9472689628601074 || training loss 0.00026299062301404774 ||\n","epoch 12:(138/293) batch || training time for 2 batch 2.9358928203582764 || training loss 0.0002743775839917362 ||\n","epoch 12:(140/293) batch || training time for 2 batch 2.9224913120269775 || training loss 0.00028948322869837284 ||\n","epoch 12:(142/293) batch || training time for 2 batch 2.9009106159210205 || training loss 0.00026489577430766076 ||\n","epoch 12:(144/293) batch || training time for 2 batch 2.91188907623291 || training loss 0.00026472200988791883 ||\n","epoch 12:(146/293) batch || training time for 2 batch 2.8795886039733887 || training loss 0.0002936112869065255 ||\n","epoch 12:(148/293) batch || training time for 2 batch 2.9147043228149414 || training loss 0.0002707713283598423 ||\n","epoch 12:(150/293) batch || training time for 2 batch 2.895334005355835 || training loss 0.0002581787048256956 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 66.90606379508972s\n","epoch 12:(152/293) batch || training time for 2 batch 73.28932785987854 || training loss 0.00035685305192600936 ||\n","epoch 12:(154/293) batch || training time for 2 batch 2.865020275115967 || training loss 0.0002619929291540757 ||\n","epoch 12:(156/293) batch || training time for 2 batch 2.9003772735595703 || training loss 0.00034100744233001024 ||\n","epoch 12:(158/293) batch || training time for 2 batch 2.8951001167297363 || training loss 0.00027070030046161264 ||\n","epoch 12:(160/293) batch || training time for 2 batch 2.942797899246216 || training loss 0.0003469044459052384 ||\n","epoch 12:(162/293) batch || training time for 2 batch 2.9209446907043457 || training loss 0.0002699958422454074 ||\n","epoch 12:(164/293) batch || training time for 2 batch 2.948045492172241 || training loss 0.00028187753923702985 ||\n","epoch 12:(166/293) batch || training time for 2 batch 3.000265121459961 || training loss 0.00025861055473797023 ||\n","epoch 12:(168/293) batch || training time for 2 batch 2.9700636863708496 || training loss 0.0002620777377160266 ||\n","epoch 12:(170/293) batch || training time for 2 batch 3.031789541244507 || training loss 0.0003140120970783755 ||\n","epoch 12:(172/293) batch || training time for 2 batch 3.039214849472046 || training loss 0.00036203049239702523 ||\n","epoch 12:(174/293) batch || training time for 2 batch 3.011648178100586 || training loss 0.00025817380810622126 ||\n","epoch 12:(176/293) batch || training time for 2 batch 3.0466413497924805 || training loss 0.0003038367140106857 ||\n","epoch 12:(178/293) batch || training time for 2 batch 3.005795478820801 || training loss 0.00030408560996875167 ||\n","epoch 12:(180/293) batch || training time for 2 batch 3.0053935050964355 || training loss 0.00028430623933672905 ||\n","epoch 12:(182/293) batch || training time for 2 batch 2.986431360244751 || training loss 0.00029476471536327153 ||\n","epoch 12:(184/293) batch || training time for 2 batch 2.939311981201172 || training loss 0.0002895600482588634 ||\n","epoch 12:(186/293) batch || training time for 2 batch 2.9265692234039307 || training loss 0.0003366844612173736 ||\n","epoch 12:(188/293) batch || training time for 2 batch 2.939791440963745 || training loss 0.00026354737929068506 ||\n","epoch 12:(190/293) batch || training time for 2 batch 2.921180248260498 || training loss 0.00027563932235352695 ||\n","epoch 12:(192/293) batch || training time for 2 batch 2.9172964096069336 || training loss 0.00026555638760328293 ||\n","epoch 12:(194/293) batch || training time for 2 batch 2.895397901535034 || training loss 0.0002628195288707502 ||\n","epoch 12:(196/293) batch || training time for 2 batch 2.931105613708496 || training loss 0.0003086335491389036 ||\n","epoch 12:(198/293) batch || training time for 2 batch 2.9235591888427734 || training loss 0.00029283048934303224 ||\n","1.8014398509481998e-06\n","epoch 12:(200/293) batch || training time for 2 batch 2.8907337188720703 || training loss 0.0002811252779792994 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 78.05039525032043s\n","epoch 12:(202/293) batch || training time for 2 batch 84.35958909988403 || training loss 0.0002945265150628984 ||\n","epoch 12:(204/293) batch || training time for 2 batch 2.8619306087493896 || training loss 0.00027796081849373877 ||\n","epoch 12:(206/293) batch || training time for 2 batch 2.910830020904541 || training loss 0.00029185444873292 ||\n","epoch 12:(208/293) batch || training time for 2 batch 2.933403253555298 || training loss 0.00044043955858796835 ||\n","epoch 12:(210/293) batch || training time for 2 batch 2.9303207397460938 || training loss 0.0002778812631731853 ||\n","epoch 12:(212/293) batch || training time for 2 batch 2.9571945667266846 || training loss 0.00028367905179038644 ||\n","epoch 12:(214/293) batch || training time for 2 batch 2.9774692058563232 || training loss 0.00027924467576667666 ||\n","epoch 12:(216/293) batch || training time for 2 batch 2.988509178161621 || training loss 0.00029310581157915294 ||\n","epoch 12:(218/293) batch || training time for 2 batch 3.0165722370147705 || training loss 0.0002865969290724024 ||\n","epoch 12:(220/293) batch || training time for 2 batch 3.0262773036956787 || training loss 0.0002900910913012922 ||\n","epoch 12:(222/293) batch || training time for 2 batch 3.0247867107391357 || training loss 0.0002690642577363178 ||\n","epoch 12:(224/293) batch || training time for 2 batch 3.0382471084594727 || training loss 0.0002810108271660283 ||\n","epoch 12:(226/293) batch || training time for 2 batch 3.057297945022583 || training loss 0.0002904391003539786 ||\n","epoch 12:(228/293) batch || training time for 2 batch 3.0176784992218018 || training loss 0.004575131373712793 ||\n","epoch 12:(230/293) batch || training time for 2 batch 2.981255531311035 || training loss 0.00026258396974299103 ||\n","epoch 12:(232/293) batch || training time for 2 batch 2.9863390922546387 || training loss 0.0003014683024957776 ||\n","epoch 12:(234/293) batch || training time for 2 batch 2.9464471340179443 || training loss 0.00026866814005188644 ||\n","epoch 12:(236/293) batch || training time for 2 batch 2.932201623916626 || training loss 0.00029611650097649544 ||\n","epoch 12:(238/293) batch || training time for 2 batch 2.923035144805908 || training loss 0.0010955188772641122 ||\n","epoch 12:(240/293) batch || training time for 2 batch 2.914098024368286 || training loss 0.00025623941473895684 ||\n","epoch 12:(242/293) batch || training time for 2 batch 2.9487500190734863 || training loss 0.00039062733412720263 ||\n","epoch 12:(244/293) batch || training time for 2 batch 2.911769151687622 || training loss 0.003888450446538627 ||\n","epoch 12:(246/293) batch || training time for 2 batch 2.906447410583496 || training loss 0.0002782343653962016 ||\n","epoch 12:(248/293) batch || training time for 2 batch 2.9182701110839844 || training loss 0.0002937222016043961 ||\n","epoch 12:(250/293) batch || training time for 2 batch 2.900722026824951 || training loss 0.0003016056725755334 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 65.85192894935608s\n","epoch 12:(252/293) batch || training time for 2 batch 72.19649028778076 || training loss 0.00031075807055458426 ||\n","epoch 12:(254/293) batch || training time for 2 batch 2.8741536140441895 || training loss 0.0002675282012205571 ||\n","epoch 12:(256/293) batch || training time for 2 batch 2.887296676635742 || training loss 0.00026089901803061366 ||\n","epoch 12:(258/293) batch || training time for 2 batch 2.912605047225952 || training loss 0.00026452892052475363 ||\n","epoch 12:(260/293) batch || training time for 2 batch 2.9444799423217773 || training loss 0.0003031896339962259 ||\n","epoch 12:(262/293) batch || training time for 2 batch 2.9554665088653564 || training loss 0.00026496819918975234 ||\n","epoch 12:(264/293) batch || training time for 2 batch 2.9147844314575195 || training loss 0.00028473991551436484 ||\n","epoch 12:(266/293) batch || training time for 2 batch 2.9852285385131836 || training loss 0.000313719516270794 ||\n","epoch 12:(268/293) batch || training time for 2 batch 3.022763967514038 || training loss 0.0004089745634701103 ||\n","epoch 12:(270/293) batch || training time for 2 batch 2.9993321895599365 || training loss 0.0002816865162458271 ||\n","epoch 12:(272/293) batch || training time for 2 batch 3.015385389328003 || training loss 0.00028735719388350844 ||\n","epoch 12:(274/293) batch || training time for 2 batch 3.023648500442505 || training loss 0.0002805831754812971 ||\n","epoch 12:(276/293) batch || training time for 2 batch 3.005732297897339 || training loss 0.0003157361061312258 ||\n","epoch 12:(278/293) batch || training time for 2 batch 3.01184344291687 || training loss 0.0002609531002235599 ||\n","epoch 12:(280/293) batch || training time for 2 batch 3.0077011585235596 || training loss 0.00029128608002793044 ||\n","epoch 12:(282/293) batch || training time for 2 batch 2.9662258625030518 || training loss 0.00039664183714194223 ||\n","epoch 12:(284/293) batch || training time for 2 batch 2.9458327293395996 || training loss 0.007544152700575069 ||\n","epoch 12:(286/293) batch || training time for 2 batch 2.931915283203125 || training loss 0.00028926480445079505 ||\n","epoch 12:(288/293) batch || training time for 2 batch 2.947068452835083 || training loss 0.00026408021221868694 ||\n","epoch 12:(290/293) batch || training time for 2 batch 2.9047746658325195 || training loss 0.00025162467500194907 ||\n","epoch 12:(292/293) batch || training time for 2 batch 2.9318203926086426 || training loss 0.021908190203248523 ||\n","epoch 13:(2/293) batch || training time for 2 batch 4.432695627212524 || training loss 0.005984741757856682 ||\n","epoch 13:(4/293) batch || training time for 2 batch 2.9148201942443848 || training loss 0.0003083410847466439 ||\n","epoch 13:(6/293) batch || training time for 2 batch 2.917417049407959 || training loss 0.00029118856764398515 ||\n","epoch 13:(8/293) batch || training time for 2 batch 2.920337677001953 || training loss 0.0006017712585162371 ||\n","epoch 13:(10/293) batch || training time for 2 batch 2.923527240753174 || training loss 0.00028529409610200673 ||\n","epoch 13:(12/293) batch || training time for 2 batch 2.8818087577819824 || training loss 0.00027286527620162815 ||\n","epoch 13:(14/293) batch || training time for 2 batch 2.8968517780303955 || training loss 0.00027990288799628615 ||\n","epoch 13:(16/293) batch || training time for 2 batch 2.907615900039673 || training loss 0.00026163777511101216 ||\n","epoch 13:(18/293) batch || training time for 2 batch 2.9388904571533203 || training loss 0.00027087415219284594 ||\n","epoch 13:(20/293) batch || training time for 2 batch 2.9432880878448486 || training loss 0.00026819681806955487 ||\n","epoch 13:(22/293) batch || training time for 2 batch 2.906744956970215 || training loss 0.0003006911283591762 ||\n","epoch 13:(24/293) batch || training time for 2 batch 2.9300522804260254 || training loss 0.00023974350187927485 ||\n","epoch 13:(26/293) batch || training time for 2 batch 2.9758031368255615 || training loss 0.00028743408620357513 ||\n","epoch 13:(28/293) batch || training time for 2 batch 2.9364864826202393 || training loss 0.000290265103103593 ||\n","epoch 13:(30/293) batch || training time for 2 batch 2.9525768756866455 || training loss 0.00029819068731740117 ||\n","epoch 13:(32/293) batch || training time for 2 batch 2.9657933712005615 || training loss 0.00026686425553634763 ||\n","epoch 13:(34/293) batch || training time for 2 batch 2.953491687774658 || training loss 0.00031670552561990917 ||\n","epoch 13:(36/293) batch || training time for 2 batch 2.962578535079956 || training loss 0.00032227151677943766 ||\n","epoch 13:(38/293) batch || training time for 2 batch 2.961298704147339 || training loss 0.0002709649706957862 ||\n","epoch 13:(40/293) batch || training time for 2 batch 2.9372050762176514 || training loss 0.00027488748310133815 ||\n","epoch 13:(42/293) batch || training time for 2 batch 2.9558141231536865 || training loss 0.000258768217463512 ||\n","epoch 13:(44/293) batch || training time for 2 batch 2.95096755027771 || training loss 0.00028897928132209927 ||\n","epoch 13:(46/293) batch || training time for 2 batch 2.9301788806915283 || training loss 0.00027774028421845287 ||\n","epoch 13:(48/293) batch || training time for 2 batch 2.937347650527954 || training loss 0.001774509932147339 ||\n","epoch 13:(50/293) batch || training time for 2 batch 2.9733710289001465 || training loss 0.00028088013641536236 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 73.53835725784302s\n","epoch 13:(52/293) batch || training time for 2 batch 79.93715620040894 || training loss 0.00027890918136108667 ||\n","epoch 13:(54/293) batch || training time for 2 batch 2.8754377365112305 || training loss 0.0002548930642660707 ||\n","epoch 13:(56/293) batch || training time for 2 batch 2.9166433811187744 || training loss 0.0002483105636201799 ||\n","epoch 13:(58/293) batch || training time for 2 batch 2.907616138458252 || training loss 0.00024195373407565057 ||\n","epoch 13:(60/293) batch || training time for 2 batch 2.952970504760742 || training loss 0.0002809269935823977 ||\n","epoch 13:(62/293) batch || training time for 2 batch 2.954462766647339 || training loss 0.00027362952823750675 ||\n","epoch 13:(64/293) batch || training time for 2 batch 2.976372241973877 || training loss 0.00029065094713587314 ||\n","epoch 13:(66/293) batch || training time for 2 batch 2.9722466468811035 || training loss 0.0013804026530124247 ||\n","epoch 13:(68/293) batch || training time for 2 batch 2.9850873947143555 || training loss 0.0002889081952162087 ||\n","epoch 13:(70/293) batch || training time for 2 batch 3.0215179920196533 || training loss 0.0002585313777672127 ||\n","epoch 13:(72/293) batch || training time for 2 batch 3.0214056968688965 || training loss 0.00460073868453037 ||\n","epoch 13:(74/293) batch || training time for 2 batch 3.0239944458007812 || training loss 0.00024341292009921744 ||\n","epoch 13:(76/293) batch || training time for 2 batch 3.0078818798065186 || training loss 0.0013999086513649672 ||\n","epoch 13:(78/293) batch || training time for 2 batch 2.997232675552368 || training loss 0.0002791001461446285 ||\n","epoch 13:(80/293) batch || training time for 2 batch 2.998661994934082 || training loss 0.0003103870549239218 ||\n","epoch 13:(82/293) batch || training time for 2 batch 2.9424219131469727 || training loss 0.00024808432499412447 ||\n","epoch 13:(84/293) batch || training time for 2 batch 2.934563636779785 || training loss 0.0002609689545352012 ||\n","epoch 13:(86/293) batch || training time for 2 batch 2.9184858798980713 || training loss 0.00026702164905145764 ||\n","epoch 13:(88/293) batch || training time for 2 batch 2.9104394912719727 || training loss 0.00026059819356305525 ||\n","epoch 13:(90/293) batch || training time for 2 batch 2.9224295616149902 || training loss 0.0035301799653097987 ||\n","epoch 13:(92/293) batch || training time for 2 batch 2.933833599090576 || training loss 0.007582857098896056 ||\n","epoch 13:(94/293) batch || training time for 2 batch 2.8743183612823486 || training loss 0.0002811273734550923 ||\n","epoch 13:(96/293) batch || training time for 2 batch 2.91705584526062 || training loss 0.00025302993890363723 ||\n","epoch 13:(98/293) batch || training time for 2 batch 2.924933910369873 || training loss 0.0005165856127860025 ||\n","epoch 13:(100/293) batch || training time for 2 batch 2.898653030395508 || training loss 0.0003039866569451988 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 70.85702013969421s\n","epoch 13:(102/293) batch || training time for 2 batch 77.21122145652771 || training loss 0.0003699460066854954 ||\n","epoch 13:(104/293) batch || training time for 2 batch 2.8953752517700195 || training loss 0.00024798823142191395 ||\n","epoch 13:(106/293) batch || training time for 2 batch 2.904991865158081 || training loss 0.00026427705597598106 ||\n","epoch 13:(108/293) batch || training time for 2 batch 2.9276061058044434 || training loss 0.0002581704466138035 ||\n","epoch 13:(110/293) batch || training time for 2 batch 2.9587552547454834 || training loss 0.0003774656361201778 ||\n","epoch 13:(112/293) batch || training time for 2 batch 2.9521892070770264 || training loss 0.00023834627791075036 ||\n","epoch 13:(114/293) batch || training time for 2 batch 2.986133575439453 || training loss 0.00027545203920453787 ||\n","epoch 13:(116/293) batch || training time for 2 batch 2.9996094703674316 || training loss 0.0002630183444125578 ||\n","epoch 13:(118/293) batch || training time for 2 batch 3.003757953643799 || training loss 0.00032406691752839833 ||\n","epoch 13:(120/293) batch || training time for 2 batch 3.025965452194214 || training loss 0.0002755471505224705 ||\n","epoch 13:(122/293) batch || training time for 2 batch 3.004396915435791 || training loss 0.000251066085183993 ||\n","epoch 13:(124/293) batch || training time for 2 batch 3.025355339050293 || training loss 0.0002745757665252313 ||\n","epoch 13:(126/293) batch || training time for 2 batch 2.9961893558502197 || training loss 0.00036240754707250744 ||\n","epoch 13:(128/293) batch || training time for 2 batch 3.0144264698028564 || training loss 0.00036586747592082247 ||\n","epoch 13:(130/293) batch || training time for 2 batch 2.9836373329162598 || training loss 0.0002951978240162134 ||\n","epoch 13:(132/293) batch || training time for 2 batch 2.955714225769043 || training loss 0.00025809937505982816 ||\n","epoch 13:(134/293) batch || training time for 2 batch 2.9579362869262695 || training loss 0.0002638641599332914 ||\n","epoch 13:(136/293) batch || training time for 2 batch 2.9409167766571045 || training loss 0.00027634354773908854 ||\n","epoch 13:(138/293) batch || training time for 2 batch 2.908859968185425 || training loss 0.00028304781881161034 ||\n","epoch 13:(140/293) batch || training time for 2 batch 2.92098331451416 || training loss 0.0002548995107645169 ||\n","epoch 13:(142/293) batch || training time for 2 batch 2.9298033714294434 || training loss 0.00026791353593580425 ||\n","epoch 13:(144/293) batch || training time for 2 batch 2.9169423580169678 || training loss 0.0002586649206932634 ||\n","epoch 13:(146/293) batch || training time for 2 batch 2.908764362335205 || training loss 0.0002446403304929845 ||\n","epoch 13:(148/293) batch || training time for 2 batch 2.9030539989471436 || training loss 0.00027707943809218705 ||\n","epoch 13:(150/293) batch || training time for 2 batch 2.903809070587158 || training loss 0.0002573740348452702 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 73.24580240249634s\n","epoch 13:(152/293) batch || training time for 2 batch 79.56478261947632 || training loss 0.00026833449373953044 ||\n","epoch 13:(154/293) batch || training time for 2 batch 2.910719633102417 || training loss 0.00029546859150286764 ||\n","epoch 13:(156/293) batch || training time for 2 batch 2.9105100631713867 || training loss 0.0003099941313848831 ||\n","epoch 13:(158/293) batch || training time for 2 batch 2.91536283493042 || training loss 0.0002844456466846168 ||\n","epoch 13:(160/293) batch || training time for 2 batch 2.9172394275665283 || training loss 0.0002946125459857285 ||\n","epoch 13:(162/293) batch || training time for 2 batch 2.967259407043457 || training loss 0.00027418305398896337 ||\n","epoch 13:(164/293) batch || training time for 2 batch 2.9601521492004395 || training loss 0.0002582206216175109 ||\n","epoch 13:(166/293) batch || training time for 2 batch 2.9867990016937256 || training loss 0.0002892556949518621 ||\n","epoch 13:(168/293) batch || training time for 2 batch 3.0322277545928955 || training loss 0.000261252440395765 ||\n","epoch 13:(170/293) batch || training time for 2 batch 3.0185134410858154 || training loss 0.00025690876645967364 ||\n","epoch 13:(172/293) batch || training time for 2 batch 3.033860445022583 || training loss 0.00023766995582263917 ||\n","epoch 13:(174/293) batch || training time for 2 batch 3.0417065620422363 || training loss 0.00030642913770861924 ||\n","epoch 13:(176/293) batch || training time for 2 batch 3.0017600059509277 || training loss 0.00026280984457116574 ||\n","epoch 13:(178/293) batch || training time for 2 batch 3.0094265937805176 || training loss 0.00026011071167886257 ||\n","epoch 13:(180/293) batch || training time for 2 batch 2.978229284286499 || training loss 0.00026241470186505467 ||\n","epoch 13:(182/293) batch || training time for 2 batch 2.9640214443206787 || training loss 0.00023259515728568658 ||\n","epoch 13:(184/293) batch || training time for 2 batch 2.939525604248047 || training loss 0.00025861179165076464 ||\n","epoch 13:(186/293) batch || training time for 2 batch 2.9304723739624023 || training loss 0.0002934750518761575 ||\n","epoch 13:(188/293) batch || training time for 2 batch 2.9078803062438965 || training loss 0.0002524131559766829 ||\n","epoch 13:(190/293) batch || training time for 2 batch 2.893205404281616 || training loss 0.0023813928564777598 ||\n","epoch 13:(192/293) batch || training time for 2 batch 2.8982105255126953 || training loss 0.0002695880539249629 ||\n","epoch 13:(194/293) batch || training time for 2 batch 2.9100773334503174 || training loss 0.0003063248295802623 ||\n","epoch 13:(196/293) batch || training time for 2 batch 2.9333364963531494 || training loss 0.0003689449076773599 ||\n","epoch 13:(198/293) batch || training time for 2 batch 2.9130735397338867 || training loss 0.000880788589711301 ||\n","1.44115188075856e-06\n","epoch 13:(200/293) batch || training time for 2 batch 2.884384870529175 || training loss 0.00291827684850432 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 73.35067963600159s\n","epoch 13:(202/293) batch || training time for 2 batch 79.72285842895508 || training loss 0.0002482071868143976 ||\n","epoch 13:(204/293) batch || training time for 2 batch 2.920271158218384 || training loss 0.00028087680402677506 ||\n","epoch 13:(206/293) batch || training time for 2 batch 2.9136455059051514 || training loss 0.00025425845524296165 ||\n","epoch 13:(208/293) batch || training time for 2 batch 2.9441258907318115 || training loss 0.0002749933482846245 ||\n","epoch 13:(210/293) batch || training time for 2 batch 2.9242608547210693 || training loss 0.00023314235295401886 ||\n","epoch 13:(212/293) batch || training time for 2 batch 2.948415517807007 || training loss 0.0002487092569936067 ||\n","epoch 13:(214/293) batch || training time for 2 batch 2.9803953170776367 || training loss 0.0002587333001429215 ||\n","epoch 13:(216/293) batch || training time for 2 batch 2.989407777786255 || training loss 0.00025503225333523005 ||\n","epoch 13:(218/293) batch || training time for 2 batch 3.031130790710449 || training loss 0.00024388788733631372 ||\n","epoch 13:(220/293) batch || training time for 2 batch 3.025045394897461 || training loss 0.00023909937590360641 ||\n","epoch 13:(222/293) batch || training time for 2 batch 3.0323004722595215 || training loss 0.00024360428506042808 ||\n","epoch 13:(224/293) batch || training time for 2 batch 3.0353589057922363 || training loss 0.0002548710035625845 ||\n","epoch 13:(226/293) batch || training time for 2 batch 3.046800374984741 || training loss 0.00025007082149386406 ||\n","epoch 13:(228/293) batch || training time for 2 batch 3.019879102706909 || training loss 0.00027365291316527873 ||\n","epoch 13:(230/293) batch || training time for 2 batch 2.98581862449646 || training loss 0.0002423973273835145 ||\n","epoch 13:(232/293) batch || training time for 2 batch 2.978151321411133 || training loss 0.0002692482667043805 ||\n","epoch 13:(234/293) batch || training time for 2 batch 2.9168851375579834 || training loss 0.0010533704044064507 ||\n","epoch 13:(236/293) batch || training time for 2 batch 2.9385323524475098 || training loss 0.0002578433632152155 ||\n","epoch 13:(238/293) batch || training time for 2 batch 2.8989663124084473 || training loss 0.0002335909812245518 ||\n","epoch 13:(240/293) batch || training time for 2 batch 2.8912065029144287 || training loss 0.0002352198789594695 ||\n","epoch 13:(242/293) batch || training time for 2 batch 2.9226555824279785 || training loss 0.00034881058672908694 ||\n","epoch 13:(244/293) batch || training time for 2 batch 2.9042375087738037 || training loss 0.00031538082839688286 ||\n","epoch 13:(246/293) batch || training time for 2 batch 2.9437994956970215 || training loss 0.0005621422606054693 ||\n","epoch 13:(248/293) batch || training time for 2 batch 2.9107789993286133 || training loss 0.00025941277272067964 ||\n","epoch 13:(250/293) batch || training time for 2 batch 2.8991122245788574 || training loss 0.00024408797617070377 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 78.94261598587036s\n","epoch 13:(252/293) batch || training time for 2 batch 85.29129958152771 || training loss 0.00024736264458624646 ||\n","epoch 13:(254/293) batch || training time for 2 batch 2.8914413452148438 || training loss 0.00022990059369476512 ||\n","epoch 13:(256/293) batch || training time for 2 batch 2.9202167987823486 || training loss 0.0005939356051385403 ||\n","epoch 13:(258/293) batch || training time for 2 batch 2.942995071411133 || training loss 0.0004636012017726898 ||\n","epoch 13:(260/293) batch || training time for 2 batch 2.939384937286377 || training loss 0.0021939932194072753 ||\n","epoch 13:(262/293) batch || training time for 2 batch 2.9445314407348633 || training loss 0.00024692951410543174 ||\n","epoch 13:(264/293) batch || training time for 2 batch 2.9516499042510986 || training loss 0.00022914768487680703 ||\n","epoch 13:(266/293) batch || training time for 2 batch 2.992201805114746 || training loss 0.0002469343671691604 ||\n","epoch 13:(268/293) batch || training time for 2 batch 2.9576408863067627 || training loss 0.0002384924446232617 ||\n","epoch 13:(270/293) batch || training time for 2 batch 2.9571728706359863 || training loss 0.0002600819861982018 ||\n","epoch 13:(272/293) batch || training time for 2 batch 2.9360883235931396 || training loss 0.00023786041128914803 ||\n","epoch 13:(274/293) batch || training time for 2 batch 2.954171895980835 || training loss 0.00023268733639270067 ||\n","epoch 13:(276/293) batch || training time for 2 batch 2.9601645469665527 || training loss 0.0002596679551061243 ||\n","epoch 13:(278/293) batch || training time for 2 batch 2.9485766887664795 || training loss 0.00024482457956764847 ||\n","epoch 13:(280/293) batch || training time for 2 batch 2.9419052600860596 || training loss 0.00026051077293232083 ||\n","epoch 13:(282/293) batch || training time for 2 batch 2.982340097427368 || training loss 0.0002547139156376943 ||\n","epoch 13:(284/293) batch || training time for 2 batch 2.953489065170288 || training loss 0.00024818024394335225 ||\n","epoch 13:(286/293) batch || training time for 2 batch 2.9589030742645264 || training loss 0.0003680394002003595 ||\n","epoch 13:(288/293) batch || training time for 2 batch 2.9537689685821533 || training loss 0.00023537555534858257 ||\n","epoch 13:(290/293) batch || training time for 2 batch 2.9432828426361084 || training loss 0.00038253671664278954 ||\n","epoch 13:(292/293) batch || training time for 2 batch 2.9242184162139893 || training loss 0.00028577396005857736 ||\n","epoch 14:(2/293) batch || training time for 2 batch 4.3967125415802 || training loss 0.0003819876001216471 ||\n","epoch 14:(4/293) batch || training time for 2 batch 2.9359731674194336 || training loss 0.0002500030386727303 ||\n","epoch 14:(6/293) batch || training time for 2 batch 2.9197208881378174 || training loss 0.00026814253942575306 ||\n","epoch 14:(8/293) batch || training time for 2 batch 2.9261651039123535 || training loss 0.0002626173591124825 ||\n","epoch 14:(10/293) batch || training time for 2 batch 2.9180076122283936 || training loss 0.0014095710357651114 ||\n","epoch 14:(12/293) batch || training time for 2 batch 2.9389073848724365 || training loss 0.00028214004123583436 ||\n","epoch 14:(14/293) batch || training time for 2 batch 2.908168077468872 || training loss 0.0002515563464839943 ||\n","epoch 14:(16/293) batch || training time for 2 batch 2.9353747367858887 || training loss 0.00027302749367663637 ||\n","epoch 14:(18/293) batch || training time for 2 batch 2.918118715286255 || training loss 0.00022798917052568868 ||\n","epoch 14:(20/293) batch || training time for 2 batch 2.920886516571045 || training loss 0.0002998341806232929 ||\n","epoch 14:(22/293) batch || training time for 2 batch 2.9445362091064453 || training loss 0.00026711851387517527 ||\n","epoch 14:(24/293) batch || training time for 2 batch 2.961977243423462 || training loss 0.0002655095304362476 ||\n","epoch 14:(26/293) batch || training time for 2 batch 2.9676849842071533 || training loss 0.0003085225325776264 ||\n","epoch 14:(28/293) batch || training time for 2 batch 2.945054292678833 || training loss 0.0002472844207659364 ||\n","epoch 14:(30/293) batch || training time for 2 batch 2.9611053466796875 || training loss 0.000243048052652739 ||\n","epoch 14:(32/293) batch || training time for 2 batch 2.95819091796875 || training loss 0.0003126046067336574 ||\n","epoch 14:(34/293) batch || training time for 2 batch 2.943085193634033 || training loss 0.00024727666459511966 ||\n","epoch 14:(36/293) batch || training time for 2 batch 2.961601495742798 || training loss 0.00022854709095554426 ||\n","epoch 14:(38/293) batch || training time for 2 batch 2.923710584640503 || training loss 0.0002508426405256614 ||\n","epoch 14:(40/293) batch || training time for 2 batch 2.96881365776062 || training loss 0.0002814261242747307 ||\n","epoch 14:(42/293) batch || training time for 2 batch 2.9526162147521973 || training loss 0.0002447782753733918 ||\n","epoch 14:(44/293) batch || training time for 2 batch 2.92993426322937 || training loss 0.000260116154095158 ||\n","epoch 14:(46/293) batch || training time for 2 batch 2.9533605575561523 || training loss 0.0002507470198906958 ||\n","epoch 14:(48/293) batch || training time for 2 batch 2.997255563735962 || training loss 0.0002389376168139279 ||\n","epoch 14:(50/293) batch || training time for 2 batch 2.939697027206421 || training loss 0.0002479272516211495 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 70.24972343444824s\n","epoch 14:(52/293) batch || training time for 2 batch 76.68622016906738 || training loss 0.00025557936169207096 ||\n","epoch 14:(54/293) batch || training time for 2 batch 2.914705753326416 || training loss 0.00024163124908227473 ||\n","epoch 14:(56/293) batch || training time for 2 batch 2.925930976867676 || training loss 0.00024800680694170296 ||\n","epoch 14:(58/293) batch || training time for 2 batch 2.9329259395599365 || training loss 0.00024556736752856523 ||\n","epoch 14:(60/293) batch || training time for 2 batch 2.96592116355896 || training loss 0.0002529886842239648 ||\n","epoch 14:(62/293) batch || training time for 2 batch 2.9671483039855957 || training loss 0.00028365566686261445 ||\n","epoch 14:(64/293) batch || training time for 2 batch 2.963724374771118 || training loss 0.0002668316592462361 ||\n","epoch 14:(66/293) batch || training time for 2 batch 3.002286911010742 || training loss 0.0003652903251349926 ||\n","epoch 14:(68/293) batch || training time for 2 batch 3.028384208679199 || training loss 0.0002627759240567684 ||\n","epoch 14:(70/293) batch || training time for 2 batch 3.02854061126709 || training loss 0.0002495794979040511 ||\n","epoch 14:(72/293) batch || training time for 2 batch 3.034930467605591 || training loss 0.00025515584275126457 ||\n","epoch 14:(74/293) batch || training time for 2 batch 3.041715621948242 || training loss 0.00023825675452826545 ||\n","epoch 14:(76/293) batch || training time for 2 batch 2.9986412525177 || training loss 0.0002676000149222091 ||\n","epoch 14:(78/293) batch || training time for 2 batch 3.0026965141296387 || training loss 0.00024881205172277987 ||\n","epoch 14:(80/293) batch || training time for 2 batch 2.965508222579956 || training loss 0.00026716462161857635 ||\n","epoch 14:(82/293) batch || training time for 2 batch 2.9388914108276367 || training loss 0.00026960877585224807 ||\n","epoch 14:(84/293) batch || training time for 2 batch 2.9446308612823486 || training loss 0.0002715181472012773 ||\n","epoch 14:(86/293) batch || training time for 2 batch 2.9543025493621826 || training loss 0.00025518536858726293 ||\n","epoch 14:(88/293) batch || training time for 2 batch 2.909855365753174 || training loss 0.00024230752751464024 ||\n","epoch 14:(90/293) batch || training time for 2 batch 2.9170360565185547 || training loss 0.0002474379289196804 ||\n","epoch 14:(92/293) batch || training time for 2 batch 2.918682098388672 || training loss 0.0002644729247549549 ||\n","epoch 14:(94/293) batch || training time for 2 batch 2.9562017917633057 || training loss 0.0011944638099521399 ||\n","epoch 14:(96/293) batch || training time for 2 batch 2.9051949977874756 || training loss 0.00028581956576090306 ||\n","epoch 14:(98/293) batch || training time for 2 batch 2.8993003368377686 || training loss 0.00026506448921281844 ||\n","epoch 14:(100/293) batch || training time for 2 batch 2.9195775985717773 || training loss 0.00024618133465992287 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 80.67504811286926s\n","epoch 14:(102/293) batch || training time for 2 batch 87.08533072471619 || training loss 0.0002576736151240766 ||\n","epoch 14:(104/293) batch || training time for 2 batch 2.9062869548797607 || training loss 0.0002752689179033041 ||\n","epoch 14:(106/293) batch || training time for 2 batch 2.9245457649230957 || training loss 0.0012398752878652886 ||\n","epoch 14:(108/293) batch || training time for 2 batch 2.9573442935943604 || training loss 0.0002662458427948877 ||\n","epoch 14:(110/293) batch || training time for 2 batch 2.969514846801758 || training loss 0.0002444013734930195 ||\n","epoch 14:(112/293) batch || training time for 2 batch 2.9783577919006348 || training loss 0.0002420333694317378 ||\n","epoch 14:(114/293) batch || training time for 2 batch 2.986435890197754 || training loss 0.00025106426619458944 ||\n","epoch 14:(116/293) batch || training time for 2 batch 2.977785348892212 || training loss 0.00025118543999269605 ||\n","epoch 14:(118/293) batch || training time for 2 batch 3.0370802879333496 || training loss 0.0025476155133219436 ||\n","epoch 14:(120/293) batch || training time for 2 batch 3.013650417327881 || training loss 0.00025888111849781126 ||\n","epoch 14:(122/293) batch || training time for 2 batch 3.004702568054199 || training loss 0.00023174940724857152 ||\n","epoch 14:(124/293) batch || training time for 2 batch 3.0072638988494873 || training loss 0.00024871694040484726 ||\n","epoch 14:(126/293) batch || training time for 2 batch 2.9926371574401855 || training loss 0.0002663838167791255 ||\n","epoch 14:(128/293) batch || training time for 2 batch 2.981527328491211 || training loss 0.00023477360082324594 ||\n","epoch 14:(130/293) batch || training time for 2 batch 2.967128038406372 || training loss 0.00026668734790291637 ||\n","epoch 14:(132/293) batch || training time for 2 batch 2.9631736278533936 || training loss 0.00024078888236545026 ||\n","epoch 14:(134/293) batch || training time for 2 batch 2.956665515899658 || training loss 0.0002672835544217378 ||\n","epoch 14:(136/293) batch || training time for 2 batch 2.9492709636688232 || training loss 0.00024771459720795974 ||\n","epoch 14:(138/293) batch || training time for 2 batch 2.918593645095825 || training loss 0.0002684704231796786 ||\n","epoch 14:(140/293) batch || training time for 2 batch 2.9072349071502686 || training loss 0.0004953741154167801 ||\n","epoch 14:(142/293) batch || training time for 2 batch 2.916421413421631 || training loss 0.0002236376458313316 ||\n","epoch 14:(144/293) batch || training time for 2 batch 2.9326107501983643 || training loss 0.00030903497827239335 ||\n","epoch 14:(146/293) batch || training time for 2 batch 2.919285774230957 || training loss 0.0002450910833431408 ||\n","epoch 14:(148/293) batch || training time for 2 batch 2.9110167026519775 || training loss 0.00021257991465972736 ||\n","epoch 14:(150/293) batch || training time for 2 batch 2.9151134490966797 || training loss 0.0003512328694341704 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 70.8744101524353s\n","epoch 14:(152/293) batch || training time for 2 batch 77.26158666610718 || training loss 0.00036074726085644215 ||\n","epoch 14:(154/293) batch || training time for 2 batch 2.9109675884246826 || training loss 0.0004129666558583267 ||\n","epoch 14:(156/293) batch || training time for 2 batch 2.939354658126831 || training loss 0.00023923562548588961 ||\n","epoch 14:(158/293) batch || training time for 2 batch 2.9445137977600098 || training loss 0.0003055261622648686 ||\n","epoch 14:(160/293) batch || training time for 2 batch 2.978789806365967 || training loss 0.00024794304772512987 ||\n","epoch 14:(162/293) batch || training time for 2 batch 2.9362192153930664 || training loss 0.00023640142899239436 ||\n","epoch 14:(164/293) batch || training time for 2 batch 2.98026704788208 || training loss 0.00029023236129432917 ||\n","epoch 14:(166/293) batch || training time for 2 batch 3.003387689590454 || training loss 0.0012110841635148972 ||\n","epoch 14:(168/293) batch || training time for 2 batch 2.991781234741211 || training loss 0.003910557454219088 ||\n","epoch 14:(170/293) batch || training time for 2 batch 3.02111554145813 || training loss 0.010447069449583068 ||\n","epoch 14:(172/293) batch || training time for 2 batch 3.0081253051757812 || training loss 0.0002522283175494522 ||\n","epoch 14:(174/293) batch || training time for 2 batch 3.0010249614715576 || training loss 0.0002463347918819636 ||\n","epoch 14:(176/293) batch || training time for 2 batch 2.989044666290283 || training loss 0.0002477399102644995 ||\n","epoch 14:(178/293) batch || training time for 2 batch 2.9669768810272217 || training loss 0.003571703826310113 ||\n","epoch 14:(180/293) batch || training time for 2 batch 2.946810245513916 || training loss 0.0002623321197461337 ||\n","epoch 14:(182/293) batch || training time for 2 batch 2.9576244354248047 || training loss 0.0002749086415860802 ||\n","epoch 14:(184/293) batch || training time for 2 batch 2.9302992820739746 || training loss 0.0002459021779941395 ||\n","epoch 14:(186/293) batch || training time for 2 batch 2.927391767501831 || training loss 0.00023860760848037899 ||\n","epoch 14:(188/293) batch || training time for 2 batch 2.914064884185791 || training loss 0.00025656794605311006 ||\n","epoch 14:(190/293) batch || training time for 2 batch 2.9603805541992188 || training loss 0.00025177908537443727 ||\n","epoch 14:(192/293) batch || training time for 2 batch 2.914083957672119 || training loss 0.00023400436475640163 ||\n","epoch 14:(194/293) batch || training time for 2 batch 2.945456027984619 || training loss 0.0003281639510532841 ||\n","epoch 14:(196/293) batch || training time for 2 batch 2.954958915710449 || training loss 0.00026958491071127355 ||\n","epoch 14:(198/293) batch || training time for 2 batch 2.9164862632751465 || training loss 0.0005349054481484927 ||\n","1.152921504606848e-06\n","epoch 14:(200/293) batch || training time for 2 batch 2.9229180812835693 || training loss 0.000312981748720631 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 67.87097024917603s\n","epoch 14:(202/293) batch || training time for 2 batch 74.22088098526001 || training loss 0.00024241812934633344 ||\n","epoch 14:(204/293) batch || training time for 2 batch 2.886983633041382 || training loss 0.003003078978508711 ||\n","epoch 14:(206/293) batch || training time for 2 batch 2.906510829925537 || training loss 0.00025351847580168396 ||\n","epoch 14:(208/293) batch || training time for 2 batch 2.9298341274261475 || training loss 0.00026212874217890203 ||\n","epoch 14:(210/293) batch || training time for 2 batch 2.9824490547180176 || training loss 0.0002961473001050763 ||\n","epoch 14:(212/293) batch || training time for 2 batch 2.9832870960235596 || training loss 0.0002271009871037677 ||\n","epoch 14:(214/293) batch || training time for 2 batch 3.010455369949341 || training loss 0.00038406378735089675 ||\n","epoch 14:(216/293) batch || training time for 2 batch 2.9989476203918457 || training loss 0.001531539877760224 ||\n","epoch 14:(218/293) batch || training time for 2 batch 3.0440328121185303 || training loss 0.00033833918860182166 ||\n","epoch 14:(220/293) batch || training time for 2 batch 3.0130932331085205 || training loss 0.00022948585683479905 ||\n","epoch 14:(222/293) batch || training time for 2 batch 3.0614359378814697 || training loss 0.00024532411771360785 ||\n","epoch 14:(224/293) batch || training time for 2 batch 3.064082622528076 || training loss 0.0002590401127235964 ||\n","epoch 14:(226/293) batch || training time for 2 batch 3.015268325805664 || training loss 0.0003088728408329189 ||\n","epoch 14:(228/293) batch || training time for 2 batch 2.995465040206909 || training loss 0.00024999180459417403 ||\n","epoch 14:(230/293) batch || training time for 2 batch 2.96591854095459 || training loss 0.0014556226815329865 ||\n","epoch 14:(232/293) batch || training time for 2 batch 2.9253437519073486 || training loss 0.00024347011640202254 ||\n","epoch 14:(234/293) batch || training time for 2 batch 2.933082103729248 || training loss 0.0002254439241369255 ||\n","epoch 14:(236/293) batch || training time for 2 batch 2.946380376815796 || training loss 0.00023686537315370515 ||\n","epoch 14:(238/293) batch || training time for 2 batch 2.945098638534546 || training loss 0.00025500045740045607 ||\n","epoch 14:(240/293) batch || training time for 2 batch 2.923963785171509 || training loss 0.000499818503158167 ||\n","epoch 14:(242/293) batch || training time for 2 batch 2.8916428089141846 || training loss 0.0002393231916357763 ||\n","epoch 14:(244/293) batch || training time for 2 batch 2.913642644882202 || training loss 0.00026695044653024524 ||\n","epoch 14:(246/293) batch || training time for 2 batch 2.903247833251953 || training loss 0.00022624503617407754 ||\n","epoch 14:(248/293) batch || training time for 2 batch 2.9211370944976807 || training loss 0.0004024999216198921 ||\n","epoch 14:(250/293) batch || training time for 2 batch 2.907865524291992 || training loss 0.00021748740982729942 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 75.56918716430664s\n","epoch 14:(252/293) batch || training time for 2 batch 81.95426273345947 || training loss 0.00022133956372272223 ||\n","epoch 14:(254/293) batch || training time for 2 batch 2.8985111713409424 || training loss 0.00022674212232232094 ||\n","epoch 14:(256/293) batch || training time for 2 batch 2.892524242401123 || training loss 0.00022465640358859673 ||\n","epoch 14:(258/293) batch || training time for 2 batch 2.9197046756744385 || training loss 0.000255381157330703 ||\n","epoch 14:(260/293) batch || training time for 2 batch 2.9073891639709473 || training loss 0.0002417715368210338 ||\n","epoch 14:(262/293) batch || training time for 2 batch 2.990832567214966 || training loss 0.0002574905229266733 ||\n","epoch 14:(264/293) batch || training time for 2 batch 3.0022647380828857 || training loss 0.0017115689115598798 ||\n","epoch 14:(266/293) batch || training time for 2 batch 3.0183207988739014 || training loss 0.0003058371657971293 ||\n","epoch 14:(268/293) batch || training time for 2 batch 3.0029709339141846 || training loss 0.00021483920136233792 ||\n","epoch 14:(270/293) batch || training time for 2 batch 3.0068812370300293 || training loss 0.0002299510670127347 ||\n","epoch 14:(272/293) batch || training time for 2 batch 3.0148184299468994 || training loss 0.0005383771349443123 ||\n","epoch 14:(274/293) batch || training time for 2 batch 3.019949436187744 || training loss 0.00024397280503762886 ||\n","epoch 14:(276/293) batch || training time for 2 batch 3.0542240142822266 || training loss 0.00043680045928340405 ||\n","epoch 14:(278/293) batch || training time for 2 batch 3.01066255569458 || training loss 0.0002321120773558505 ||\n","epoch 14:(280/293) batch || training time for 2 batch 2.9781293869018555 || training loss 0.00024618352472316474 ||\n","epoch 14:(282/293) batch || training time for 2 batch 2.943866729736328 || training loss 0.00022770219220547006 ||\n","epoch 14:(284/293) batch || training time for 2 batch 2.9659876823425293 || training loss 0.00022944547527004033 ||\n","epoch 14:(286/293) batch || training time for 2 batch 2.918755292892456 || training loss 0.00025667474255897105 ||\n","epoch 14:(288/293) batch || training time for 2 batch 2.938690423965454 || training loss 0.00025309812190243974 ||\n","epoch 14:(290/293) batch || training time for 2 batch 2.9128165245056152 || training loss 0.00029322292539291084 ||\n","epoch 14:(292/293) batch || training time for 2 batch 2.8953909873962402 || training loss 0.00023150180641096085 ||\n","epoch 15:(2/293) batch || training time for 2 batch 4.358087778091431 || training loss 0.0004580392051138915 ||\n","epoch 15:(4/293) batch || training time for 2 batch 2.893630027770996 || training loss 0.0002242875925730914 ||\n","epoch 15:(6/293) batch || training time for 2 batch 2.913105010986328 || training loss 0.00022492186690215021 ||\n","epoch 15:(8/293) batch || training time for 2 batch 2.9061288833618164 || training loss 0.000262566318269819 ||\n","epoch 15:(10/293) batch || training time for 2 batch 2.912620782852173 || training loss 0.00022136577899800614 ||\n","epoch 15:(12/293) batch || training time for 2 batch 2.9128079414367676 || training loss 0.0002346707187825814 ||\n","epoch 15:(14/293) batch || training time for 2 batch 2.9110159873962402 || training loss 0.0002318789265700616 ||\n","epoch 15:(16/293) batch || training time for 2 batch 2.9201929569244385 || training loss 0.0002469756727805361 ||\n","epoch 15:(18/293) batch || training time for 2 batch 2.938807249069214 || training loss 0.00023354827135335654 ||\n","epoch 15:(20/293) batch || training time for 2 batch 2.9392893314361572 || training loss 0.003919677830708679 ||\n","epoch 15:(22/293) batch || training time for 2 batch 2.916883707046509 || training loss 0.0002120602221111767 ||\n","epoch 15:(24/293) batch || training time for 2 batch 2.9479222297668457 || training loss 0.00024192247656174004 ||\n","epoch 15:(26/293) batch || training time for 2 batch 2.941399335861206 || training loss 0.0002433395929983817 ||\n","epoch 15:(28/293) batch || training time for 2 batch 2.952378749847412 || training loss 0.00023677042190684006 ||\n","epoch 15:(30/293) batch || training time for 2 batch 2.968635082244873 || training loss 0.0016778035715105943 ||\n","epoch 15:(32/293) batch || training time for 2 batch 2.9558298587799072 || training loss 0.00023190495267044753 ||\n","epoch 15:(34/293) batch || training time for 2 batch 2.97865629196167 || training loss 0.00020681288151536137 ||\n","epoch 15:(36/293) batch || training time for 2 batch 2.9506609439849854 || training loss 0.0004515690088737756 ||\n","epoch 15:(38/293) batch || training time for 2 batch 2.980563163757324 || training loss 0.000263982881733682 ||\n","epoch 15:(40/293) batch || training time for 2 batch 2.961533546447754 || training loss 0.0002272454439662397 ||\n","epoch 15:(42/293) batch || training time for 2 batch 2.977738380432129 || training loss 0.000249260563577991 ||\n","epoch 15:(44/293) batch || training time for 2 batch 2.9478368759155273 || training loss 0.0002458701783325523 ||\n","epoch 15:(46/293) batch || training time for 2 batch 2.919567108154297 || training loss 0.00022756425460102037 ||\n","epoch 15:(48/293) batch || training time for 2 batch 2.916762590408325 || training loss 0.00023556039377581328 ||\n","epoch 15:(50/293) batch || training time for 2 batch 2.9276530742645264 || training loss 0.0002453645793139003 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 72.34036612510681s\n","epoch 15:(52/293) batch || training time for 2 batch 78.63838791847229 || training loss 0.00023040384985506535 ||\n","epoch 15:(54/293) batch || training time for 2 batch 2.8959097862243652 || training loss 0.0002680501784197986 ||\n","epoch 15:(56/293) batch || training time for 2 batch 2.902766704559326 || training loss 0.00023234044783748686 ||\n","epoch 15:(58/293) batch || training time for 2 batch 2.9144833087921143 || training loss 0.00023743510246276855 ||\n","epoch 15:(60/293) batch || training time for 2 batch 2.926604747772217 || training loss 0.00023884377151262015 ||\n","epoch 15:(62/293) batch || training time for 2 batch 2.9339215755462646 || training loss 0.0002325821842532605 ||\n","epoch 15:(64/293) batch || training time for 2 batch 2.974029541015625 || training loss 0.0009774769423529506 ||\n","epoch 15:(66/293) batch || training time for 2 batch 2.9824843406677246 || training loss 0.00028167813434265554 ||\n","epoch 15:(68/293) batch || training time for 2 batch 3.006726026535034 || training loss 0.0002373212919337675 ||\n","epoch 15:(70/293) batch || training time for 2 batch 3.014944314956665 || training loss 0.0002520123671274632 ||\n","epoch 15:(72/293) batch || training time for 2 batch 3.033773422241211 || training loss 0.00021816708613187075 ||\n","epoch 15:(74/293) batch || training time for 2 batch 3.032813310623169 || training loss 0.00025217007350875065 ||\n","epoch 15:(76/293) batch || training time for 2 batch 3.0337302684783936 || training loss 0.00023024419351713732 ||\n","epoch 15:(78/293) batch || training time for 2 batch 2.989424467086792 || training loss 0.000801047841378022 ||\n","epoch 15:(80/293) batch || training time for 2 batch 2.9996347427368164 || training loss 0.0002303527362528257 ||\n","epoch 15:(82/293) batch || training time for 2 batch 2.9498794078826904 || training loss 0.00023420804063789546 ||\n","epoch 15:(84/293) batch || training time for 2 batch 2.9555504322052 || training loss 0.00023204271565191448 ||\n","epoch 15:(86/293) batch || training time for 2 batch 2.9336276054382324 || training loss 0.0002828342840075493 ||\n","epoch 15:(88/293) batch || training time for 2 batch 2.9288227558135986 || training loss 0.0003714799240697175 ||\n","epoch 15:(90/293) batch || training time for 2 batch 2.9115793704986572 || training loss 0.0002440412645228207 ||\n","epoch 15:(92/293) batch || training time for 2 batch 2.912046432495117 || training loss 0.00022059086040826514 ||\n","epoch 15:(94/293) batch || training time for 2 batch 2.9146485328674316 || training loss 0.00021497409761650488 ||\n","epoch 15:(96/293) batch || training time for 2 batch 2.898510456085205 || training loss 0.0002139353091479279 ||\n","epoch 15:(98/293) batch || training time for 2 batch 2.8907971382141113 || training loss 0.00024209812545450404 ||\n","epoch 15:(100/293) batch || training time for 2 batch 2.9005374908447266 || training loss 0.00023734246497042477 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 68.2658040523529s\n","epoch 15:(102/293) batch || training time for 2 batch 74.6137056350708 || training loss 0.00031604303512722254 ||\n","epoch 15:(104/293) batch || training time for 2 batch 2.9064807891845703 || training loss 0.000527821306604892 ||\n","epoch 15:(106/293) batch || training time for 2 batch 2.94407320022583 || training loss 0.00027692938601830974 ||\n","epoch 15:(108/293) batch || training time for 2 batch 2.937347888946533 || training loss 0.0002300696141901426 ||\n","epoch 15:(110/293) batch || training time for 2 batch 2.941423177719116 || training loss 0.00023763451463310048 ||\n","epoch 15:(112/293) batch || training time for 2 batch 2.9729206562042236 || training loss 0.0003777588572120294 ||\n","epoch 15:(114/293) batch || training time for 2 batch 2.955721378326416 || training loss 0.00024529969959985465 ||\n","epoch 15:(116/293) batch || training time for 2 batch 3.0258467197418213 || training loss 0.000268328411038965 ||\n","epoch 15:(118/293) batch || training time for 2 batch 3.0251595973968506 || training loss 0.0004520385555224493 ||\n","epoch 15:(120/293) batch || training time for 2 batch 3.066307306289673 || training loss 0.025233044230844826 ||\n","epoch 15:(122/293) batch || training time for 2 batch 3.049417018890381 || training loss 0.00024281324294861406 ||\n","epoch 15:(124/293) batch || training time for 2 batch 3.030355215072632 || training loss 0.0002620990126160905 ||\n","epoch 15:(126/293) batch || training time for 2 batch 3.021024703979492 || training loss 0.00020389471319504082 ||\n","epoch 15:(128/293) batch || training time for 2 batch 2.987520933151245 || training loss 0.0002590106741990894 ||\n","epoch 15:(130/293) batch || training time for 2 batch 2.9823193550109863 || training loss 0.0002625541092129424 ||\n","epoch 15:(132/293) batch || training time for 2 batch 2.965322732925415 || training loss 0.00022831761452835053 ||\n","epoch 15:(134/293) batch || training time for 2 batch 2.945051908493042 || training loss 0.0002847822179319337 ||\n","epoch 15:(136/293) batch || training time for 2 batch 2.9694080352783203 || training loss 0.0002430451277177781 ||\n","epoch 15:(138/293) batch || training time for 2 batch 2.976686954498291 || training loss 0.0002525227755540982 ||\n","epoch 15:(140/293) batch || training time for 2 batch 2.9372622966766357 || training loss 0.00023408739070873708 ||\n","epoch 15:(142/293) batch || training time for 2 batch 2.905660629272461 || training loss 0.00024056840629782528 ||\n","epoch 15:(144/293) batch || training time for 2 batch 2.8901517391204834 || training loss 0.00024315551127074286 ||\n","epoch 15:(146/293) batch || training time for 2 batch 2.9138238430023193 || training loss 0.0002235934734926559 ||\n","epoch 15:(148/293) batch || training time for 2 batch 2.8941261768341064 || training loss 0.0002324012530152686 ||\n","epoch 15:(150/293) batch || training time for 2 batch 2.8963205814361572 || training loss 0.00022113131126388907 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 73.8517804145813s\n","epoch 15:(152/293) batch || training time for 2 batch 80.21441006660461 || training loss 0.00028124547679908574 ||\n","epoch 15:(154/293) batch || training time for 2 batch 2.8868050575256348 || training loss 0.0002381555168540217 ||\n","epoch 15:(156/293) batch || training time for 2 batch 2.9207475185394287 || training loss 0.0002337302648811601 ||\n","epoch 15:(158/293) batch || training time for 2 batch 2.93650221824646 || training loss 0.00024071581719908863 ||\n","epoch 15:(160/293) batch || training time for 2 batch 2.960543155670166 || training loss 0.00022753136727260426 ||\n","epoch 15:(162/293) batch || training time for 2 batch 2.9667396545410156 || training loss 0.0012728731235256419 ||\n","epoch 15:(164/293) batch || training time for 2 batch 2.9640884399414062 || training loss 0.0003336778318043798 ||\n","epoch 15:(166/293) batch || training time for 2 batch 2.99718976020813 || training loss 0.0002382823222433217 ||\n","epoch 15:(168/293) batch || training time for 2 batch 3.0066280364990234 || training loss 0.000266192015260458 ||\n","epoch 15:(170/293) batch || training time for 2 batch 3.008070230484009 || training loss 0.0002333523952984251 ||\n","epoch 15:(172/293) batch || training time for 2 batch 3.032641649246216 || training loss 0.0002173469911213033 ||\n","epoch 15:(174/293) batch || training time for 2 batch 3.0269436836242676 || training loss 0.000226711985305883 ||\n","epoch 15:(176/293) batch || training time for 2 batch 3.0176594257354736 || training loss 0.00022679202811559662 ||\n","epoch 15:(178/293) batch || training time for 2 batch 3.0068399906158447 || training loss 0.002846000366844237 ||\n","epoch 15:(180/293) batch || training time for 2 batch 3.0108840465545654 || training loss 0.00022722571884514764 ||\n","epoch 15:(182/293) batch || training time for 2 batch 2.950934648513794 || training loss 0.00022003662888891995 ||\n","epoch 15:(184/293) batch || training time for 2 batch 2.970749855041504 || training loss 0.0028544232191052288 ||\n","epoch 15:(186/293) batch || training time for 2 batch 2.9391002655029297 || training loss 0.00022570208966499195 ||\n","epoch 15:(188/293) batch || training time for 2 batch 2.945201873779297 || training loss 0.000230558704060968 ||\n","epoch 15:(190/293) batch || training time for 2 batch 2.9096615314483643 || training loss 0.00022988147975411266 ||\n","epoch 15:(192/293) batch || training time for 2 batch 2.89231538772583 || training loss 0.001624431402888149 ||\n","epoch 15:(194/293) batch || training time for 2 batch 2.90354585647583 || training loss 0.00020075225620530546 ||\n","epoch 15:(196/293) batch || training time for 2 batch 2.9079318046569824 || training loss 0.00022063707001507282 ||\n","epoch 15:(198/293) batch || training time for 2 batch 2.859459638595581 || training loss 0.0020805453677894548 ||\n","9.223372036854785e-07\n","epoch 15:(200/293) batch || training time for 2 batch 2.9254186153411865 || training loss 0.00023982928541954607 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 73.3812608718872s\n","epoch 15:(202/293) batch || training time for 2 batch 79.77708840370178 || training loss 0.00023084098938852549 ||\n","epoch 15:(204/293) batch || training time for 2 batch 2.896446466445923 || training loss 0.0002406663988949731 ||\n","epoch 15:(206/293) batch || training time for 2 batch 2.898676872253418 || training loss 0.00021457107504829764 ||\n","epoch 15:(208/293) batch || training time for 2 batch 2.900023937225342 || training loss 0.00020246590429451317 ||\n","epoch 15:(210/293) batch || training time for 2 batch 2.92063045501709 || training loss 0.005344700621208176 ||\n","epoch 15:(212/293) batch || training time for 2 batch 2.9942193031311035 || training loss 0.00032630341593176126 ||\n","epoch 15:(214/293) batch || training time for 2 batch 2.99483323097229 || training loss 0.0002272640122100711 ||\n","epoch 15:(216/293) batch || training time for 2 batch 2.999171733856201 || training loss 0.00020982311252737418 ||\n","epoch 15:(218/293) batch || training time for 2 batch 3.0247809886932373 || training loss 0.00021046106849098578 ||\n","epoch 15:(220/293) batch || training time for 2 batch 3.051485061645508 || training loss 0.0002355549659114331 ||\n","epoch 15:(222/293) batch || training time for 2 batch 3.07973051071167 || training loss 0.00023361769854091108 ||\n","epoch 15:(224/293) batch || training time for 2 batch 3.0177204608917236 || training loss 0.0002236582149635069 ||\n","epoch 15:(226/293) batch || training time for 2 batch 3.0481674671173096 || training loss 0.00022458293824456632 ||\n","epoch 15:(228/293) batch || training time for 2 batch 2.9787189960479736 || training loss 0.0002031057811109349 ||\n","epoch 15:(230/293) batch || training time for 2 batch 2.9922256469726562 || training loss 0.0002973991067847237 ||\n","epoch 15:(232/293) batch || training time for 2 batch 2.937093496322632 || training loss 0.00024309460422955453 ||\n","epoch 15:(234/293) batch || training time for 2 batch 2.96901798248291 || training loss 0.003750396426767111 ||\n","epoch 15:(236/293) batch || training time for 2 batch 2.942225456237793 || training loss 0.0023542019043816254 ||\n","epoch 15:(238/293) batch || training time for 2 batch 2.9187042713165283 || training loss 0.0002805722615448758 ||\n","epoch 15:(240/293) batch || training time for 2 batch 2.9424726963043213 || training loss 0.00040652474854141474 ||\n","epoch 15:(242/293) batch || training time for 2 batch 2.9036576747894287 || training loss 0.00024864573788363487 ||\n","epoch 15:(244/293) batch || training time for 2 batch 2.897522449493408 || training loss 0.0002532316430006176 ||\n","epoch 15:(246/293) batch || training time for 2 batch 2.8900094032287598 || training loss 0.0002234947169199586 ||\n","epoch 15:(248/293) batch || training time for 2 batch 2.9045166969299316 || training loss 0.0002284259899170138 ||\n","epoch 15:(250/293) batch || training time for 2 batch 2.8759124279022217 || training loss 0.0002216628781752661 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 73.15351295471191s\n","epoch 15:(252/293) batch || training time for 2 batch 79.53641843795776 || training loss 0.00029031303711235523 ||\n","epoch 15:(254/293) batch || training time for 2 batch 2.894097328186035 || training loss 0.00022253626957535744 ||\n","epoch 15:(256/293) batch || training time for 2 batch 2.917992115020752 || training loss 0.0002355622564209625 ||\n","epoch 15:(258/293) batch || training time for 2 batch 2.9068241119384766 || training loss 0.00023468238941859454 ||\n","epoch 15:(260/293) batch || training time for 2 batch 2.9373693466186523 || training loss 0.0002551187280914746 ||\n","epoch 15:(262/293) batch || training time for 2 batch 2.981668472290039 || training loss 0.0002171769883716479 ||\n","epoch 15:(264/293) batch || training time for 2 batch 2.977996587753296 || training loss 0.00022228194575291127 ||\n","epoch 15:(266/293) batch || training time for 2 batch 2.998789072036743 || training loss 0.0002562200024840422 ||\n","epoch 15:(268/293) batch || training time for 2 batch 2.984280586242676 || training loss 0.0003475269040791318 ||\n","epoch 15:(270/293) batch || training time for 2 batch 3.0309157371520996 || training loss 0.0002238562301499769 ||\n","epoch 15:(272/293) batch || training time for 2 batch 3.046779155731201 || training loss 0.00021888948685955256 ||\n","epoch 15:(274/293) batch || training time for 2 batch 3.0500075817108154 || training loss 0.00020687274081865326 ||\n","epoch 15:(276/293) batch || training time for 2 batch 3.0423805713653564 || training loss 0.0003738686355063692 ||\n","epoch 15:(278/293) batch || training time for 2 batch 3.01562762260437 || training loss 0.00024148538068402559 ||\n","epoch 15:(280/293) batch || training time for 2 batch 3.001838207244873 || training loss 0.0002468256207066588 ||\n","epoch 15:(282/293) batch || training time for 2 batch 2.990575075149536 || training loss 0.0003297560688224621 ||\n","epoch 15:(284/293) batch || training time for 2 batch 2.946561574935913 || training loss 0.00024352317996090278 ||\n","epoch 15:(286/293) batch || training time for 2 batch 2.9597721099853516 || training loss 0.0002475808432791382 ||\n","epoch 15:(288/293) batch || training time for 2 batch 2.9300386905670166 || training loss 0.0002815355110215023 ||\n","epoch 15:(290/293) batch || training time for 2 batch 2.9515433311462402 || training loss 0.00022177294158609584 ||\n","epoch 15:(292/293) batch || training time for 2 batch 2.9240832328796387 || training loss 0.00022061420895624906 ||\n","epoch 16:(2/293) batch || training time for 2 batch 4.354356050491333 || training loss 0.00034003760083578527 ||\n","epoch 16:(4/293) batch || training time for 2 batch 2.930203914642334 || training loss 0.00043688714504241943 ||\n","epoch 16:(6/293) batch || training time for 2 batch 2.895550012588501 || training loss 0.0002536896718083881 ||\n","epoch 16:(8/293) batch || training time for 2 batch 2.8976082801818848 || training loss 0.00024119250883813947 ||\n","epoch 16:(10/293) batch || training time for 2 batch 2.890017032623291 || training loss 0.00021903748711338267 ||\n","epoch 16:(12/293) batch || training time for 2 batch 2.9248857498168945 || training loss 0.004064601147547364 ||\n","epoch 16:(14/293) batch || training time for 2 batch 2.90871524810791 || training loss 0.00022687843011226505 ||\n","epoch 16:(16/293) batch || training time for 2 batch 2.916182279586792 || training loss 0.0002262255220557563 ||\n","epoch 16:(18/293) batch || training time for 2 batch 2.9487297534942627 || training loss 0.0003301163378637284 ||\n","epoch 16:(20/293) batch || training time for 2 batch 2.955254554748535 || training loss 0.00022036880545783788 ||\n","epoch 16:(22/293) batch || training time for 2 batch 2.978869915008545 || training loss 0.005330208601662889 ||\n","epoch 16:(24/293) batch || training time for 2 batch 2.950366973876953 || training loss 0.00022418298613047227 ||\n","epoch 16:(26/293) batch || training time for 2 batch 2.968736171722412 || training loss 0.00023096646327758208 ||\n","epoch 16:(28/293) batch || training time for 2 batch 2.9486067295074463 || training loss 0.00023525593132944778 ||\n","epoch 16:(30/293) batch || training time for 2 batch 2.9825246334075928 || training loss 0.00021038419072283432 ||\n","epoch 16:(32/293) batch || training time for 2 batch 2.952657699584961 || training loss 0.00023032614262774587 ||\n","epoch 16:(34/293) batch || training time for 2 batch 2.9850575923919678 || training loss 0.00020676116400863975 ||\n","epoch 16:(36/293) batch || training time for 2 batch 2.9756062030792236 || training loss 0.00026866511325351894 ||\n","epoch 16:(38/293) batch || training time for 2 batch 2.96108078956604 || training loss 0.0020458616490941495 ||\n","epoch 16:(40/293) batch || training time for 2 batch 2.929616928100586 || training loss 0.00030210151453502476 ||\n","epoch 16:(42/293) batch || training time for 2 batch 2.9389560222625732 || training loss 0.004504486809310038 ||\n","epoch 16:(44/293) batch || training time for 2 batch 2.945807933807373 || training loss 0.0002382276943535544 ||\n","epoch 16:(46/293) batch || training time for 2 batch 2.943800449371338 || training loss 0.00021785219723824412 ||\n","epoch 16:(48/293) batch || training time for 2 batch 2.9435412883758545 || training loss 0.00022337479458656162 ||\n","epoch 16:(50/293) batch || training time for 2 batch 2.93204402923584 || training loss 0.0002804426694638096 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 79.50415778160095s\n","epoch 16:(52/293) batch || training time for 2 batch 85.82299065589905 || training loss 0.0002211090541095473 ||\n","epoch 16:(54/293) batch || training time for 2 batch 2.8849637508392334 || training loss 0.00022060983610572293 ||\n","epoch 16:(56/293) batch || training time for 2 batch 2.9005818367004395 || training loss 0.00023186017642728984 ||\n","epoch 16:(58/293) batch || training time for 2 batch 2.9178521633148193 || training loss 0.00020934213534928858 ||\n","epoch 16:(60/293) batch || training time for 2 batch 2.96342134475708 || training loss 0.0002417005889583379 ||\n","epoch 16:(62/293) batch || training time for 2 batch 2.9695937633514404 || training loss 0.00020733643032144755 ||\n","epoch 16:(64/293) batch || training time for 2 batch 3.0140769481658936 || training loss 0.0007803688931744546 ||\n","epoch 16:(66/293) batch || training time for 2 batch 3.0007057189941406 || training loss 0.00022711358178639784 ||\n","epoch 16:(68/293) batch || training time for 2 batch 3.0330469608306885 || training loss 0.00038734322151867673 ||\n","epoch 16:(70/293) batch || training time for 2 batch 3.035665512084961 || training loss 0.00022387831268133596 ||\n","epoch 16:(72/293) batch || training time for 2 batch 3.0537257194519043 || training loss 0.00023589156626258045 ||\n","epoch 16:(74/293) batch || training time for 2 batch 3.0327751636505127 || training loss 0.00025098393234657124 ||\n","epoch 16:(76/293) batch || training time for 2 batch 3.0284130573272705 || training loss 0.0002464165663695894 ||\n","epoch 16:(78/293) batch || training time for 2 batch 3.0285134315490723 || training loss 0.00021006326278438792 ||\n","epoch 16:(80/293) batch || training time for 2 batch 2.969494581222534 || training loss 0.0002209010999649763 ||\n","epoch 16:(82/293) batch || training time for 2 batch 2.9695820808410645 || training loss 0.00023930778115754947 ||\n","epoch 16:(84/293) batch || training time for 2 batch 2.920510768890381 || training loss 0.00022869143867865205 ||\n","epoch 16:(86/293) batch || training time for 2 batch 2.95324969291687 || training loss 0.0014777439791942015 ||\n","epoch 16:(88/293) batch || training time for 2 batch 2.947688579559326 || training loss 0.0002446154394419864 ||\n","epoch 16:(90/293) batch || training time for 2 batch 2.9468142986297607 || training loss 0.00022542841179529205 ||\n","epoch 16:(92/293) batch || training time for 2 batch 2.892814874649048 || training loss 0.00022307394101517275 ||\n","epoch 16:(94/293) batch || training time for 2 batch 2.900984287261963 || training loss 0.00023469521693186834 ||\n","epoch 16:(96/293) batch || training time for 2 batch 2.9105613231658936 || training loss 0.00026713144325185567 ||\n","epoch 16:(98/293) batch || training time for 2 batch 2.9152989387512207 || training loss 0.0002067259410978295 ||\n","epoch 16:(100/293) batch || training time for 2 batch 2.87943696975708 || training loss 0.0002093056245939806 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 79.23454856872559s\n","epoch 16:(102/293) batch || training time for 2 batch 85.55864477157593 || training loss 0.0002095848903991282 ||\n","epoch 16:(104/293) batch || training time for 2 batch 2.9038772583007812 || training loss 0.0002966624961118214 ||\n","epoch 16:(106/293) batch || training time for 2 batch 2.899803638458252 || training loss 0.00021989729430060834 ||\n","epoch 16:(108/293) batch || training time for 2 batch 2.9602744579315186 || training loss 0.00024963426403701305 ||\n","epoch 16:(110/293) batch || training time for 2 batch 2.935336112976074 || training loss 0.0002006256181630306 ||\n","epoch 16:(112/293) batch || training time for 2 batch 2.973687171936035 || training loss 0.0002163019307772629 ||\n","epoch 16:(114/293) batch || training time for 2 batch 2.972541570663452 || training loss 0.002733500885369722 ||\n","epoch 16:(116/293) batch || training time for 2 batch 2.986314296722412 || training loss 0.0002385095867794007 ||\n","epoch 16:(118/293) batch || training time for 2 batch 3.0199968814849854 || training loss 0.00024691203725524247 ||\n","epoch 16:(120/293) batch || training time for 2 batch 3.019920825958252 || training loss 0.0002199392911279574 ||\n","epoch 16:(122/293) batch || training time for 2 batch 3.0250301361083984 || training loss 0.00023780087212799117 ||\n","epoch 16:(124/293) batch || training time for 2 batch 3.0143277645111084 || training loss 0.0002185024495702237 ||\n","epoch 16:(126/293) batch || training time for 2 batch 3.0183732509613037 || training loss 0.00021686741820303723 ||\n","epoch 16:(128/293) batch || training time for 2 batch 2.969003200531006 || training loss 0.00023426226107403636 ||\n","epoch 16:(130/293) batch || training time for 2 batch 2.9945595264434814 || training loss 0.00035197960096411407 ||\n","epoch 16:(132/293) batch || training time for 2 batch 2.9421236515045166 || training loss 0.00022994309256318957 ||\n","epoch 16:(134/293) batch || training time for 2 batch 2.958559513092041 || training loss 0.00025056378945009783 ||\n","epoch 16:(136/293) batch || training time for 2 batch 2.9269680976867676 || training loss 0.0009190392156597227 ||\n","epoch 16:(138/293) batch || training time for 2 batch 2.9361259937286377 || training loss 0.00023165183665696532 ||\n","epoch 16:(140/293) batch || training time for 2 batch 2.898408889770508 || training loss 0.00024615865550003946 ||\n","epoch 16:(142/293) batch || training time for 2 batch 2.9196226596832275 || training loss 0.00022521143546327949 ||\n","epoch 16:(144/293) batch || training time for 2 batch 2.9359073638916016 || training loss 0.0002450782048981637 ||\n","epoch 16:(146/293) batch || training time for 2 batch 2.9096083641052246 || training loss 0.0002421459721517749 ||\n","epoch 16:(148/293) batch || training time for 2 batch 2.9080381393432617 || training loss 0.00021103107428643852 ||\n","epoch 16:(150/293) batch || training time for 2 batch 2.877725601196289 || training loss 0.00022300693672150373 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 80.99417448043823s\n","epoch 16:(152/293) batch || training time for 2 batch 87.34974694252014 || training loss 0.00020415112521732226 ||\n","epoch 16:(154/293) batch || training time for 2 batch 2.895221471786499 || training loss 0.00020892731117783114 ||\n","epoch 16:(156/293) batch || training time for 2 batch 2.9167943000793457 || training loss 0.00022149814321892336 ||\n","epoch 16:(158/293) batch || training time for 2 batch 2.893611431121826 || training loss 0.00021503573952941224 ||\n","epoch 16:(160/293) batch || training time for 2 batch 2.930610179901123 || training loss 0.00019969492859672755 ||\n","epoch 16:(162/293) batch || training time for 2 batch 2.9675049781799316 || training loss 0.00021112716785864905 ||\n","epoch 16:(164/293) batch || training time for 2 batch 2.9587769508361816 || training loss 0.0010943996530841105 ||\n","epoch 16:(166/293) batch || training time for 2 batch 3.0063867568969727 || training loss 0.00022327451733872294 ||\n","epoch 16:(168/293) batch || training time for 2 batch 3.016416072845459 || training loss 0.00021307892893673852 ||\n","epoch 16:(170/293) batch || training time for 2 batch 3.054326295852661 || training loss 0.0002442950426484458 ||\n","epoch 16:(172/293) batch || training time for 2 batch 3.0481503009796143 || training loss 0.00027568205405259505 ||\n","epoch 16:(174/293) batch || training time for 2 batch 3.0410990715026855 || training loss 0.00023659153521293774 ||\n","epoch 16:(176/293) batch || training time for 2 batch 3.084023952484131 || training loss 0.0003259373042965308 ||\n","epoch 16:(178/293) batch || training time for 2 batch 3.011573553085327 || training loss 0.00022208448353921995 ||\n","epoch 16:(180/293) batch || training time for 2 batch 2.9804890155792236 || training loss 0.0002588443094282411 ||\n","epoch 16:(182/293) batch || training time for 2 batch 2.9754514694213867 || training loss 0.00021967646171106026 ||\n","epoch 16:(184/293) batch || training time for 2 batch 2.962845802307129 || training loss 0.0002222306575276889 ||\n","epoch 16:(186/293) batch || training time for 2 batch 2.9233343601226807 || training loss 0.00021407431631814688 ||\n","epoch 16:(188/293) batch || training time for 2 batch 2.9384870529174805 || training loss 0.00020724625937873498 ||\n","epoch 16:(190/293) batch || training time for 2 batch 2.9409046173095703 || training loss 0.00021398078388301656 ||\n","epoch 16:(192/293) batch || training time for 2 batch 2.9114365577697754 || training loss 0.0002603990214993246 ||\n","epoch 16:(194/293) batch || training time for 2 batch 2.892826795578003 || training loss 0.0002231507169199176 ||\n","epoch 16:(196/293) batch || training time for 2 batch 2.9245543479919434 || training loss 0.00023984893778106198 ||\n","epoch 16:(198/293) batch || training time for 2 batch 2.9103920459747314 || training loss 0.00023564213188365102 ||\n","7.378697629483828e-07\n","epoch 16:(200/293) batch || training time for 2 batch 2.892397165298462 || training loss 0.00021736144844908267 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 80.86266350746155s\n","epoch 16:(202/293) batch || training time for 2 batch 87.23886823654175 || training loss 0.00020764688815688714 ||\n","epoch 16:(204/293) batch || training time for 2 batch 2.8946115970611572 || training loss 0.00022821911989012733 ||\n","epoch 16:(206/293) batch || training time for 2 batch 2.8913004398345947 || training loss 0.0002346461988054216 ||\n","epoch 16:(208/293) batch || training time for 2 batch 2.9158542156219482 || training loss 0.00021718067000620067 ||\n","epoch 16:(210/293) batch || training time for 2 batch 2.942673683166504 || training loss 0.00021570977696683258 ||\n","epoch 16:(212/293) batch || training time for 2 batch 2.9380009174346924 || training loss 0.00020021847740281373 ||\n","epoch 16:(214/293) batch || training time for 2 batch 2.973594903945923 || training loss 0.0002201173483626917 ||\n","epoch 16:(216/293) batch || training time for 2 batch 2.965728521347046 || training loss 0.00022982351947575808 ||\n","epoch 16:(218/293) batch || training time for 2 batch 3.030306339263916 || training loss 0.0002115991956088692 ||\n","epoch 16:(220/293) batch || training time for 2 batch 3.0341203212738037 || training loss 0.00021537761494982988 ||\n","epoch 16:(222/293) batch || training time for 2 batch 3.031754970550537 || training loss 0.00019621495448518544 ||\n","epoch 16:(224/293) batch || training time for 2 batch 3.018672466278076 || training loss 0.00019301414431538433 ||\n","epoch 16:(226/293) batch || training time for 2 batch 3.021958112716675 || training loss 0.00021918721904512495 ||\n","epoch 16:(228/293) batch || training time for 2 batch 3.0115647315979004 || training loss 0.0002347164845559746 ||\n","epoch 16:(230/293) batch || training time for 2 batch 2.953735589981079 || training loss 0.00023094192147254944 ||\n","epoch 16:(232/293) batch || training time for 2 batch 2.9523134231567383 || training loss 0.00023656757548451424 ||\n","epoch 16:(234/293) batch || training time for 2 batch 2.9575653076171875 || training loss 0.00021175065921852365 ||\n","epoch 16:(236/293) batch || training time for 2 batch 2.945312261581421 || training loss 0.0003960077156079933 ||\n","epoch 16:(238/293) batch || training time for 2 batch 2.914912700653076 || training loss 0.002495741755410563 ||\n","epoch 16:(240/293) batch || training time for 2 batch 2.919100761413574 || training loss 0.00021710955479647964 ||\n","epoch 16:(242/293) batch || training time for 2 batch 2.881014585494995 || training loss 0.0002021911641350016 ||\n","epoch 16:(244/293) batch || training time for 2 batch 2.9077792167663574 || training loss 0.00022482058557216078 ||\n","epoch 16:(246/293) batch || training time for 2 batch 2.908052921295166 || training loss 0.0002538646513130516 ||\n","epoch 16:(248/293) batch || training time for 2 batch 2.9028573036193848 || training loss 0.0002151135413441807 ||\n","epoch 16:(250/293) batch || training time for 2 batch 2.900129556655884 || training loss 0.00021148393716430292 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 80.31042098999023s\n","epoch 16:(252/293) batch || training time for 2 batch 86.72828316688538 || training loss 0.00023639445862499997 ||\n","epoch 16:(254/293) batch || training time for 2 batch 2.917856454849243 || training loss 0.0002647369328769855 ||\n","epoch 16:(256/293) batch || training time for 2 batch 2.898709297180176 || training loss 0.00019336333207320422 ||\n","epoch 16:(258/293) batch || training time for 2 batch 2.9257915019989014 || training loss 0.00024093870888464153 ||\n","epoch 16:(260/293) batch || training time for 2 batch 2.912989377975464 || training loss 0.00020869715081062168 ||\n","epoch 16:(262/293) batch || training time for 2 batch 2.961866855621338 || training loss 0.00022348843776853755 ||\n","epoch 16:(264/293) batch || training time for 2 batch 2.973313808441162 || training loss 0.00023250419326359406 ||\n","epoch 16:(266/293) batch || training time for 2 batch 2.9792609214782715 || training loss 0.000203015748411417 ||\n","epoch 16:(268/293) batch || training time for 2 batch 3.023993730545044 || training loss 0.0002246039657620713 ||\n","epoch 16:(270/293) batch || training time for 2 batch 3.020732879638672 || training loss 0.0002304993977304548 ||\n","epoch 16:(272/293) batch || training time for 2 batch 3.0203912258148193 || training loss 0.0002550145727582276 ||\n","epoch 16:(274/293) batch || training time for 2 batch 3.0463674068450928 || training loss 0.0002471448096912354 ||\n","epoch 16:(276/293) batch || training time for 2 batch 3.0014986991882324 || training loss 0.00023625112953595817 ||\n","epoch 16:(278/293) batch || training time for 2 batch 3.0023679733276367 || training loss 0.00020869056606898084 ||\n","epoch 16:(280/293) batch || training time for 2 batch 2.980159044265747 || training loss 0.0002201543902629055 ||\n","epoch 16:(282/293) batch || training time for 2 batch 2.9661450386047363 || training loss 0.0002310126947122626 ||\n","epoch 16:(284/293) batch || training time for 2 batch 2.908012866973877 || training loss 0.00023480660456698388 ||\n","epoch 16:(286/293) batch || training time for 2 batch 2.9259259700775146 || training loss 0.00022835816344013438 ||\n","epoch 16:(288/293) batch || training time for 2 batch 2.9178884029388428 || training loss 0.00022527424152940512 ||\n","epoch 16:(290/293) batch || training time for 2 batch 2.925206422805786 || training loss 0.00021666418615495786 ||\n","epoch 16:(292/293) batch || training time for 2 batch 2.9134509563446045 || training loss 0.00029967533191666007 ||\n","epoch 17:(2/293) batch || training time for 2 batch 4.346744537353516 || training loss 0.002191263221902773 ||\n","epoch 17:(4/293) batch || training time for 2 batch 2.9042158126831055 || training loss 0.00023314675490837544 ||\n","epoch 17:(6/293) batch || training time for 2 batch 2.8900132179260254 || training loss 0.0002176350972149521 ||\n","epoch 17:(8/293) batch || training time for 2 batch 2.9112510681152344 || training loss 0.00022605451522395015 ||\n","epoch 17:(10/293) batch || training time for 2 batch 2.8902218341827393 || training loss 0.00023682781466050074 ||\n","epoch 17:(12/293) batch || training time for 2 batch 2.9397480487823486 || training loss 0.0008691695984452963 ||\n","epoch 17:(14/293) batch || training time for 2 batch 2.9146718978881836 || training loss 0.00021195041335886344 ||\n","epoch 17:(16/293) batch || training time for 2 batch 2.933192014694214 || training loss 0.000278898369288072 ||\n","epoch 17:(18/293) batch || training time for 2 batch 2.92158842086792 || training loss 0.00022236579388845712 ||\n","epoch 17:(20/293) batch || training time for 2 batch 2.9346823692321777 || training loss 0.000270591102889739 ||\n","epoch 17:(22/293) batch || training time for 2 batch 2.9367144107818604 || training loss 0.0002146531260223128 ||\n","epoch 17:(24/293) batch || training time for 2 batch 2.9563114643096924 || training loss 0.0027796465146820992 ||\n","epoch 17:(26/293) batch || training time for 2 batch 2.946707248687744 || training loss 0.00022482205531559885 ||\n","epoch 17:(28/293) batch || training time for 2 batch 2.9869251251220703 || training loss 0.00022332130174618214 ||\n","epoch 17:(30/293) batch || training time for 2 batch 3.000251054763794 || training loss 0.0002595853729872033 ||\n","epoch 17:(32/293) batch || training time for 2 batch 2.993037462234497 || training loss 0.0002230342142865993 ||\n","epoch 17:(34/293) batch || training time for 2 batch 2.9662392139434814 || training loss 0.00022191159223439172 ||\n","epoch 17:(36/293) batch || training time for 2 batch 3.003061056137085 || training loss 0.00023110186884878203 ||\n","epoch 17:(38/293) batch || training time for 2 batch 2.960270881652832 || training loss 0.0015812879573786631 ||\n","epoch 17:(40/293) batch || training time for 2 batch 2.9438440799713135 || training loss 0.00023790557315805927 ||\n","epoch 17:(42/293) batch || training time for 2 batch 2.9511520862579346 || training loss 0.00021499987633433193 ||\n","epoch 17:(44/293) batch || training time for 2 batch 2.9469265937805176 || training loss 0.00021613481658278033 ||\n","epoch 17:(46/293) batch || training time for 2 batch 2.9266812801361084 || training loss 0.00021143197955098003 ||\n","epoch 17:(48/293) batch || training time for 2 batch 2.912996292114258 || training loss 0.00021648262190865353 ||\n","epoch 17:(50/293) batch || training time for 2 batch 2.934715747833252 || training loss 0.00024515755649190396 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 69.2634129524231s\n","epoch 17:(52/293) batch || training time for 2 batch 75.69574666023254 || training loss 0.00023036281345412135 ||\n","epoch 17:(54/293) batch || training time for 2 batch 2.8932244777679443 || training loss 0.000239929482631851 ||\n","epoch 17:(56/293) batch || training time for 2 batch 2.9067869186401367 || training loss 0.00023826323740649968 ||\n","epoch 17:(58/293) batch || training time for 2 batch 2.9234366416931152 || training loss 0.00021309981093509123 ||\n","epoch 17:(60/293) batch || training time for 2 batch 2.9754388332366943 || training loss 0.00023299911117646843 ||\n","epoch 17:(62/293) batch || training time for 2 batch 2.966029644012451 || training loss 0.0002174040928366594 ||\n","epoch 17:(64/293) batch || training time for 2 batch 2.9615602493286133 || training loss 0.00021740349620813504 ||\n","epoch 17:(66/293) batch || training time for 2 batch 3.009620428085327 || training loss 0.00021865587041247636 ||\n","epoch 17:(68/293) batch || training time for 2 batch 3.0063116550445557 || training loss 0.00020474442862905562 ||\n","epoch 17:(70/293) batch || training time for 2 batch 3.029263734817505 || training loss 0.0002122858422808349 ||\n","epoch 17:(72/293) batch || training time for 2 batch 3.016815423965454 || training loss 0.00020389702694956213 ||\n","epoch 17:(74/293) batch || training time for 2 batch 3.0119595527648926 || training loss 0.0001958214124897495 ||\n","epoch 17:(76/293) batch || training time for 2 batch 3.0328874588012695 || training loss 0.0013630041503347456 ||\n","epoch 17:(78/293) batch || training time for 2 batch 3.016571521759033 || training loss 0.00027856920496560633 ||\n","epoch 17:(80/293) batch || training time for 2 batch 3.016167640686035 || training loss 0.00021512695821002126 ||\n","epoch 17:(82/293) batch || training time for 2 batch 3.004767656326294 || training loss 0.00023768948449287564 ||\n","epoch 17:(84/293) batch || training time for 2 batch 2.95076322555542 || training loss 0.0002030098403338343 ||\n","epoch 17:(86/293) batch || training time for 2 batch 2.954948663711548 || training loss 0.00021967242355458438 ||\n","epoch 17:(88/293) batch || training time for 2 batch 2.936004400253296 || training loss 0.00020589657651726156 ||\n","epoch 17:(90/293) batch || training time for 2 batch 2.9301702976226807 || training loss 0.000214299070648849 ||\n","epoch 17:(92/293) batch || training time for 2 batch 2.9376471042633057 || training loss 0.00023224721371661872 ||\n","epoch 17:(94/293) batch || training time for 2 batch 2.9188716411590576 || training loss 0.0002578027342678979 ||\n","epoch 17:(96/293) batch || training time for 2 batch 2.919072389602661 || training loss 0.00020625974866561592 ||\n","epoch 17:(98/293) batch || training time for 2 batch 2.9109175205230713 || training loss 0.00020165559544693679 ||\n","epoch 17:(100/293) batch || training time for 2 batch 2.8750038146972656 || training loss 0.00021123260376043618 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 69.74723887443542s\n","epoch 17:(102/293) batch || training time for 2 batch 76.0978353023529 || training loss 0.00023472352040698752 ||\n","epoch 17:(104/293) batch || training time for 2 batch 2.8845102787017822 || training loss 0.000227458294830285 ||\n","epoch 17:(106/293) batch || training time for 2 batch 2.9157450199127197 || training loss 0.0003239768702769652 ||\n","epoch 17:(108/293) batch || training time for 2 batch 2.9253475666046143 || training loss 0.0003068878722842783 ||\n","epoch 17:(110/293) batch || training time for 2 batch 2.9288060665130615 || training loss 0.0002363340900046751 ||\n","epoch 17:(112/293) batch || training time for 2 batch 2.9518322944641113 || training loss 0.00022200163220986724 ||\n","epoch 17:(114/293) batch || training time for 2 batch 2.9561233520507812 || training loss 0.0002105981548083946 ||\n","epoch 17:(116/293) batch || training time for 2 batch 2.978273391723633 || training loss 0.00020540162222459912 ||\n","epoch 17:(118/293) batch || training time for 2 batch 2.9974637031555176 || training loss 0.00027280813810648397 ||\n","epoch 17:(120/293) batch || training time for 2 batch 2.9882569313049316 || training loss 0.0002356236582272686 ||\n","epoch 17:(122/293) batch || training time for 2 batch 3.018059492111206 || training loss 0.000200414375285618 ||\n","epoch 17:(124/293) batch || training time for 2 batch 3.050208568572998 || training loss 0.0005786220135632902 ||\n","epoch 17:(126/293) batch || training time for 2 batch 3.0388150215148926 || training loss 0.00022521602659253404 ||\n","epoch 17:(128/293) batch || training time for 2 batch 3.000413417816162 || training loss 0.0002122032892657444 ||\n","epoch 17:(130/293) batch || training time for 2 batch 2.982997417449951 || training loss 0.0002036808873526752 ||\n","epoch 17:(132/293) batch || training time for 2 batch 2.9795169830322266 || training loss 0.00023213784879771993 ||\n","epoch 17:(134/293) batch || training time for 2 batch 2.9481124877929688 || training loss 0.00022063928190618753 ||\n","epoch 17:(136/293) batch || training time for 2 batch 2.93107008934021 || training loss 0.00019192967738490552 ||\n","epoch 17:(138/293) batch || training time for 2 batch 2.937056541442871 || training loss 0.0004586723953252658 ||\n","epoch 17:(140/293) batch || training time for 2 batch 2.883310079574585 || training loss 0.00026581514975987375 ||\n","epoch 17:(142/293) batch || training time for 2 batch 2.8930561542510986 || training loss 0.0002160102958441712 ||\n","epoch 17:(144/293) batch || training time for 2 batch 2.910655975341797 || training loss 0.00020824535749852657 ||\n","epoch 17:(146/293) batch || training time for 2 batch 2.8976478576660156 || training loss 0.00022052790882298723 ||\n","epoch 17:(148/293) batch || training time for 2 batch 2.8882710933685303 || training loss 0.00022196562349563465 ||\n","epoch 17:(150/293) batch || training time for 2 batch 2.916623115539551 || training loss 0.00021580429893219844 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 68.50857543945312s\n","epoch 17:(152/293) batch || training time for 2 batch 74.80019402503967 || training loss 0.00033040778362192214 ||\n","epoch 17:(154/293) batch || training time for 2 batch 2.8175222873687744 || training loss 0.0002115645766025409 ||\n","epoch 17:(156/293) batch || training time for 2 batch 2.825955629348755 || training loss 0.0018747622380033135 ||\n","epoch 17:(158/293) batch || training time for 2 batch 2.826136589050293 || training loss 0.0002527436809032224 ||\n","epoch 17:(160/293) batch || training time for 2 batch 2.81380295753479 || training loss 0.0002159545401809737 ||\n","epoch 17:(162/293) batch || training time for 2 batch 2.8245697021484375 || training loss 0.00024349026352865621 ||\n","epoch 17:(164/293) batch || training time for 2 batch 2.8169777393341064 || training loss 0.00021094895782880485 ||\n","epoch 17:(166/293) batch || training time for 2 batch 2.846630811691284 || training loss 0.0002579530410002917 ||\n","epoch 17:(168/293) batch || training time for 2 batch 2.829934597015381 || training loss 0.0003501804021652788 ||\n","epoch 17:(170/293) batch || training time for 2 batch 2.8268959522247314 || training loss 0.00021922348969383165 ||\n","epoch 17:(172/293) batch || training time for 2 batch 2.860790491104126 || training loss 0.00024328361178049818 ||\n","epoch 17:(174/293) batch || training time for 2 batch 2.826046943664551 || training loss 0.00021074680989841 ||\n","epoch 17:(176/293) batch || training time for 2 batch 2.8468210697174072 || training loss 0.00023565786250401288 ||\n","epoch 17:(178/293) batch || training time for 2 batch 2.832597255706787 || training loss 0.00021667731925845146 ||\n","epoch 17:(180/293) batch || training time for 2 batch 2.837472438812256 || training loss 0.00021545768686337397 ||\n","epoch 17:(182/293) batch || training time for 2 batch 2.8544812202453613 || training loss 0.00020414428581716493 ||\n","epoch 17:(184/293) batch || training time for 2 batch 2.8549306392669678 || training loss 0.00021407823805930093 ||\n","epoch 17:(186/293) batch || training time for 2 batch 2.8527615070343018 || training loss 0.0002167794227716513 ||\n","epoch 17:(188/293) batch || training time for 2 batch 2.8510348796844482 || training loss 0.000240675377426669 ||\n","epoch 17:(190/293) batch || training time for 2 batch 2.85969614982605 || training loss 0.00019931871793232858 ||\n","epoch 17:(192/293) batch || training time for 2 batch 2.907680034637451 || training loss 0.0002456041984260082 ||\n","epoch 17:(194/293) batch || training time for 2 batch 2.8744521141052246 || training loss 0.0003138386891805567 ||\n","epoch 17:(196/293) batch || training time for 2 batch 2.874617576599121 || training loss 0.00020701521134469658 ||\n","epoch 17:(198/293) batch || training time for 2 batch 2.8947086334228516 || training loss 0.00020493845659075305 ||\n","5.902958103587063e-07\n","epoch 17:(200/293) batch || training time for 2 batch 2.878180980682373 || training loss 0.00020204418251523748 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 69.7717502117157s\n","epoch 17:(202/293) batch || training time for 2 batch 76.00215435028076 || training loss 0.00021673210721928626 ||\n","epoch 17:(204/293) batch || training time for 2 batch 2.7553646564483643 || training loss 0.0001938430141308345 ||\n","epoch 17:(206/293) batch || training time for 2 batch 2.746061325073242 || training loss 0.0002572118683019653 ||\n","epoch 17:(208/293) batch || training time for 2 batch 2.782257080078125 || training loss 0.00031326196040026844 ||\n","epoch 17:(210/293) batch || training time for 2 batch 2.7380638122558594 || training loss 0.0002949125919258222 ||\n","epoch 17:(212/293) batch || training time for 2 batch 2.7662241458892822 || training loss 0.00020813182345591486 ||\n","epoch 17:(214/293) batch || training time for 2 batch 2.7484445571899414 || training loss 0.0002272335987072438 ||\n","epoch 17:(216/293) batch || training time for 2 batch 2.7626588344573975 || training loss 0.00021175062283873558 ||\n","epoch 17:(218/293) batch || training time for 2 batch 2.768192768096924 || training loss 0.0011869499503518455 ||\n","epoch 17:(220/293) batch || training time for 2 batch 2.7784745693206787 || training loss 0.0002141324948752299 ||\n","epoch 17:(222/293) batch || training time for 2 batch 2.7736072540283203 || training loss 0.0002933266951004043 ||\n","epoch 17:(224/293) batch || training time for 2 batch 2.7733609676361084 || training loss 0.00020700097957160324 ||\n","epoch 17:(226/293) batch || training time for 2 batch 2.79606294631958 || training loss 0.00019736698595806956 ||\n","epoch 17:(228/293) batch || training time for 2 batch 2.802489995956421 || training loss 0.0016733081720303744 ||\n","epoch 17:(230/293) batch || training time for 2 batch 2.799107074737549 || training loss 0.0011640070588327944 ||\n","epoch 17:(232/293) batch || training time for 2 batch 2.7993557453155518 || training loss 0.0019749674247577786 ||\n","epoch 17:(234/293) batch || training time for 2 batch 2.807698965072632 || training loss 0.00020416956976987422 ||\n","epoch 17:(236/293) batch || training time for 2 batch 2.8277390003204346 || training loss 0.00019784185860771686 ||\n","epoch 17:(238/293) batch || training time for 2 batch 2.7892556190490723 || training loss 0.00018487558554625139 ||\n","epoch 17:(240/293) batch || training time for 2 batch 2.8328890800476074 || training loss 0.00022765102767152712 ||\n","epoch 17:(242/293) batch || training time for 2 batch 2.8216099739074707 || training loss 0.00022396925487555563 ||\n","epoch 17:(244/293) batch || training time for 2 batch 2.8066189289093018 || training loss 0.0002799254216370173 ||\n","epoch 17:(246/293) batch || training time for 2 batch 2.8301243782043457 || training loss 0.00024352987384190783 ||\n","epoch 17:(248/293) batch || training time for 2 batch 2.8126652240753174 || training loss 0.00021978937002131715 ||\n","epoch 17:(250/293) batch || training time for 2 batch 2.817493438720703 || training loss 0.0004075431570527144 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 63.40093731880188s\n","epoch 17:(252/293) batch || training time for 2 batch 69.61867928504944 || training loss 0.0002221816248493269 ||\n","epoch 17:(254/293) batch || training time for 2 batch 2.741912603378296 || training loss 0.00020659909205278382 ||\n","epoch 17:(256/293) batch || training time for 2 batch 2.752791166305542 || training loss 0.00040042914042714983 ||\n","epoch 17:(258/293) batch || training time for 2 batch 2.71635365486145 || training loss 0.00018934983381768689 ||\n","epoch 17:(260/293) batch || training time for 2 batch 2.748248338699341 || training loss 0.00019398487347643822 ||\n","epoch 17:(262/293) batch || training time for 2 batch 2.837320566177368 || training loss 0.000200517381017562 ||\n","epoch 17:(264/293) batch || training time for 2 batch 2.7600765228271484 || training loss 0.0002015205900534056 ||\n","epoch 17:(266/293) batch || training time for 2 batch 2.75634765625 || training loss 0.0002607994247227907 ||\n","epoch 17:(268/293) batch || training time for 2 batch 2.8041882514953613 || training loss 0.0004556034691631794 ||\n","epoch 17:(270/293) batch || training time for 2 batch 2.7827677726745605 || training loss 0.000226420204853639 ||\n","epoch 17:(272/293) batch || training time for 2 batch 2.7982659339904785 || training loss 0.0010112692543771118 ||\n","epoch 17:(274/293) batch || training time for 2 batch 2.7649972438812256 || training loss 0.0002018316081375815 ||\n","epoch 17:(276/293) batch || training time for 2 batch 2.788604497909546 || training loss 0.00020801751088583842 ||\n","epoch 17:(278/293) batch || training time for 2 batch 2.76778244972229 || training loss 0.00022547350090462714 ||\n","epoch 17:(280/293) batch || training time for 2 batch 2.7797608375549316 || training loss 0.00023092098126653582 ||\n","epoch 17:(282/293) batch || training time for 2 batch 2.7771148681640625 || training loss 0.00019881829211954027 ||\n","epoch 17:(284/293) batch || training time for 2 batch 2.7832834720611572 || training loss 0.00019076315948041156 ||\n","epoch 17:(286/293) batch || training time for 2 batch 2.7616634368896484 || training loss 0.00021911284420639277 ||\n","epoch 17:(288/293) batch || training time for 2 batch 2.787727117538452 || training loss 0.0008613448299001902 ||\n","epoch 17:(290/293) batch || training time for 2 batch 2.7995834350585938 || training loss 0.00023237843561219051 ||\n","epoch 17:(292/293) batch || training time for 2 batch 2.7955286502838135 || training loss 0.00021404423750936985 ||\n","epoch 18:(2/293) batch || training time for 2 batch 4.18242073059082 || training loss 0.001158574472356122 ||\n","epoch 18:(4/293) batch || training time for 2 batch 2.7844431400299072 || training loss 0.0002063276042463258 ||\n","epoch 18:(6/293) batch || training time for 2 batch 2.8153631687164307 || training loss 0.00022826349595561624 ||\n","epoch 18:(8/293) batch || training time for 2 batch 2.830322742462158 || training loss 0.00021455389651237056 ||\n","epoch 18:(10/293) batch || training time for 2 batch 2.821744918823242 || training loss 0.00022179922234499827 ||\n","epoch 18:(12/293) batch || training time for 2 batch 2.83282732963562 || training loss 0.00020650863007176667 ||\n","epoch 18:(14/293) batch || training time for 2 batch 2.8081741333007812 || training loss 0.00020664384646806866 ||\n","epoch 18:(16/293) batch || training time for 2 batch 2.8192059993743896 || training loss 0.0001976950079551898 ||\n","epoch 18:(18/293) batch || training time for 2 batch 2.823263168334961 || training loss 0.0011780071727116592 ||\n","epoch 18:(20/293) batch || training time for 2 batch 2.8322978019714355 || training loss 0.00021829453180544078 ||\n","epoch 18:(22/293) batch || training time for 2 batch 2.839138984680176 || training loss 0.00026037482894025743 ||\n","epoch 18:(24/293) batch || training time for 2 batch 2.841324806213379 || training loss 0.00019658501696540043 ||\n","epoch 18:(26/293) batch || training time for 2 batch 2.8414952754974365 || training loss 0.00019533568411134183 ||\n","epoch 18:(28/293) batch || training time for 2 batch 2.829303026199341 || training loss 0.00021655310411006212 ||\n","epoch 18:(30/293) batch || training time for 2 batch 2.836975574493408 || training loss 0.00021472478692885488 ||\n","epoch 18:(32/293) batch || training time for 2 batch 2.8261733055114746 || training loss 0.00021112537069711834 ||\n","epoch 18:(34/293) batch || training time for 2 batch 2.814253330230713 || training loss 0.002298773397342302 ||\n","epoch 18:(36/293) batch || training time for 2 batch 2.840853214263916 || training loss 0.00020680662419181317 ||\n","epoch 18:(38/293) batch || training time for 2 batch 2.842573642730713 || training loss 0.0003063548356294632 ||\n","epoch 18:(40/293) batch || training time for 2 batch 2.870600461959839 || training loss 0.000246673145738896 ||\n","epoch 18:(42/293) batch || training time for 2 batch 2.847846269607544 || training loss 0.0002071014605462551 ||\n","epoch 18:(44/293) batch || training time for 2 batch 2.8369810581207275 || training loss 0.00020548026077449322 ||\n","epoch 18:(46/293) batch || training time for 2 batch 2.8706798553466797 || training loss 0.04107925033895299 ||\n","epoch 18:(48/293) batch || training time for 2 batch 2.8292832374572754 || training loss 0.00019774869724642485 ||\n","epoch 18:(50/293) batch || training time for 2 batch 2.887929916381836 || training loss 0.00021693350572604686 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 63.54232048988342s\n","epoch 18:(52/293) batch || training time for 2 batch 69.7616114616394 || training loss 0.00028741630376316607 ||\n","epoch 18:(54/293) batch || training time for 2 batch 2.729200601577759 || training loss 0.002279597887536511 ||\n","epoch 18:(56/293) batch || training time for 2 batch 2.7561206817626953 || training loss 0.00021139695309102535 ||\n","epoch 18:(58/293) batch || training time for 2 batch 2.7380762100219727 || training loss 0.00020901308016618714 ||\n","epoch 18:(60/293) batch || training time for 2 batch 2.757445812225342 || training loss 0.00021049181668786332 ||\n","epoch 18:(62/293) batch || training time for 2 batch 2.7602193355560303 || training loss 0.0001836982846725732 ||\n","epoch 18:(64/293) batch || training time for 2 batch 2.7806334495544434 || training loss 0.00037595670437440276 ||\n","epoch 18:(66/293) batch || training time for 2 batch 2.78848934173584 || training loss 0.00026830776187125593 ||\n","epoch 18:(68/293) batch || training time for 2 batch 2.758190870285034 || training loss 0.0003349368562339805 ||\n","epoch 18:(70/293) batch || training time for 2 batch 2.788576126098633 || training loss 0.00020280281023588032 ||\n","epoch 18:(72/293) batch || training time for 2 batch 2.7580578327178955 || training loss 0.00020460587984416634 ||\n","epoch 18:(74/293) batch || training time for 2 batch 2.7721526622772217 || training loss 0.00021289647702360526 ||\n","epoch 18:(76/293) batch || training time for 2 batch 2.792264461517334 || training loss 0.00022779282880946994 ||\n","epoch 18:(78/293) batch || training time for 2 batch 2.7940449714660645 || training loss 0.0002140373399015516 ||\n","epoch 18:(80/293) batch || training time for 2 batch 2.8147337436676025 || training loss 0.00018974224803969264 ||\n","epoch 18:(82/293) batch || training time for 2 batch 2.7972803115844727 || training loss 0.00019656040967674926 ||\n","epoch 18:(84/293) batch || training time for 2 batch 2.796177864074707 || training loss 0.00020638479327317327 ||\n","epoch 18:(86/293) batch || training time for 2 batch 2.8095104694366455 || training loss 0.00021890753123443574 ||\n","epoch 18:(88/293) batch || training time for 2 batch 2.8161227703094482 || training loss 0.00020095036597922444 ||\n","epoch 18:(90/293) batch || training time for 2 batch 2.8111298084259033 || training loss 0.00022676728258375078 ||\n","epoch 18:(92/293) batch || training time for 2 batch 2.8076608180999756 || training loss 0.00022006577637512237 ||\n","epoch 18:(94/293) batch || training time for 2 batch 2.824759006500244 || training loss 0.0002427067156531848 ||\n","epoch 18:(96/293) batch || training time for 2 batch 2.7973663806915283 || training loss 0.00019180971139576286 ||\n","epoch 18:(98/293) batch || training time for 2 batch 2.8012685775756836 || training loss 0.00019979356875410303 ||\n","epoch 18:(100/293) batch || training time for 2 batch 2.792536497116089 || training loss 0.00018927578639704734 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 68.00985383987427s\n","epoch 18:(102/293) batch || training time for 2 batch 74.20521950721741 || training loss 0.00023053553013596684 ||\n","epoch 18:(104/293) batch || training time for 2 batch 2.7539098262786865 || training loss 0.00022910872212378308 ||\n","epoch 18:(106/293) batch || training time for 2 batch 2.733658790588379 || training loss 0.0002069949041469954 ||\n","epoch 18:(108/293) batch || training time for 2 batch 2.7334916591644287 || training loss 0.00018742281827144325 ||\n","epoch 18:(110/293) batch || training time for 2 batch 2.739081859588623 || training loss 0.00018160828039981425 ||\n","epoch 18:(112/293) batch || training time for 2 batch 2.7528438568115234 || training loss 0.0002523987932363525 ||\n","epoch 18:(114/293) batch || training time for 2 batch 2.767658233642578 || training loss 0.0003877676645061001 ||\n","epoch 18:(116/293) batch || training time for 2 batch 2.7475526332855225 || training loss 0.00024064662284217775 ||\n","epoch 18:(118/293) batch || training time for 2 batch 2.760763645172119 || training loss 0.00024257147742900997 ||\n","epoch 18:(120/293) batch || training time for 2 batch 2.7529382705688477 || training loss 0.0002510391641408205 ||\n","epoch 18:(122/293) batch || training time for 2 batch 2.7588350772857666 || training loss 0.0005087367899250239 ||\n","epoch 18:(124/293) batch || training time for 2 batch 2.754831075668335 || training loss 0.00024021379795158282 ||\n","epoch 18:(126/293) batch || training time for 2 batch 2.7696049213409424 || training loss 0.00028412094980012625 ||\n","epoch 18:(128/293) batch || training time for 2 batch 2.7983462810516357 || training loss 0.00024910260981414467 ||\n","epoch 18:(130/293) batch || training time for 2 batch 2.7824039459228516 || training loss 0.00019931892165914178 ||\n","epoch 18:(132/293) batch || training time for 2 batch 2.779775857925415 || training loss 0.00020635795954149216 ||\n","epoch 18:(134/293) batch || training time for 2 batch 2.7787275314331055 || training loss 0.00020021744421683252 ||\n","epoch 18:(136/293) batch || training time for 2 batch 2.761470079421997 || training loss 0.00020755639707203954 ||\n","epoch 18:(138/293) batch || training time for 2 batch 2.763413906097412 || training loss 0.0001987013893085532 ||\n","epoch 18:(140/293) batch || training time for 2 batch 2.772505283355713 || training loss 0.0002062813364318572 ||\n","epoch 18:(142/293) batch || training time for 2 batch 2.7950503826141357 || training loss 0.00026766637165565044 ||\n","epoch 18:(144/293) batch || training time for 2 batch 2.779890298843384 || training loss 0.0002411867317277938 ||\n","epoch 18:(146/293) batch || training time for 2 batch 2.7885513305664062 || training loss 0.0005689480603905395 ||\n","epoch 18:(148/293) batch || training time for 2 batch 2.8121402263641357 || training loss 0.002019649895373732 ||\n","epoch 18:(150/293) batch || training time for 2 batch 2.7851555347442627 || training loss 0.00021206281962804496 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 71.1572904586792s\n","epoch 18:(152/293) batch || training time for 2 batch 77.34846377372742 || training loss 0.00023512999177910388 ||\n","epoch 18:(154/293) batch || training time for 2 batch 2.7279670238494873 || training loss 0.0002287474853801541 ||\n","epoch 18:(156/293) batch || training time for 2 batch 2.749376058578491 || training loss 0.0017355804739054292 ||\n","epoch 18:(158/293) batch || training time for 2 batch 2.7427573204040527 || training loss 0.0005042726988904178 ||\n","epoch 18:(160/293) batch || training time for 2 batch 2.7459828853607178 || training loss 0.0003521119651850313 ||\n","epoch 18:(162/293) batch || training time for 2 batch 2.71947979927063 || training loss 0.0001906581164803356 ||\n","epoch 18:(164/293) batch || training time for 2 batch 2.7170283794403076 || training loss 0.00021619233302772045 ||\n","epoch 18:(166/293) batch || training time for 2 batch 2.741971492767334 || training loss 0.0025217451620846987 ||\n","epoch 18:(168/293) batch || training time for 2 batch 2.7494661808013916 || training loss 0.00019640994287328795 ||\n","epoch 18:(170/293) batch || training time for 2 batch 2.7628867626190186 || training loss 0.00020276261784601957 ||\n","epoch 18:(172/293) batch || training time for 2 batch 2.7484636306762695 || training loss 0.00131825843709521 ||\n","epoch 18:(174/293) batch || training time for 2 batch 2.761195421218872 || training loss 0.0002639930316945538 ||\n","epoch 18:(176/293) batch || training time for 2 batch 2.755492687225342 || training loss 0.00019194630294805393 ||\n","epoch 18:(178/293) batch || training time for 2 batch 2.770023822784424 || training loss 0.0002419309748802334 ||\n","epoch 18:(180/293) batch || training time for 2 batch 2.7849531173706055 || training loss 0.00022312457440420985 ||\n","epoch 18:(182/293) batch || training time for 2 batch 2.7816975116729736 || training loss 0.0002232631013612263 ||\n","epoch 18:(184/293) batch || training time for 2 batch 2.772855043411255 || training loss 0.00019145714031765237 ||\n","epoch 18:(186/293) batch || training time for 2 batch 2.7685844898223877 || training loss 0.0003335586952744052 ||\n","epoch 18:(188/293) batch || training time for 2 batch 2.775883436203003 || training loss 0.00018974199338117614 ||\n","epoch 18:(190/293) batch || training time for 2 batch 2.790437698364258 || training loss 0.00020630716608138755 ||\n","epoch 18:(192/293) batch || training time for 2 batch 2.7951550483703613 || training loss 0.00021059215941932052 ||\n","epoch 18:(194/293) batch || training time for 2 batch 2.7878756523132324 || training loss 0.00021768776787212119 ||\n","epoch 18:(196/293) batch || training time for 2 batch 2.7611818313598633 || training loss 0.00021943257161183283 ||\n","epoch 18:(198/293) batch || training time for 2 batch 2.7699427604675293 || training loss 0.00022688492754241452 ||\n","4.7223664828696504e-07\n","epoch 18:(200/293) batch || training time for 2 batch 2.766502618789673 || training loss 0.00019710318883880973 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 70.50526666641235s\n","epoch 18:(202/293) batch || training time for 2 batch 76.68595457077026 || training loss 0.0002013804332818836 ||\n","epoch 18:(204/293) batch || training time for 2 batch 2.747392416000366 || training loss 0.00025953297154046595 ||\n","epoch 18:(206/293) batch || training time for 2 batch 2.708388566970825 || training loss 0.0002029862444032915 ||\n","epoch 18:(208/293) batch || training time for 2 batch 2.7322280406951904 || training loss 0.0028793887904612347 ||\n","epoch 18:(210/293) batch || training time for 2 batch 2.7527151107788086 || training loss 0.0003004403115483001 ||\n","epoch 18:(212/293) batch || training time for 2 batch 2.730668783187866 || training loss 0.0002458502713125199 ||\n","epoch 18:(214/293) batch || training time for 2 batch 2.7272589206695557 || training loss 0.00019089320267084986 ||\n","epoch 18:(216/293) batch || training time for 2 batch 2.7176733016967773 || training loss 0.00021181509509915486 ||\n","epoch 18:(218/293) batch || training time for 2 batch 2.755645751953125 || training loss 0.00019477570458548144 ||\n","epoch 18:(220/293) batch || training time for 2 batch 2.750364065170288 || training loss 0.0002326484172954224 ||\n","epoch 18:(222/293) batch || training time for 2 batch 2.7447919845581055 || training loss 0.00020185309404041618 ||\n","epoch 18:(224/293) batch || training time for 2 batch 2.7430179119110107 || training loss 0.00020942286209901795 ||\n","epoch 18:(226/293) batch || training time for 2 batch 2.7702202796936035 || training loss 0.00019376992713660002 ||\n","epoch 18:(228/293) batch || training time for 2 batch 2.7580134868621826 || training loss 0.00018065746553475037 ||\n","epoch 18:(230/293) batch || training time for 2 batch 2.7700915336608887 || training loss 0.00017977776587940753 ||\n","epoch 18:(232/293) batch || training time for 2 batch 2.7619268894195557 || training loss 0.00019245330622652546 ||\n","epoch 18:(234/293) batch || training time for 2 batch 2.7717719078063965 || training loss 0.000264366484771017 ||\n","epoch 18:(236/293) batch || training time for 2 batch 2.752985715866089 || training loss 0.00017843754903879017 ||\n","epoch 18:(238/293) batch || training time for 2 batch 2.7739109992980957 || training loss 0.00022924432414583862 ||\n","epoch 18:(240/293) batch || training time for 2 batch 2.776345729827881 || training loss 0.00020229692017892376 ||\n","epoch 18:(242/293) batch || training time for 2 batch 2.783261299133301 || training loss 0.0001943779643625021 ||\n","epoch 18:(244/293) batch || training time for 2 batch 2.776441812515259 || training loss 0.0009702674287836999 ||\n","epoch 18:(246/293) batch || training time for 2 batch 2.7936365604400635 || training loss 0.00027205671358387917 ||\n","epoch 18:(248/293) batch || training time for 2 batch 2.7839128971099854 || training loss 0.00020495778881013393 ||\n","epoch 18:(250/293) batch || training time for 2 batch 2.8015449047088623 || training loss 0.00020434331963770092 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 71.7112512588501s\n","epoch 18:(252/293) batch || training time for 2 batch 77.90546822547913 || training loss 0.00019311290088808164 ||\n","epoch 18:(254/293) batch || training time for 2 batch 2.7213962078094482 || training loss 0.000230726960580796 ||\n","epoch 18:(256/293) batch || training time for 2 batch 2.7241241931915283 || training loss 0.0004806154756806791 ||\n","epoch 18:(258/293) batch || training time for 2 batch 2.745312452316284 || training loss 0.00029061519308015704 ||\n","epoch 18:(260/293) batch || training time for 2 batch 2.7334659099578857 || training loss 0.00033374699705746025 ||\n","epoch 18:(262/293) batch || training time for 2 batch 2.7354354858398438 || training loss 0.0005278348253341392 ||\n","epoch 18:(264/293) batch || training time for 2 batch 2.7657694816589355 || training loss 0.00021408020984381437 ||\n","epoch 18:(266/293) batch || training time for 2 batch 2.7368762493133545 || training loss 0.0001943172319442965 ||\n","epoch 18:(268/293) batch || training time for 2 batch 2.7672531604766846 || training loss 0.00021974802075419575 ||\n","epoch 18:(270/293) batch || training time for 2 batch 2.7586002349853516 || training loss 0.000184682707185857 ||\n","epoch 18:(272/293) batch || training time for 2 batch 2.7577648162841797 || training loss 0.0003839944547507912 ||\n","epoch 18:(274/293) batch || training time for 2 batch 2.775538444519043 || training loss 0.00025143598759314045 ||\n","epoch 18:(276/293) batch || training time for 2 batch 2.77292537689209 || training loss 0.0002217620931332931 ||\n","epoch 18:(278/293) batch || training time for 2 batch 2.76181697845459 || training loss 0.0007572870672447607 ||\n","epoch 18:(280/293) batch || training time for 2 batch 2.7623648643493652 || training loss 0.00022179269581101835 ||\n","epoch 18:(282/293) batch || training time for 2 batch 2.7786593437194824 || training loss 0.00019586877897381783 ||\n","epoch 18:(284/293) batch || training time for 2 batch 2.772294282913208 || training loss 0.0002120254939654842 ||\n","epoch 18:(286/293) batch || training time for 2 batch 2.7664999961853027 || training loss 0.0002031408148468472 ||\n","epoch 18:(288/293) batch || training time for 2 batch 2.7573447227478027 || training loss 0.00020199952268740162 ||\n","epoch 18:(290/293) batch || training time for 2 batch 2.78800630569458 || training loss 0.00021881069551454857 ||\n","epoch 18:(292/293) batch || training time for 2 batch 2.784709930419922 || training loss 0.00019765901379287243 ||\n","epoch 19:(2/293) batch || training time for 2 batch 4.132107973098755 || training loss 0.00033960518339881673 ||\n","epoch 19:(4/293) batch || training time for 2 batch 2.7637476921081543 || training loss 0.0015574056378682144 ||\n","epoch 19:(6/293) batch || training time for 2 batch 2.763695001602173 || training loss 0.00020020080410176888 ||\n","epoch 19:(8/293) batch || training time for 2 batch 2.781933546066284 || training loss 0.0002213991538155824 ||\n","epoch 19:(10/293) batch || training time for 2 batch 2.7767698764801025 || training loss 0.00019841035827994347 ||\n","epoch 19:(12/293) batch || training time for 2 batch 2.7901318073272705 || training loss 0.00019469756807666272 ||\n","epoch 19:(14/293) batch || training time for 2 batch 2.7881834506988525 || training loss 0.0002000965178012848 ||\n","epoch 19:(16/293) batch || training time for 2 batch 2.796170234680176 || training loss 0.0016569786821492016 ||\n","epoch 19:(18/293) batch || training time for 2 batch 2.7972590923309326 || training loss 0.00023009301366982982 ||\n","epoch 19:(20/293) batch || training time for 2 batch 2.7921435832977295 || training loss 0.00018487219495000318 ||\n","epoch 19:(22/293) batch || training time for 2 batch 2.8284811973571777 || training loss 0.0011250615789322183 ||\n","epoch 19:(24/293) batch || training time for 2 batch 2.7948434352874756 || training loss 0.0002203028416261077 ||\n","epoch 19:(26/293) batch || training time for 2 batch 2.8094069957733154 || training loss 0.0009213997109327465 ||\n","epoch 19:(28/293) batch || training time for 2 batch 2.833827495574951 || training loss 0.00021589311654679477 ||\n","epoch 19:(30/293) batch || training time for 2 batch 2.7896358966827393 || training loss 0.00020303620112827048 ||\n","epoch 19:(32/293) batch || training time for 2 batch 2.821408271789551 || training loss 0.00023322910419665277 ||\n","epoch 19:(34/293) batch || training time for 2 batch 2.8265721797943115 || training loss 0.0014650640077888966 ||\n","epoch 19:(36/293) batch || training time for 2 batch 2.8023641109466553 || training loss 0.0020777936733793467 ||\n","epoch 19:(38/293) batch || training time for 2 batch 2.8410751819610596 || training loss 0.0002560517459642142 ||\n","epoch 19:(40/293) batch || training time for 2 batch 2.8184847831726074 || training loss 0.0001878167677205056 ||\n","epoch 19:(42/293) batch || training time for 2 batch 2.840111017227173 || training loss 0.0007281244907062501 ||\n","epoch 19:(44/293) batch || training time for 2 batch 2.836453914642334 || training loss 0.00020118148677283898 ||\n","epoch 19:(46/293) batch || training time for 2 batch 2.83733868598938 || training loss 0.0012812888453481719 ||\n","epoch 19:(48/293) batch || training time for 2 batch 2.827849864959717 || training loss 0.00022469888790510595 ||\n","epoch 19:(50/293) batch || training time for 2 batch 2.7984254360198975 || training loss 0.000293278404569719 ||\n","Saving state, index: 50\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights50.pth\n","elapsed time : 72.33064341545105s\n","epoch 19:(52/293) batch || training time for 2 batch 78.46016359329224 || training loss 0.00020483600383158773 ||\n","epoch 19:(54/293) batch || training time for 2 batch 2.734433174133301 || training loss 0.0001787017536116764 ||\n","epoch 19:(56/293) batch || training time for 2 batch 2.7320809364318848 || training loss 0.0002408354266663082 ||\n","epoch 19:(58/293) batch || training time for 2 batch 2.7292609214782715 || training loss 0.0012134124990552664 ||\n","epoch 19:(60/293) batch || training time for 2 batch 2.728416681289673 || training loss 0.00020697923173429444 ||\n","epoch 19:(62/293) batch || training time for 2 batch 2.7620506286621094 || training loss 0.0004898538609268144 ||\n","epoch 19:(64/293) batch || training time for 2 batch 2.771498918533325 || training loss 0.00021084061881992966 ||\n","epoch 19:(66/293) batch || training time for 2 batch 2.7577784061431885 || training loss 0.00020071720064152032 ||\n","epoch 19:(68/293) batch || training time for 2 batch 2.7584991455078125 || training loss 0.0002075934171443805 ||\n","epoch 19:(70/293) batch || training time for 2 batch 2.7517945766448975 || training loss 0.00020517226948868483 ||\n","epoch 19:(72/293) batch || training time for 2 batch 2.7962684631347656 || training loss 0.00021231406572042033 ||\n","epoch 19:(74/293) batch || training time for 2 batch 2.7967727184295654 || training loss 0.00021821170230396092 ||\n","epoch 19:(76/293) batch || training time for 2 batch 2.7607100009918213 || training loss 0.00019478047761367634 ||\n","epoch 19:(78/293) batch || training time for 2 batch 2.7718803882598877 || training loss 0.00020091528131160885 ||\n","epoch 19:(80/293) batch || training time for 2 batch 2.8185460567474365 || training loss 0.00022685493604512885 ||\n","epoch 19:(82/293) batch || training time for 2 batch 2.770012855529785 || training loss 0.00020604511519195512 ||\n","epoch 19:(84/293) batch || training time for 2 batch 2.7953808307647705 || training loss 0.00020833876624237746 ||\n","epoch 19:(86/293) batch || training time for 2 batch 2.7849602699279785 || training loss 0.00020081279944861308 ||\n","epoch 19:(88/293) batch || training time for 2 batch 2.7953684329986572 || training loss 0.00019506522949086502 ||\n","epoch 19:(90/293) batch || training time for 2 batch 2.7685537338256836 || training loss 0.00021710115106543526 ||\n","epoch 19:(92/293) batch || training time for 2 batch 2.7732737064361572 || training loss 0.00018841839482774958 ||\n","epoch 19:(94/293) batch || training time for 2 batch 2.781755208969116 || training loss 0.00018224927043775097 ||\n","epoch 19:(96/293) batch || training time for 2 batch 2.7760138511657715 || training loss 0.00019704940496012568 ||\n","epoch 19:(98/293) batch || training time for 2 batch 2.8223798274993896 || training loss 0.00018727812130236998 ||\n","epoch 19:(100/293) batch || training time for 2 batch 2.802281618118286 || training loss 0.0002439905729261227 ||\n","Saving state, index: 100\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights100.pth\n","elapsed time : 72.67677021026611s\n","epoch 19:(102/293) batch || training time for 2 batch 78.88123869895935 || training loss 0.0001900293427752331 ||\n","epoch 19:(104/293) batch || training time for 2 batch 2.713507890701294 || training loss 0.0001930575745063834 ||\n","epoch 19:(106/293) batch || training time for 2 batch 2.697711706161499 || training loss 0.00020246353960828856 ||\n","epoch 19:(108/293) batch || training time for 2 batch 2.7137582302093506 || training loss 0.00018512763926992193 ||\n","epoch 19:(110/293) batch || training time for 2 batch 2.7404651641845703 || training loss 0.00018710511358221993 ||\n","epoch 19:(112/293) batch || training time for 2 batch 2.7322134971618652 || training loss 0.00019006292859558016 ||\n","epoch 19:(114/293) batch || training time for 2 batch 2.743911027908325 || training loss 0.00022573355090571567 ||\n","epoch 19:(116/293) batch || training time for 2 batch 2.7475738525390625 || training loss 0.00022335223911795765 ||\n","epoch 19:(118/293) batch || training time for 2 batch 2.7376060485839844 || training loss 0.00020030345331178978 ||\n","epoch 19:(120/293) batch || training time for 2 batch 2.7485907077789307 || training loss 0.00019501200586091727 ||\n","epoch 19:(122/293) batch || training time for 2 batch 2.758963108062744 || training loss 0.00020166903414065018 ||\n","epoch 19:(124/293) batch || training time for 2 batch 2.746123790740967 || training loss 0.0002123414888046682 ||\n","epoch 19:(126/293) batch || training time for 2 batch 2.7644083499908447 || training loss 0.00019668419554363936 ||\n","epoch 19:(128/293) batch || training time for 2 batch 2.773538589477539 || training loss 0.00021320169616956264 ||\n","epoch 19:(130/293) batch || training time for 2 batch 2.769495725631714 || training loss 0.002688477950869128 ||\n","epoch 19:(132/293) batch || training time for 2 batch 2.747624635696411 || training loss 0.0003990235418314114 ||\n","epoch 19:(134/293) batch || training time for 2 batch 2.779737949371338 || training loss 0.00020874605979770422 ||\n","epoch 19:(136/293) batch || training time for 2 batch 2.761594295501709 || training loss 0.00020207947090966627 ||\n","epoch 19:(138/293) batch || training time for 2 batch 2.798253297805786 || training loss 0.0006033952085999772 ||\n","epoch 19:(140/293) batch || training time for 2 batch 2.77681827545166 || training loss 0.00020556378876790404 ||\n","epoch 19:(142/293) batch || training time for 2 batch 2.783186435699463 || training loss 0.00020656392007367685 ||\n","epoch 19:(144/293) batch || training time for 2 batch 2.750192165374756 || training loss 0.000203334006073419 ||\n","epoch 19:(146/293) batch || training time for 2 batch 2.809919834136963 || training loss 0.0001904695382108912 ||\n","epoch 19:(148/293) batch || training time for 2 batch 2.789863348007202 || training loss 0.000227663382247556 ||\n","epoch 19:(150/293) batch || training time for 2 batch 2.8005447387695312 || training loss 0.0003970402467530221 ||\n","Saving state, index: 150\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights150.pth\n","elapsed time : 65.69298458099365s\n","epoch 19:(152/293) batch || training time for 2 batch 71.83946633338928 || training loss 0.00019381812307983637 ||\n","epoch 19:(154/293) batch || training time for 2 batch 2.716834545135498 || training loss 0.0001765654087648727 ||\n","epoch 19:(156/293) batch || training time for 2 batch 2.7153337001800537 || training loss 0.0002055164222838357 ||\n","epoch 19:(158/293) batch || training time for 2 batch 2.744459390640259 || training loss 0.00021338752412702888 ||\n","epoch 19:(160/293) batch || training time for 2 batch 2.7239551544189453 || training loss 0.00020938863599440083 ||\n","epoch 19:(162/293) batch || training time for 2 batch 2.74623966217041 || training loss 0.00019197449000785127 ||\n","epoch 19:(164/293) batch || training time for 2 batch 2.7661638259887695 || training loss 0.00023761712509440258 ||\n","epoch 19:(166/293) batch || training time for 2 batch 2.758064031600952 || training loss 0.00021774183551315218 ||\n","epoch 19:(168/293) batch || training time for 2 batch 2.775209426879883 || training loss 0.00021111479873070493 ||\n","epoch 19:(170/293) batch || training time for 2 batch 2.7795567512512207 || training loss 0.00019361548766028136 ||\n","epoch 19:(172/293) batch || training time for 2 batch 2.775916814804077 || training loss 0.00024879038392100483 ||\n","epoch 19:(174/293) batch || training time for 2 batch 2.7642743587493896 || training loss 0.00021118477161508054 ||\n","epoch 19:(176/293) batch || training time for 2 batch 2.7930908203125 || training loss 0.0002056588273262605 ||\n","epoch 19:(178/293) batch || training time for 2 batch 2.781198024749756 || training loss 0.00025907306553563103 ||\n","epoch 19:(180/293) batch || training time for 2 batch 2.784717559814453 || training loss 0.0002793368330458179 ||\n","epoch 19:(182/293) batch || training time for 2 batch 2.787864923477173 || training loss 0.0002337629848625511 ||\n","epoch 19:(184/293) batch || training time for 2 batch 2.801182985305786 || training loss 0.00028893534909002483 ||\n","epoch 19:(186/293) batch || training time for 2 batch 2.754041910171509 || training loss 0.00018709400319494307 ||\n","epoch 19:(188/293) batch || training time for 2 batch 2.7780439853668213 || training loss 0.0001941518348758109 ||\n","epoch 19:(190/293) batch || training time for 2 batch 2.768566131591797 || training loss 0.0002098582626786083 ||\n","epoch 19:(192/293) batch || training time for 2 batch 2.7914841175079346 || training loss 0.00019865837384713814 ||\n","epoch 19:(194/293) batch || training time for 2 batch 2.778529405593872 || training loss 0.00020347740064607933 ||\n","epoch 19:(196/293) batch || training time for 2 batch 2.7656960487365723 || training loss 0.001717153878416866 ||\n","epoch 19:(198/293) batch || training time for 2 batch 2.7917587757110596 || training loss 0.00018629061378305778 ||\n","3.7778931862957207e-07\n","epoch 19:(200/293) batch || training time for 2 batch 2.778109312057495 || training loss 0.000202615097805392 ||\n","Saving state, index: 200\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights200.pth\n","elapsed time : 70.10036540031433s\n","epoch 19:(202/293) batch || training time for 2 batch 76.26823711395264 || training loss 0.0002342964507988654 ||\n","epoch 19:(204/293) batch || training time for 2 batch 2.699831008911133 || training loss 0.00021444262529257685 ||\n","epoch 19:(206/293) batch || training time for 2 batch 2.7119109630584717 || training loss 0.00019041012274101377 ||\n","epoch 19:(208/293) batch || training time for 2 batch 2.7401466369628906 || training loss 0.00021691024448955432 ||\n","epoch 19:(210/293) batch || training time for 2 batch 2.728005886077881 || training loss 0.00020813741866732016 ||\n","epoch 19:(212/293) batch || training time for 2 batch 2.722843885421753 || training loss 0.00019325617176946253 ||\n","epoch 19:(214/293) batch || training time for 2 batch 2.7146360874176025 || training loss 0.00019791828526649624 ||\n","epoch 19:(216/293) batch || training time for 2 batch 2.727564811706543 || training loss 0.0002479706017766148 ||\n","epoch 19:(218/293) batch || training time for 2 batch 2.740288257598877 || training loss 0.0002843599140760489 ||\n","epoch 19:(220/293) batch || training time for 2 batch 2.7697982788085938 || training loss 0.0002068257672362961 ||\n","epoch 19:(222/293) batch || training time for 2 batch 2.7750113010406494 || training loss 0.00023241227609105408 ||\n","epoch 19:(224/293) batch || training time for 2 batch 2.768435001373291 || training loss 0.00019545444229152054 ||\n","epoch 19:(226/293) batch || training time for 2 batch 2.7647323608398438 || training loss 0.00018736864876700565 ||\n","epoch 19:(228/293) batch || training time for 2 batch 2.7460379600524902 || training loss 0.00020151890203123912 ||\n","epoch 19:(230/293) batch || training time for 2 batch 2.7476701736450195 || training loss 0.00020444577239686623 ||\n","epoch 19:(232/293) batch || training time for 2 batch 2.7551794052124023 || training loss 0.00019812378741335124 ||\n","epoch 19:(234/293) batch || training time for 2 batch 2.7672078609466553 || training loss 0.0001928496203618124 ||\n","epoch 19:(236/293) batch || training time for 2 batch 2.7828779220581055 || training loss 0.00021465278405230492 ||\n","epoch 19:(238/293) batch || training time for 2 batch 2.771702766418457 || training loss 0.0004495380417210981 ||\n","epoch 19:(240/293) batch || training time for 2 batch 2.7878499031066895 || training loss 0.012545552774099633 ||\n","epoch 19:(242/293) batch || training time for 2 batch 2.7862730026245117 || training loss 0.0001949043435161002 ||\n","epoch 19:(244/293) batch || training time for 2 batch 2.7732551097869873 || training loss 0.00022916856687515974 ||\n","epoch 19:(246/293) batch || training time for 2 batch 2.785428762435913 || training loss 0.00020611603395082057 ||\n","epoch 19:(248/293) batch || training time for 2 batch 2.800783395767212 || training loss 0.00022704325965605676 ||\n","epoch 19:(250/293) batch || training time for 2 batch 2.7877280712127686 || training loss 0.00019028956739930436 ||\n","Saving state, index: 250\n","Loading weights from checkpoint ./pretrain/data/CRAFT-pytorch/synweights/synweights250.pth\n","elapsed time : 70.33235239982605s\n","epoch 19:(252/293) batch || training time for 2 batch 76.52067613601685 || training loss 0.0001883514560176991 ||\n","epoch 19:(254/293) batch || training time for 2 batch 2.7215194702148438 || training loss 0.0003174326557200402 ||\n","epoch 19:(256/293) batch || training time for 2 batch 2.7129292488098145 || training loss 0.00018975271086674184 ||\n","epoch 19:(258/293) batch || training time for 2 batch 2.7336103916168213 || training loss 0.00018853468645829707 ||\n","epoch 19:(260/293) batch || training time for 2 batch 2.7208383083343506 || training loss 0.0001825347644626163 ||\n","epoch 19:(262/293) batch || training time for 2 batch 2.7159454822540283 || training loss 0.0002029840907198377 ||\n","epoch 19:(264/293) batch || training time for 2 batch 2.7395999431610107 || training loss 0.00017633601237321272 ||\n","epoch 19:(266/293) batch || training time for 2 batch 2.7602360248565674 || training loss 0.00021744140394730493 ||\n","epoch 19:(268/293) batch || training time for 2 batch 2.7516539096832275 || training loss 0.00018902110605267808 ||\n","epoch 19:(270/293) batch || training time for 2 batch 2.750018835067749 || training loss 0.00018747022841125727 ||\n","epoch 19:(272/293) batch || training time for 2 batch 2.732697010040283 || training loss 0.00017630164074944332 ||\n","epoch 19:(274/293) batch || training time for 2 batch 2.7772068977355957 || training loss 0.00018826596351573244 ||\n","epoch 19:(276/293) batch || training time for 2 batch 2.769029140472412 || training loss 0.0007399590103887022 ||\n","epoch 19:(278/293) batch || training time for 2 batch 2.7562317848205566 || training loss 0.0001864197984104976 ||\n","epoch 19:(280/293) batch || training time for 2 batch 2.767202377319336 || training loss 0.00020623410819098353 ||\n","epoch 19:(282/293) batch || training time for 2 batch 2.7584950923919678 || training loss 0.00018362642731517553 ||\n","epoch 19:(284/293) batch || training time for 2 batch 2.7636191844940186 || training loss 0.0015183385767159052 ||\n","epoch 19:(286/293) batch || training time for 2 batch 2.7575273513793945 || training loss 0.00020371963182697073 ||\n","epoch 19:(288/293) batch || training time for 2 batch 2.801970958709717 || training loss 0.00018390171317150816 ||\n","epoch 19:(290/293) batch || training time for 2 batch 2.774116039276123 || training loss 0.0002140094802598469 ||\n","epoch 19:(292/293) batch || training time for 2 batch 2.7751333713531494 || training loss 0.001242592763446737 ||\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wVbDxxcJ-5Vx"},"source":[""],"execution_count":null,"outputs":[]}]}